
---
title: "DmClock算法原理分析"
date: 2016-11-10 14:01:26
categories: [mClock]
tags: [mClock]
toc: true
---

**术语**

| 英文 | 中文 |
|:--|:--|
| share/weight | 权重 |
| reservation | 预留或下限 | 
| limit | 上限 |


存储IO调度和CPU、内存调度相比存在以下几个不同点：

* CPU、内存通常是主机内部的资源调度。但存储IO通常是跨主机的，多台主机使用共享的存储资源，主机内IO的变化会影响相邻主机的IO。
* Hypervisor中IO调度层位于客户机IO调度的底层。因此会滋生出许多问题，例如同台主机不同VM间的局部性、同时存在多种不同大小的IO、不同请求的优先级以及突发负载等。

<!--more-->


| 算法类别 | 按比例 | 延时 | 预留 | 上限 | 突发负载 |
|:--|:--|:--|:--|:--|:--|
| 第1类算法 | Y | N | N | N | N |
| 第2类算法 | Y | Y | N | N | N |
| 第3类算法 | Y | Y | Y | N | N |
| mClock    | Y | Y | Y | Y | Y |

IO资源调度一般有3类相关的算法。第1类算法按比例分配IO资源，例如SFQ(D)，IO资源主要包括IOPS和带宽资源。第2类算法也是按比例分配IO资源，但IO资源包括延迟。第3类算法还是按比例分配IO资源，支持对给定的客户机提供最低资源保证。

## 一个例子

| VM | IOPS | 延时 | 带宽 | 权重(S) | 上限(L) | 预留(R) |
|:--|:--|:--|:--|:--|:--|:--|
| RD   | 低 | 低 | - | 100 | Inf | 250 |
| OLTP | 高 | 低 | - | 200 | Inf | 250 |
| DM   | 高 | - | - |  300 | 1000 | 0 |

上表给出3个VM，分别跑不同的业务。RD运行远程桌面，对IOPS要求不高但对延迟要求高，当IOPS较低时用户体验会很差因此保证预留为250个IOPS。OLTP要求高IOPS和低延迟，同样地预留为250个IOPS。DM运行数据迁移业务，设置上限为1000 IOPS，避免过度消耗系统带宽影响其它业务。mClock的目标是将VM的IO限制在预留和上限之间。

当系统总IOPS为1200时，按**权重比例**分配IO，那么每个VM分配到的IOPS如下：

| VM | IOPS(按比例) | IOPS(mclock) |
|:--|:--|:--|
| RD | 200 | 250 |
| OLTP | 400 | 380 |
| DM | 600 | 570 |

虽然此时系统拥有能够满足各个VM预留的IOPS（250 + 250 + 0 = 500），但RD只分配到了200IOPS低于其预留值。这种情况下，mclock的做法是：首先分配250 IOPS给RD，然后将剩余的950 IOPS按2:3的比例分配给OLTP和DM。
当系统总IOPS低于500时，无法满足每个VM的预留，因此mclock的做法是将可用的系统IO按照**预留比例**分配给VM，其中DM无法分配到IO。
当系统总IOPS在1500到2000时，直接按**权重比例**分配IO。此时不会出现VM的IO低于预留或者高于上限的情况。
当系统总IOPS高于2000时，直接按**权重比例**分配IO会导致DM的IOPS高于其上限。mclock的做法是：先为DM分配1000个IOPS，然后将剩余的IOPS按1:2的比例分配给RD和OLTP。

总而言之，分配策略会根据总吞吐量和活动VM而动态变化。将VM划分为三类：预留(R)、上限(L)、权重(P)，系统总吞吐量为T，那么IO分配公式如下：

![](http://ohn764ue3.bkt.clouddn.com/DmClock/paper/Eq.png-name)

mclock通过一种创新的tagging分配策略来达成上述公式。
上述公式存在的问题是，Tp可能小于0，也就是说系统总吞吐量要低于所有VM的总预留值。当出现这种情况时，mclock按**预留比例**来分配IO资源。这在某些情况下可能无法满足用户需求，例如有些VM没有分配预留值或者预留值为0，那么该VM可能永远无法分配到IO。不过这可以通过为每个VM都设置预留来解决。此外，针对这种情况也可考虑通过为预留设置优先级来解决（本文不再涉及）。


# mClock算法

适用于单主机的情况。
mClock是种基于tag的算法，基于tag的算法的**基本思路**: 为每个请求赋予1个tag，scheduler按照tag从小到大的顺序依次处理请求。

| VM | 1th | 2nd | 3rd | 4th | 5th | 
|:--|:--|:--|:--|:--|:--|
| A | **2** | **4** | **6** | 8 | 10 | 
| B | **3** | **6** | 9 | 12 | 15 | 
| C | **6** | 12 | 18| 24 | 30 | 

举个例子，假设A、B、C三台VM的权重分别为1/2,1/3和1/6。
scheduler为每个请求设置tag的方法：在上个请求权重的基础上递增1/wi。假设主机分别接收到了5个来自各台VM的请求，每个请求的tag如上表所示。假设主机在给定的时间段内只能处理其中的6个请求，那么按照tag的顺序，被处理的请求为：A的前3个、B的前2个、C的前1个请求。A、B、C被处理的请求数目恰好是它们权重的比例3:2:1。

这种tag赋值方法隐含着一个前提条件，那就是每个VM都是同时启动同时处于活动状态的。假如B一开始处于非活动状态，直到A接收到第5个请求时B才启动，这时B的连续若干个请求的tag值都会小于A，这将导致IO分配不符合预先配置的比例。为解决这个问题需要引入一个全局虚拟时钟（global virtual time），每个请求的tag尽量保持与gvt相同。

mClock提出两个主要的概念：多时钟(multiple real-time clocks)和动态时钟选择(dynamic clock selection)。多时钟是指mClock分别为上限(L)、预留(R)和权重(P)提供独立的tag，每个VM的请求都包含3个不同的tag值。动态始终选择指scheduler动态选择1个tag来调度IO资源。

mClock由三大组件组成：Tag赋值、Tag微调、请求调度。


## Tag赋值 Tag Assigment

![](http://ohn764ue3.bkt.clouddn.com/DmClock/paper/Symbols.png-name)

目的是为每个请求设置预留(R)、上限(L)和权重(P)三个Tag值。3个Tag的计算公式都相同，此处以预留为例。

![](http://ohn764ue3.bkt.clouddn.com/DmClock/paper/Eq_3.png-name)


假设对给定VM1，设置其预留的IOPS为10。也就说，平均每隔1/10秒处理一个请求。将VM1的请求对应到R Tag时间轴上，相邻两个请求的时间间隔为1/10。假设Server从T1时刻开始接收来自VM1的请求，在T2时刻开始处理请求。T2时刻只要完成R Tag时间轴中Tag位于T2时刻之前的请求，就能够满足预留的要求了。这是因为，R Tag时间轴的[T1, T2)区间可以排列10\*(T2-T1)个请求，完成10\*(T2-T1)个请求刚好可以满足平均IOPS为10。如果这段时间内Server接收到超过10\*(T2-T1)个请求，那么超过部分将排列到T2时刻后面，可暂不处理。如果这段时间内Server接收到的请求数目小于或等于10\*(T2-T1)，那么这些请求都将排列在T2时刻前面，被处理掉。

R Tag时间轴上的时间段[T1, T2)就像一个固定大小的容器，容器的大小便是用户配置的预留。这段时间内，如果有大量的IO请求，那么这些请求将填满容器，同时多出的部分将被放到后面的容器。T2时刻处理请求时，只需完成容器内部的请求就能够满足预留要求。

根据公式，VM1的第一个请求的R Tag将被设置为T1，T1是Server接收到VM1第一个请求的时刻。为什么R Tag的计算公式要跟当前时间进行比较？考虑如下场景：VM1先在T1时刻启动，运行段时间后在T2时刻关闭，后又在T3时刻启动。那么，从T3时刻开始VM1的请求的R Tag将相对较小，因为它是基于T2时刻前接收到的最后一个请求为基准的，而Server是按照R Tag从小到大的顺序处理请求的, 也就说，VM1 T3时刻后的请求具有空闲优先，将被优先处理掉。这对那些一直处于活动状态的VM来说是不公平的。算法实现时，可以通过一个定时器来定时检查VM是否空闲。空闲的判断标准是，如果VM的最后一个请求的达到时间到当前时间的时间差大于给定的阈值时，认为VM空闲。这样，VM重新活动后的第一个请求将被设置为该请求的达到时间，以此来消除空闲优势。

L Tag和R Tag比较类似，都可以理解为IOPS。L Tag的[T1, T2)容器的大小代表的是VM的上限。T2时刻处理请求时，若VM1剩余请求的R Tag都大于T2，那就不能继续处理VM1的请求了，否则，将导致VM1突破IOPS上限。

P Tag不同于L和R Tag，P Tag是个**相对值**(比例值)不能理解为IOPS。它的主要作用是按照给定的比例将Server的IO能力分配给不同的VM。

## Tag校准 Tag Adjustment

上文提到的**空闲优势**是VM处于非活动状态带来的一个问题。除此之外，**新VM**（刚处于活动状态的VM）也会带来一些相对于**老VM**(一直处于活动状态的VM)来说不公平的调度问题。老VM运行一段时间后，尤其是一直处于繁忙状态的VM，将越来越偏离Server的当前时间。

![](http://ohn764ue3.bkt.clouddn.com/DmClock/paper/adjust_01.png-name)

假设，如上图所示，VM_A在T1时刻启动，其首个请求的Tag设置为T1。VM_B在T2时刻启动，此时VM_A剩余请求中P Tag最小的请求的Tag为T3。如此，新启动的VM_B将获得相对于VM_A有(T3-T2)的**后起优势**。

![](http://ohn764ue3.bkt.clouddn.com/DmClock/paper/adjust_02.png-name)

消除新VM带来的后起优势的问题也很简单。将老VM的剩余请求中最早的请求的P Tag调整到当前时间（新VM的启动时间），同时保证老VM的剩余请求的相对位置不变。简而言之，将VM_A的所有剩余请求的P Tag都减少(T3-T1)，如此调整后新VM和老VM将处于相同的起跑线。此例中只有两个VM，一个老VM，如果有多个老VM时，调整时还要保证老VM间的相对位置不变。因此，P Tag的差值应该是**所有老VM**中最小请求的P Tag和当前时间的差值。

## 请求调度 Request Scheduling

mClock的请求调度分两个阶段，这两个阶段交替执行。第一个阶段称为**Constraint-based**阶段，目的是保证每个VM的预留。根据上文的解释，只要在R Tag时间轴上将所有低于当前时间的请求全部发送即可。第二个阶段称为**Weight-based**阶段，顾名思义，就是依据P Tag按比例处理不同VM的请求个数。同时，这个阶段还考虑VM的上限，对L Tag高于当前时间的请求不予处理。

![](http://ohn764ue3.bkt.clouddn.com/DmClock/paper/alg.png-name)

Weight-based阶段的请求调度会影响到下一轮的Constraint-based阶段的请求调度。假设T1时刻开始第一轮请求调度，T2时刻开始第二轮请求调度，T1时刻VM1请求的P Tag已经排到T2后面了。如果第1轮请求调度的Weight-based阶段将VM1中R Tag位于[T1, T2)时间段的请求全部处理掉，那么第2轮请求调度的Constraint-based阶段VM1将无请求可调度，也就是说，在T1~T2时间段内VM1的平均IOPS为0。这不符合用户的预留要求。

破解这个问题的方法是，在Weight-based阶段每处理一个VM1的请求时，就将VM1中所有剩余请求的R Tag递减1/r。如此，原来R Tag排在T2后面的请求在Weigt-based阶段完成后将被重排到到T1、T2间。第2轮请求调度的Constraint-based阶段将不会出现无请求可调度的情况。

## 特定于存储的问题 Storage-specific Issue

mClock算法能够应用于多种资源的调度，例如CPU、内存和存储。在应用于存储资源调度时，有诸多特定于存储相关的问题。

### 突发请求 Burst Handling

目标，为突发请求设置一定的优先处理的好处。何为突发请求？VM空闲了一小段时间，突然又有新请求的情况。给予好处的方法很简单，对这种情况原先是直接将新请求的P Tag设置为当前时间，现在可以将其设置为比当前时间更早点的时间。时间越早，好处越大。注意，此处只修改P Tag，不会影响R Tag。

### 请求类型 Request Type

无差别对待读请求和写请求。不区分读写请求的原因是，在Hypervisor层如果调整了读写请求的顺序将出现一致性问题。但，如果读写顺序在应用层保证，那么在存储层或许可以调整读、写请求的相对顺序。实际上，很多商业存储都允许分别为读或写设置优先级。

### IO大小 IO Size

P Tag是个相对值。默认情况，这个相对值指的是IOPS。因为，它只考虑了请求的个数，没有考虑每个请求的大小、延迟等其它信息。

若P Tag指的是带宽，那请考虑请求大小。假设VM1、VM2的P值都为10，希望这两个VM的带宽是相同的。同时假设，VM1 请求的大小为1M，VM2请求的大小为2M。按照原来的Tag Assigment方法，每次请求调度VM1和VM2处理的请求数目应该都相同，这将导致VM2的带宽为VM1的两倍。解决方法是，以1M大小的请求为参考，将2M大小的请求间的间隔调整为1M请求的2倍。如此调整，Server原本每处理10个VM1的请求就也要同时处理10个VM2的请求，现在每处理10个VM1的请求的同时只需要处理5个VM2的请求，从而使得VM1和VM2的带宽相同。

### 请求局部性 Request Location

IO请求具有一定的空间局部性，为提高整体的处理性能，mClock允许批量地处理一组来自同个VM的请求。只要这些请求的总大小接近逻辑块(logical block number space)大小，例如4MB。同时，设定一次批量处理中请求的数目，例如最多处理8个。

上述的优化方法会影响IO的时延。举个例子，本来按照排序是第N个进行处理的请求，由于批量处理前面N个请求（属于其它VM）都各自夹带了8个相邻的请求，从而实际上将自己排到了第8N的位置。

# dmClock算法

不同于单主机的情况，给定VM的请求全部经由该主机进行处理，多主机的情况中给定VM的请求将分散给不同的主机进行处理。因此，在Tag赋值阶段，处理请求的主机并不知道VM的前一个请求的Tag值，只知道本主机处理的来自该VM的前个请求的Tag值。为解决这个问题，VM向目标主机发送请求时需要携带相关信息。目标主机根据这些信息来分配Tag的大小。

![](http://ohn764ue3.bkt.clouddn.com/DmClock/paper/Eq_5.png-name)

对给定的VM，delta代表处理当前请求的存储主机从接收到前个请求到当前请求这段时间内，VM发送给其它主机的请求数目。同样地，rou代表对给定的VM，处理当前请求的目标主机从接收到前个请求到当前请求这段时间内，VM发送给其它主机并且在constraint-based阶段被处理的请求的个数。

![](http://ohn764ue3.bkt.clouddn.com/DmClock/paper/dmclock_01.png-name)

举个例子，如上图所示，对Server3而言，只在T1、T2两个时间点接收到请求。在T2时刻时，delta值为Server1和Server2在T1、T2时间段中接收的请求数目，为8。rou为在T1、T2时间段内Server1和Server2中在constraint-based阶段处理掉的请求个数，为3。
