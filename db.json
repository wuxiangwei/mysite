{"meta":{"version":1,"warehouse":"2.2.0"},"models":{"Asset":[{"_id":"themes/next/source/css/main.styl","path":"css/main.styl","modified":1,"renderable":1},{"_id":"themes/next/source/images/algolia_logo.svg","path":"images/algolia_logo.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/avatar.gif","path":"images/avatar.gif","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-by-nc-nd.svg","path":"images/cc-by-nc-nd.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-by-nc-sa.svg","path":"images/cc-by-nc-sa.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-by-nc.svg","path":"images/cc-by-nc.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-by-nd.svg","path":"images/cc-by-nd.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-by-sa.svg","path":"images/cc-by-sa.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-by.svg","path":"images/cc-by.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-zero.svg","path":"images/cc-zero.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/loading.gif","path":"images/loading.gif","modified":1,"renderable":1},{"_id":"themes/next/source/images/placeholder.gif","path":"images/placeholder.gif","modified":1,"renderable":1},{"_id":"themes/next/source/images/quote-l.svg","path":"images/quote-l.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/quote-r.svg","path":"images/quote-r.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/searchicon.png","path":"images/searchicon.png","modified":1,"renderable":1},{"_id":"themes/next/source/images/webchat_pay.png","path":"images/webchat_pay.png","modified":1,"renderable":1},{"_id":"themes/next/source/images/webchat_qcode.png","path":"images/webchat_qcode.png","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/affix.js","path":"js/src/affix.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/algolia-search.js","path":"js/src/algolia-search.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/bootstrap.js","path":"js/src/bootstrap.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/hook-duoshuo.js","path":"js/src/hook-duoshuo.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/motion.js","path":"js/src/motion.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/post-details.js","path":"js/src/post-details.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/scrollspy.js","path":"js/src/scrollspy.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/utils.js","path":"js/src/utils.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/algolia-instant-search/instantsearch.min.css","path":"lib/algolia-instant-search/instantsearch.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fastclick/LICENSE","path":"lib/fastclick/LICENSE","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fastclick/README.md","path":"lib/fastclick/README.md","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fastclick/bower.json","path":"lib/fastclick/bower.json","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/HELP-US-OUT.txt","path":"lib/font-awesome/HELP-US-OUT.txt","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/bower.json","path":"lib/font-awesome/bower.json","modified":1,"renderable":1},{"_id":"themes/next/source/lib/jquery_lazyload/CONTRIBUTING.md","path":"lib/jquery_lazyload/CONTRIBUTING.md","modified":1,"renderable":1},{"_id":"themes/next/source/lib/jquery_lazyload/README.md","path":"lib/jquery_lazyload/README.md","modified":1,"renderable":1},{"_id":"themes/next/source/lib/jquery_lazyload/bower.json","path":"lib/jquery_lazyload/bower.json","modified":1,"renderable":1},{"_id":"themes/next/source/lib/jquery_lazyload/jquery.lazyload.js","path":"lib/jquery_lazyload/jquery.lazyload.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/jquery_lazyload/jquery.scrollstop.js","path":"lib/jquery_lazyload/jquery.scrollstop.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/velocity/bower.json","path":"lib/velocity/bower.json","modified":1,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.min.js","path":"lib/velocity/velocity.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.ui.js","path":"lib/velocity/velocity.ui.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.ui.min.js","path":"lib/velocity/velocity.ui.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/jquery/index.js","path":"lib/jquery/index.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/schemes/pisces.js","path":"js/src/schemes/pisces.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/blank.gif","path":"lib/fancybox/source/blank.gif","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_loading.gif","path":"lib/fancybox/source/fancybox_loading.gif","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_loading@2x.gif","path":"lib/fancybox/source/fancybox_loading@2x.gif","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_overlay.png","path":"lib/fancybox/source/fancybox_overlay.png","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_sprite.png","path":"lib/fancybox/source/fancybox_sprite.png","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_sprite@2x.png","path":"lib/fancybox/source/fancybox_sprite@2x.png","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.css","path":"lib/fancybox/source/jquery.fancybox.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.js","path":"lib/fancybox/source/jquery.fancybox.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.pack.js","path":"lib/fancybox/source/jquery.fancybox.pack.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fastclick/lib/fastclick.min.js","path":"lib/fastclick/lib/fastclick.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fastclick/lib/fastclick.js","path":"lib/fastclick/lib/fastclick.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css","path":"lib/font-awesome/css/font-awesome.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css.map","path":"lib/font-awesome/css/font-awesome.css.map","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.min.css","path":"lib/font-awesome/css/font-awesome.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/ua-parser-js/dist/ua-parser.min.js","path":"lib/ua-parser-js/dist/ua-parser.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/ua-parser-js/dist/ua-parser.pack.js","path":"lib/ua-parser-js/dist/ua-parser.pack.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/FontAwesome.otf","path":"lib/font-awesome/fonts/FontAwesome.otf","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.eot","path":"lib/font-awesome/fonts/fontawesome-webfont.eot","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff","path":"lib/font-awesome/fonts/fontawesome-webfont.woff","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff2","path":"lib/font-awesome/fonts/fontawesome-webfont.woff2","modified":1,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.js","path":"lib/velocity/velocity.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/fancybox_buttons.png","path":"lib/fancybox/source/helpers/fancybox_buttons.png","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-buttons.css","path":"lib/fancybox/source/helpers/jquery.fancybox-buttons.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-buttons.js","path":"lib/fancybox/source/helpers/jquery.fancybox-buttons.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-media.js","path":"lib/fancybox/source/helpers/jquery.fancybox-media.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-thumbs.css","path":"lib/fancybox/source/helpers/jquery.fancybox-thumbs.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-thumbs.js","path":"lib/fancybox/source/helpers/jquery.fancybox-thumbs.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.ttf","path":"lib/font-awesome/fonts/fontawesome-webfont.ttf","modified":1,"renderable":1},{"_id":"themes/next/source/lib/algolia-instant-search/instantsearch.min.js","path":"lib/algolia-instant-search/instantsearch.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.svg","path":"lib/font-awesome/fonts/fontawesome-webfont.svg","modified":1,"renderable":1}],"Cache":[{"_id":"themes/next/.bowerrc","hash":"3228a58ed0ece9f85e1e3136352094080b8dece1","modified":1482483431000},{"_id":"themes/next/.editorconfig","hash":"792fd2bd8174ece1a75d5fd24ab16594886f3a7f","modified":1482483431000},{"_id":"themes/next/.git","hash":"042ff34da0707513a5681580b37513c890c671ef","modified":1482483431000},{"_id":"themes/next/.gitignore","hash":"5f09fca02e030b7676c1d312cd88ce8fbccf381c","modified":1482483431000},{"_id":"themes/next/.hound.yml","hash":"b76daa84c9ca3ad292c78412603370a367cc2bc3","modified":1482483431000},{"_id":"themes/next/.javascript_ignore","hash":"f9ea3c5395f8feb225a24e2c32baa79afda30c16","modified":1482483431000},{"_id":"themes/next/.jshintrc","hash":"9928f81bd822f6a8d67fdbc909b517178533bca9","modified":1482483431000},{"_id":"themes/next/README.en.md","hash":"565ba52b3825b85a9f05b41183caca7f18b741d4","modified":1482483431000},{"_id":"themes/next/README.md","hash":"500b5606eb6a09c979d16128f8b00f4bf9bc95ac","modified":1482483431000},{"_id":"themes/next/_config.yml","hash":"8b3e0ccf92837a2d2bc25855b04ca9db8f206be2","modified":1482483573000},{"_id":"themes/next/bower.json","hash":"5abc236d9cc2512f5457ed57c1fba76669eb7399","modified":1482483431000},{"_id":"themes/next/gulpfile.coffee","hash":"61ef0606a8134894d7ac796bc8d0fa4ba6a94483","modified":1482483431000},{"_id":"themes/next/package.json","hash":"877cb98025e59015532c4c9a04a33e2af4ad56f9","modified":1482483431000},{"_id":"source/about/index.md","hash":"56623e40709f3ce158eb0af01da8947f23f5f777","modified":1481786320000},{"_id":"source/categories/index.md","hash":"38e3fafaa66288f79cad09986219cd76c2f41a02","modified":1481786320000},{"_id":"source/tags/index.md","hash":"9c07a34ac934cc0d76373b3aa766109a0a408e1a","modified":1481786320000},{"_id":"themes/next/.github/ISSUE_TEMPLATE.md","hash":"c2024ded82143807c28a299c5fe6b927ef3525ff","modified":1482483431000},{"_id":"themes/next/.github/CONTRIBUTING.md","hash":"5ab257af816986cd0e53f9527a92d5934ac70ae9","modified":1482483431000},{"_id":"themes/next/languages/de.yml","hash":"1fdea1f84b7f691f5b4dd4d2b43eeb27b10fa0c8","modified":1482483431000},{"_id":"themes/next/languages/default.yml","hash":"767470a80dc257e23e14c3a78e8c52a46c9d6209","modified":1482483431000},{"_id":"themes/next/languages/en.yml","hash":"40057d6608e825d06e0864bac4dcd27ed88ada87","modified":1482483431000},{"_id":"themes/next/languages/fr-FR.yml","hash":"9fca01ef917d33ae2ae6bc04561ec6799dff5351","modified":1482483431000},{"_id":"themes/next/languages/id.yml","hash":"34396bef27c4ab9e9a3c5d3e3aa94b0e3b3a7b0d","modified":1482483431000},{"_id":"themes/next/languages/ja.yml","hash":"49f12149edcc1892b26a6207328cda64da20116d","modified":1482483431000},{"_id":"themes/next/languages/ko.yml","hash":"b6bc5d6b0c000deb44099b42d3aebb8c49dbfca9","modified":1482483431000},{"_id":"themes/next/languages/pt-BR.yml","hash":"7742ba4c0d682cbe1d38305332ebc928abd754b5","modified":1482483431000},{"_id":"themes/next/languages/pt.yml","hash":"6b660b117314cad93f08757601df3adb04c68beb","modified":1482483431000},{"_id":"themes/next/languages/ru.yml","hash":"257d11e626cbe4b9b78785a764190b9278f95c28","modified":1482483431000},{"_id":"themes/next/languages/zh-Hans.yml","hash":"f6c9fafa0f5f0050cd07ca2cf5e38fbae3e28145","modified":1482483431000},{"_id":"themes/next/languages/zh-hk.yml","hash":"34c84c6d04447a25bd5eac576922a13947c000e2","modified":1482483431000},{"_id":"themes/next/languages/zh-tw.yml","hash":"c97a5c41149de9b17f33439b0ecf0eff6fdae50e","modified":1482483431000},{"_id":"themes/next/layout/_layout.swig","hash":"7a1e4443c3ba1e08c20e64ddbf0b8255d034dab0","modified":1482483431000},{"_id":"themes/next/layout/archive.swig","hash":"b5b59d70fc1563f482fa07afd435752774ad5981","modified":1482483431000},{"_id":"themes/next/layout/category.swig","hash":"6422d196ceaff4220d54b8af770e7e957f3364ad","modified":1482483431000},{"_id":"themes/next/layout/index.swig","hash":"427d0b95b854e311ae363088ab39a393bf8fdc8b","modified":1482483431000},{"_id":"themes/next/layout/page.swig","hash":"3727fab9dadb967e9c2204edca787dc72264674a","modified":1482483431000},{"_id":"themes/next/layout/post.swig","hash":"e2e512142961ddfe77eba29eaa88f4a2ee43ae18","modified":1482483431000},{"_id":"themes/next/layout/schedule.swig","hash":"1f1cdc268f4ef773fd3ae693bbdf7d0b2f45c3a3","modified":1482483431000},{"_id":"themes/next/layout/tag.swig","hash":"07cf49c49c39a14dfbe9ce8e7d7eea3d4d0a4911","modified":1482483431000},{"_id":"themes/next/scripts/merge-configs.js","hash":"0c56be2e85c694247cfa327ea6d627b99ca265e8","modified":1482483431000},{"_id":"themes/next/test/.jshintrc","hash":"19f93d13d1689fe033c82eb2d5f3ce30b6543cc0","modified":1482483431000},{"_id":"themes/next/test/helpers.js","hash":"a1f5de25154c3724ffc24a91ddc576cdbd60864f","modified":1482483431000},{"_id":"themes/next/test/intern.js","hash":"11fa8a4f5c3b4119a179ae0a2584c8187f907a73","modified":1482483431000},{"_id":"themes/next/source/fonts/.gitkeep","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1482483431000},{"_id":"source/_posts/Ceph/crush_hash.md","hash":"71d77473b01cd85064e7537ab4f8e59219b7de93","modified":1481786320000},{"_id":"source/_posts/Ceph/message.md","hash":"65912880884cdcf2ecc763c9e6fab60df93589de","modified":1481786320000},{"_id":"source/_posts/Ceph/ood-context.md","hash":"f1cf9849f81ba47083f405acea2dbf9db1f9a826","modified":1481786320000},{"_id":"source/_posts/Ceph/paxos.md","hash":"554662cd0fecf32f2b25f7d2e8e66b723a79d838","modified":1481786320000},{"_id":"source/_posts/Ceph/paxos_2.md","hash":"926cd3860b7c8d391befa6e0e5ae3d4494084dc8","modified":1481786320000},{"_id":"source/_posts/Ceph/thread_pool.md","hash":"c33ad446e1a6c8a56a5741f76adb9a68a58dcdb0","modified":1481786320000},{"_id":"source/_posts/DmClock/.MCLOCK_paper.md.swp","hash":"78e7c9a0ed4e6bae7a22ea8a3d1f4c96ccd6cdbc","modified":1482480598000},{"_id":"source/_posts/DmClock/MCLOCK_compile.md","hash":"d70c0c95df3ec687babce8ceb535d274f739df8f","modified":1481786320000},{"_id":"source/_posts/DmClock/MCLOCK_dmclock.md","hash":"c97f22aebf81db5519ad555c1c98262c21a1b492","modified":1481786320000},{"_id":"source/_posts/DmClock/MCLOCK_paper.md","hash":"794e0d4f39d5ded788893c402230875f91481137","modified":1482479741000},{"_id":"source/_posts/DmClock/MCLOCK_sim.md","hash":"a9d41eb99efcddc86b3cd910d033d7731296dcc0","modified":1481786320000},{"_id":"source/_posts/DmClock/MCLOCK_test.md","hash":"e43747e7f6c209b56463f633f7bf4204007032b2","modified":1481786320000},{"_id":"source/_posts/Linux/LDAP_netgroup.md","hash":"347a4fa9d8668470a6f2558b11222702c3f7e0b8","modified":1481786320000},{"_id":"source/_posts/Linux/crontab-netstat-iostat-sar.md","hash":"a39f9f6a13a2b5d1cd22f9a3d2d642895dd977b2","modified":1481786320000},{"_id":"source/_posts/Linux/ctdb.md","hash":"0ab818b7f566ed0e11389a2c56b34b26638540ac","modified":1481786320000},{"_id":"source/_posts/Linux/fdisk.md","hash":"4f2f0fa48439076fe2f15abfb776da1a380761a1","modified":1481786320000},{"_id":"source/_posts/Linux/msys2.md","hash":"46ca3266bdcb45f880197070cf661b5117bbbc11","modified":1481786320000},{"_id":"source/_posts/Linux/nfs-setup.md","hash":"f90e76e3b1a46b96c561c0ef05f48be2b9d4fd19","modified":1481786320000},{"_id":"source/_posts/Linux/ntp.md","hash":"fbf242366a227e69ba62e94f1d0d015b078c6de5","modified":1481786320000},{"_id":"source/_posts/Linux/ocfs2-ceph-tgt.md","hash":"a8c6038ff5689f257ac0c790b7d7071021cc654c","modified":1481786320000},{"_id":"source/_posts/Linux/ocfs2-setup.md","hash":"63c95693b86a862154bc37935a25c128a57f8dce","modified":1481786320000},{"_id":"source/_posts/Other/Markdown-浠ュ啓浠ｇ爜鐨勬€濈淮鍐欐枃妗_markdown","hash":"083044813a02dd08b5a5453e960e2687dfd3bb1f","modified":1481786320000},{"_id":"source/_posts/Other/git_submodule.md","hash":"d1c70c03213968bf80f656395c754cd33283faeb","modified":1481786320000},{"_id":"source/_posts/Other/hexo_note.md","hash":"49960b8a6de81b0321010ea8678160f78f616fef","modified":1481786320000},{"_id":"source/_posts/Python/python-notes.md","hash":"94e3694ef54c1bdcc9841dcc3e85a2433a1ffaf5","modified":1481786320000},{"_id":"source/_posts/Vim/config.md","hash":"9854e6104f7165c12ac7b66dc0018e6f31e883c9","modified":1481786320000},{"_id":"source/_posts/iSCSI/iscsi-multipath.md","hash":"93f3907e95162ed8e4b1cd1eea11b255bc74a1cc","modified":1481786320000},{"_id":"source/_posts/iSCSI/iscsi-setup.md","hash":"729528ea7b58d6664cf73598b40c901d1f891e04","modified":1481786320000},{"_id":"themes/next/layout/_custom/header.swig","hash":"adc83b19e793491b1c6ea0fd8b46cd9f32e592fc","modified":1482483431000},{"_id":"themes/next/layout/_custom/sidebar.swig","hash":"adc83b19e793491b1c6ea0fd8b46cd9f32e592fc","modified":1482483431000},{"_id":"themes/next/layout/_macro/post-collapse.swig","hash":"43c3433155ccd9abcbe7dce2e6bfa1f3a66af18b","modified":1482483431000},{"_id":"themes/next/layout/_macro/post.swig","hash":"f12f108c1f8e91cc55d49805d42c1fd96cdf51a6","modified":1482483431000},{"_id":"themes/next/layout/_macro/reward.swig","hash":"37e5b7c42ec17b9b6b786c5512bcc481a21c974e","modified":1482483431000},{"_id":"themes/next/layout/_macro/sidebar.swig","hash":"b8aaa008aafe4c6e325f7513719e1c251430883e","modified":1482483431000},{"_id":"themes/next/layout/_macro/wechat-subscriber.swig","hash":"14e785adeb0e671ba0ff9a553e6f0d8def6c670c","modified":1482483431000},{"_id":"themes/next/layout/_scripts/baidu-push.swig","hash":"c057b17f79e8261680fbae8dc4e81317a127c799","modified":1482483431000},{"_id":"themes/next/layout/_scripts/boostrap.swig","hash":"03aaebe9d50f6acb007ec38cc04acd1cfceb404d","modified":1482483431000},{"_id":"themes/next/layout/_scripts/commons.swig","hash":"766b2bdda29523ed6cd8d7aa197f996022f8fd94","modified":1482483431000},{"_id":"themes/next/layout/_scripts/vendors.swig","hash":"0b91cadecead8e0b5211cc42b085998d94af503a","modified":1482483431000},{"_id":"themes/next/layout/_partials/comments.swig","hash":"7a3ef28678467c45ee9416b41b943252e8036285","modified":1482483431000},{"_id":"themes/next/layout/_partials/duoshuo-hot-articles.swig","hash":"5d4638c46aef65bf32a01681495b62416ccc98db","modified":1482483431000},{"_id":"themes/next/layout/_partials/footer.swig","hash":"7172c6053118b7c291a56a7860128a652ae66b83","modified":1482483431000},{"_id":"themes/next/layout/_partials/head.swig","hash":"ca56f92e2fa82b03853869f5073ee1a5626a4796","modified":1482483431000},{"_id":"themes/next/layout/_partials/header.swig","hash":"f3627f51810bc906e4020a3fef61bc3629b63581","modified":1482483431000},{"_id":"themes/next/layout/_partials/page-header.swig","hash":"39d613e5a9f8389d4ea52d6082502af8e833b9f2","modified":1482483431000},{"_id":"themes/next/layout/_partials/pagination.swig","hash":"9e8e21d194ef44d271b1cca0bc1448c14d7edf4f","modified":1482483431000},{"_id":"themes/next/layout/_partials/search.swig","hash":"1431719d1dbba3f5ee385eebc46376d1a960b2d5","modified":1482483431000},{"_id":"themes/next/scripts/tags/center-quote.js","hash":"535fc542781021c4326dec24d8495cbb1387634a","modified":1482483431000},{"_id":"themes/next/scripts/tags/full-image.js","hash":"8eeb3fb89540299bdbb799edfdfdac3743b50596","modified":1482483431000},{"_id":"themes/next/scripts/tags/group-pictures.js","hash":"49252824cd53184dc9b97b2f2d87ff28e1b3ef27","modified":1482483431000},{"_id":"themes/next/scripts/tags/note.js","hash":"6752925eedbdb939d8ec4d11bdfb75199f18dd70","modified":1482483431000},{"_id":"themes/next/source/css/main.styl","hash":"20702c48d6053c92c5bcdbc68e8d0ef1369848a0","modified":1482483431000},{"_id":"themes/next/source/images/algolia_logo.svg","hash":"90035272fa31a3f65b3c0e2cb8a633876ef457dc","modified":1482483431000},{"_id":"themes/next/source/images/avatar.gif","hash":"264082bb3a1af70d5499c7d22b0902cb454b6d12","modified":1482483431000},{"_id":"themes/next/source/images/cc-by-nc-nd.svg","hash":"c6524ece3f8039a5f612feaf865d21ec8a794564","modified":1482483431000},{"_id":"themes/next/source/images/cc-by-nc-sa.svg","hash":"3031be41e8753c70508aa88e84ed8f4f653f157e","modified":1482483431000},{"_id":"themes/next/source/images/cc-by-nc.svg","hash":"8d39b39d88f8501c0d27f8df9aae47136ebc59b7","modified":1482483431000},{"_id":"themes/next/source/images/cc-by-nd.svg","hash":"c563508ce9ced1e66948024ba1153400ac0e0621","modified":1482483431000},{"_id":"themes/next/source/images/cc-by-sa.svg","hash":"aa4742d733c8af8d38d4c183b8adbdcab045872e","modified":1482483431000},{"_id":"themes/next/source/images/cc-by.svg","hash":"28a0a4fe355a974a5e42f68031652b76798d4f7e","modified":1482483431000},{"_id":"themes/next/source/images/cc-zero.svg","hash":"87669bf8ac268a91d027a0a4802c92a1473e9030","modified":1482483431000},{"_id":"themes/next/source/images/loading.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1482483431000},{"_id":"themes/next/source/images/placeholder.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1482483431000},{"_id":"themes/next/source/images/quote-l.svg","hash":"94e870b4c8c48da61d09522196d4dd40e277a98f","modified":1482483431000},{"_id":"themes/next/source/images/quote-r.svg","hash":"e60ae504f9d99b712c793c3740c6b100d057d4ec","modified":1482483431000},{"_id":"themes/next/source/images/searchicon.png","hash":"67727a6a969be0b2659b908518fa6706eed307b8","modified":1482483431000},{"_id":"themes/next/layout/_scripts/schemes/mist.swig","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1482483431000},{"_id":"themes/next/layout/_scripts/schemes/muse.swig","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1482483431000},{"_id":"themes/next/source/css/_mixins/Mist.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1482483431000},{"_id":"themes/next/source/css/_mixins/Muse.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1482483431000},{"_id":"themes/next/source/css/_mixins/custom.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1482483431000},{"_id":"themes/next/source/css/_variables/Muse.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1482483431000},{"_id":"themes/next/source/images/webchat_pay.png","hash":"5b2abdfc8a043b8acd473c16ee7fc97488ddc7e4","modified":1482483431000},{"_id":"themes/next/layout/_components/algolia-search/assets.swig","hash":"28ff4ed6714c59124569ffcbd10f1173d53ca923","modified":1482483431000},{"_id":"themes/next/layout/_components/algolia-search/dom.swig","hash":"636f1181dd5887a70b4a08ca8f655d4e46635792","modified":1482483431000},{"_id":"themes/next/layout/_scripts/pages/post-details.swig","hash":"069d1357c717572256e5cdee09574ebce529cbae","modified":1482483431000},{"_id":"themes/next/layout/_scripts/schemes/pisces.swig","hash":"a44acf9b0d0f44ef3dfc767376a95c984cc127de","modified":1482483431000},{"_id":"themes/next/layout/_scripts/third-party/analytics.swig","hash":"394d9fff7951287cc90f52acc2d4cbfd1bae079d","modified":1482483431000},{"_id":"themes/next/layout/_scripts/third-party/comments.swig","hash":"82a2ac14d4200480a36bf10abcc3cc554ad744d6","modified":1482483431000},{"_id":"themes/next/layout/_scripts/third-party/lean-analytics.swig","hash":"92dc60821307fc9769bea9b2d60adaeb798342af","modified":1482483431000},{"_id":"themes/next/layout/_scripts/third-party/localsearch.swig","hash":"b460e27db3dcd4ab40b17d8926a5c4e624f293a9","modified":1482483431000},{"_id":"themes/next/layout/_scripts/third-party/mathjax.swig","hash":"6d25596d6a7c57700d37b607f8d9a62d89708683","modified":1482483431000},{"_id":"themes/next/layout/_scripts/third-party/schedule.swig","hash":"22369026c87fc23893c35a7f250b42f3bb1b60f1","modified":1482483431000},{"_id":"themes/next/layout/_scripts/third-party/tinysou.swig","hash":"cb3a5d36dbe1630bab84e03a52733a46df7c219b","modified":1482483431000},{"_id":"themes/next/layout/_partials/head/custom-head.swig","hash":"9e1b9666efa77f4cf8d8261bcfa445a9ac608e53","modified":1482483431000},{"_id":"themes/next/layout/_partials/head/external-fonts.swig","hash":"7ce76358411184482bb0934e70037949dd0da8ca","modified":1482483431000},{"_id":"themes/next/layout/_partials/search/localsearch.swig","hash":"ff5523d5dacaa77a55a24e50e6e6530c3b98bfad","modified":1482483431000},{"_id":"themes/next/layout/_partials/search/swiftype.swig","hash":"959b7e04a96a5596056e4009b73b6489c117597e","modified":1482483431000},{"_id":"themes/next/layout/_partials/search/tinysou.swig","hash":"eefe2388ff3d424694045eda21346989b123977c","modified":1482483431000},{"_id":"themes/next/layout/_partials/share/add-this.swig","hash":"23e23dc0f76ef3c631f24c65277adf7ea517b383","modified":1482483431000},{"_id":"themes/next/layout/_partials/share/baidushare.swig","hash":"1f1107468aaf03f7d0dcd7eb2b653e2813a675b4","modified":1482483431000},{"_id":"themes/next/layout/_partials/share/duoshuo_share.swig","hash":"89c5a5240ecb223acfe1d12377df5562a943fd5d","modified":1482483431000},{"_id":"themes/next/layout/_partials/share/jiathis.swig","hash":"63315fcf210799f894208c9f512737096df84962","modified":1482483431000},{"_id":"themes/next/source/css/_custom/custom.styl","hash":"328d9a9696cc2ccf59c67d3c26000d569f46344c","modified":1482483431000},{"_id":"themes/next/source/css/_mixins/Pisces.styl","hash":"715d5b40dc52f319fe4bff0325beb874774d9bd9","modified":1482483431000},{"_id":"themes/next/source/css/_mixins/base.styl","hash":"78a83c38f69a8747bb74e420e6c9eeef1ea76525","modified":1482483431000},{"_id":"themes/next/source/css/_variables/Mist.styl","hash":"c8d35a6b9e3bff6d8fdb66de853065af9d37562d","modified":1482483431000},{"_id":"themes/next/source/css/_variables/Pisces.styl","hash":"c5b28519b446c2af1e8754a6ae4d766823e6b348","modified":1482483431000},{"_id":"themes/next/source/css/_variables/base.styl","hash":"3f0d6aa424f434e82ea507f740eeff110f996269","modified":1482483431000},{"_id":"themes/next/source/css/_variables/custom.styl","hash":"0583f0a9730f40948b51fae1a550fce8b1a9757a","modified":1482483431000},{"_id":"themes/next/source/images/webchat_qcode.png","hash":"346f5b0feb6d719d7e2d48115652216db265831a","modified":1482483431000},{"_id":"themes/next/source/js/src/affix.js","hash":"978e0422b5bf1b560236d8d10ebc1adcf66392e3","modified":1482483431000},{"_id":"themes/next/source/js/src/algolia-search.js","hash":"96b29f69b8b916b22f62c9959a117b5a968200a5","modified":1482483431000},{"_id":"themes/next/source/js/src/bootstrap.js","hash":"39bf93769d9080fa01a9a875183b43198f79bc19","modified":1482483431000},{"_id":"themes/next/source/js/src/hook-duoshuo.js","hash":"a6119070c0119f33e08b29da7d2cce2635eb40a0","modified":1482483431000},{"_id":"themes/next/source/js/src/motion.js","hash":"269414e84df544a4ccb88519f6abae4943db3c67","modified":1482483431000},{"_id":"themes/next/source/js/src/post-details.js","hash":"2038f54e289b6da5def09689e69f623187147be5","modified":1482483431000},{"_id":"themes/next/source/js/src/scrollspy.js","hash":"fe4da1b9fe73518226446f5f27d2831e4426fc35","modified":1482483431000},{"_id":"themes/next/source/js/src/utils.js","hash":"384e17ff857f073060f5bf8c6e4f4b7353236331","modified":1482483431000},{"_id":"themes/next/source/lib/algolia-instant-search/instantsearch.min.css","hash":"90ef19edc982645b118b095615838d9c5eaba0de","modified":1482483431000},{"_id":"themes/next/source/lib/fancybox/.bower.json","hash":"cc40a9b11e52348e554c84e4a5c058056f6b7aeb","modified":1482483431000},{"_id":"themes/next/source/lib/fancybox/.gitattributes","hash":"2db21acfbd457452462f71cc4048a943ee61b8e0","modified":1482483431000},{"_id":"themes/next/source/lib/fastclick/.bower.json","hash":"93ebd5b35e632f714dcf1753e1f6db77ec74449b","modified":1482483431000},{"_id":"themes/next/source/lib/fastclick/LICENSE","hash":"dcd5b6b43095d9e90353a28b09cb269de8d4838e","modified":1482483431000},{"_id":"themes/next/source/lib/fastclick/README.md","hash":"1decd8e1adad2cd6db0ab50cf56de6035156f4ea","modified":1482483431000},{"_id":"themes/next/source/lib/fastclick/bower.json","hash":"13379463c7463b4b96d13556b46faa4cc38d81e6","modified":1482483431000},{"_id":"themes/next/source/lib/font-awesome/.bower.json","hash":"c1072942459fa0880e8a33a1bd929176b62b4171","modified":1482483431000},{"_id":"themes/next/source/lib/font-awesome/.gitignore","hash":"69d152fa46b517141ec3b1114dd6134724494d83","modified":1482483431000},{"_id":"themes/next/source/lib/font-awesome/.npmignore","hash":"dcf470ab3a358103bb896a539cc03caeda10fa8b","modified":1482483431000},{"_id":"themes/next/source/lib/font-awesome/HELP-US-OUT.txt","hash":"4f7bf961f1bed448f6ba99aeb9219fabf930ba96","modified":1482483431000},{"_id":"themes/next/source/lib/font-awesome/bower.json","hash":"279a8a718ab6c930a67c41237f0aac166c1b9440","modified":1482483431000},{"_id":"themes/next/source/lib/jquery/.bower.json","hash":"91745c2cc6c946c7275f952b2b0760b880cea69e","modified":1482483431000},{"_id":"themes/next/source/lib/jquery_lazyload/.bower.json","hash":"b7638afc93e9cd350d0783565ee9a7da6805ad8e","modified":1482483431000},{"_id":"themes/next/source/lib/jquery_lazyload/CONTRIBUTING.md","hash":"4891864c24c28efecd81a6a8d3f261145190f901","modified":1482483431000},{"_id":"themes/next/source/lib/jquery_lazyload/README.md","hash":"895d50fa29759af7835256522e9dd7dac597765c","modified":1482483431000},{"_id":"themes/next/source/lib/jquery_lazyload/bower.json","hash":"65bc85d12197e71c40a55c0cd7f6823995a05222","modified":1482483431000},{"_id":"themes/next/source/lib/jquery_lazyload/jquery.lazyload.js","hash":"481fd478650e12b67c201a0ea41e92743f8b45a3","modified":1482483431000},{"_id":"themes/next/source/lib/jquery_lazyload/jquery.scrollstop.js","hash":"0e9a81785a011c98be5ea821a8ed7d411818cfd1","modified":1482483431000},{"_id":"themes/next/source/lib/velocity/.bower.json","hash":"05f960846f1c7a93dab1d3f9a1121e86812e8c88","modified":1482483431000},{"_id":"themes/next/source/lib/velocity/bower.json","hash":"2ec99573e84c7117368beccb9e94b6bf35d2db03","modified":1482483431000},{"_id":"themes/next/source/lib/velocity/velocity.min.js","hash":"2f1afadc12e4cf59ef3b405308d21baa97e739c6","modified":1482483431000},{"_id":"themes/next/source/lib/velocity/velocity.ui.js","hash":"6a1d101eab3de87527bb54fcc8c7b36b79d8f0df","modified":1482483431000},{"_id":"themes/next/source/lib/velocity/velocity.ui.min.js","hash":"ed5e534cd680a25d8d14429af824f38a2c7d9908","modified":1482483431000},{"_id":"themes/next/source/lib/jquery/index.js","hash":"41b4bfbaa96be6d1440db6e78004ade1c134e276","modified":1482483431000},{"_id":"themes/next/layout/_scripts/third-party/analytics/application-insights.swig","hash":"60426bf73f8a89ba61fb1be2df3ad5398e32c4ef","modified":1482483431000},{"_id":"themes/next/layout/_scripts/third-party/analytics/baidu-analytics.swig","hash":"deda6a814ed48debc694c4e0c466f06c127163d0","modified":1482483431000},{"_id":"themes/next/layout/_scripts/third-party/analytics/busuanzi-counter.swig","hash":"4fcbf57c4918528ab51d3d042cff92cf5aefb599","modified":1482483431000},{"_id":"themes/next/layout/_scripts/third-party/analytics/cnzz-analytics.swig","hash":"8160b27bee0aa372c7dc7c8476c05bae57f58d0f","modified":1482483431000},{"_id":"themes/next/layout/_scripts/third-party/analytics/facebook-sdk.swig","hash":"394d008e5e94575280407ad8a1607a028026cbc3","modified":1482483431000},{"_id":"themes/next/layout/_scripts/third-party/analytics/google-analytics.swig","hash":"30a23fa7e816496fdec0e932aa42e2d13098a9c2","modified":1482483431000},{"_id":"themes/next/layout/_scripts/third-party/analytics/tencent-analytics.swig","hash":"3658414379e0e8a34c45c40feadc3edc8dc55f88","modified":1482483431000},{"_id":"themes/next/layout/_scripts/third-party/comments/disqus.swig","hash":"fb1d04ede838b52ca7541973f86c3810f1ad396e","modified":1482483431000},{"_id":"themes/next/layout/_scripts/third-party/comments/duoshuo.swig","hash":"a356b2185d40914447fde817eb3d358ab6b3e4c3","modified":1482483431000},{"_id":"themes/next/layout/_scripts/third-party/comments/hypercomments.swig","hash":"3e8dc5c6c912628a37e3b5f886bec7b2e5ed14ea","modified":1482483431000},{"_id":"themes/next/source/css/_common/components/back-to-top.styl","hash":"b49efc66bd055a2d0be7deabfcb02ee72a9a28c8","modified":1482483431000},{"_id":"themes/next/source/css/_common/components/buttons.styl","hash":"0dfb4b3ba3180d7285e66f270e1d3fa0f132c3d2","modified":1482483431000},{"_id":"themes/next/source/css/_common/components/comments.styl","hash":"471f1627891aca5c0e1973e09fbcb01e1510d193","modified":1482483431000},{"_id":"themes/next/source/css/_common/components/components.styl","hash":"10994990d6e0b4d965a728a22cf7f6ee29cae9f6","modified":1482483431000},{"_id":"themes/next/source/css/_common/components/pagination.styl","hash":"711c8830886619d4f4a0598b0cde5499dce50c62","modified":1482483431000},{"_id":"themes/next/source/css/_common/components/tag-cloud.styl","hash":"dd8a3b22fc2f222ac6e6c05bd8a773fb039169c0","modified":1482483431000},{"_id":"themes/next/source/css/_common/outline/outline.styl","hash":"2186be20e317505cd31886f1291429cc21f76703","modified":1482483431000},{"_id":"themes/next/source/css/_common/scaffolding/base.styl","hash":"5304f99581da3a31de3ecec959b7adf9002fde83","modified":1482483431000},{"_id":"themes/next/source/css/_common/scaffolding/helpers.styl","hash":"54c90cf7bdbf5c596179d8dae6e671bad1292662","modified":1482483431000},{"_id":"themes/next/source/css/_common/scaffolding/normalize.styl","hash":"ece571f38180febaf02ace8187ead8318a300ea7","modified":1482483431000},{"_id":"themes/next/source/css/_common/scaffolding/scaffolding.styl","hash":"013619c472c7e4b08311c464fcbe9fcf5edde603","modified":1482483431000},{"_id":"themes/next/source/css/_common/scaffolding/tables.styl","hash":"64f5d56c08d74a338813df1265580ca0cbf0190b","modified":1482483431000},{"_id":"themes/next/source/css/_schemes/Mist/_base.styl","hash":"c2d079788d6fc2e9a191ccdae94e50d55bf849dc","modified":1482483431000},{"_id":"themes/next/source/css/_schemes/Mist/_header.styl","hash":"5ae7906dc7c1d9468c7f4b4a6feddddc555797a1","modified":1482483431000},{"_id":"themes/next/source/css/_schemes/Mist/_logo.styl","hash":"38e5df90c8689a71c978fd83ba74af3d4e4e5386","modified":1482483431000},{"_id":"themes/next/source/css/_schemes/Mist/_menu.styl","hash":"b0dcca862cd0cc6e732e33d975b476d744911742","modified":1482483431000},{"_id":"themes/next/source/css/_schemes/Mist/_posts-expanded.styl","hash":"4303776991ef28f5742ca51c7dffe6f12f0acf34","modified":1482483431000},{"_id":"themes/next/source/css/_schemes/Mist/_search.styl","hash":"1452cbe674cc1d008e1e9640eb4283841058fc64","modified":1482483431000},{"_id":"themes/next/source/css/_schemes/Mist/index.styl","hash":"9a5581a770af8964064fef7afd3e16963e45547f","modified":1482483431000},{"_id":"themes/next/source/css/_schemes/Muse/_layout.styl","hash":"0efa036a15c18f5abb058b7c0fad1dd9ac5eed4c","modified":1482483431000},{"_id":"themes/next/source/css/_schemes/Muse/_logo.styl","hash":"8829bc556ca38bfec4add4f15a2f028092ac6d46","modified":1482483431000},{"_id":"themes/next/source/css/_schemes/Muse/_menu.styl","hash":"82bbaa6322764779a1ac2e2c8390ce901c7972e2","modified":1482483431000},{"_id":"themes/next/source/css/_schemes/Muse/_search.styl","hash":"1452cbe674cc1d008e1e9640eb4283841058fc64","modified":1482483431000},{"_id":"themes/next/source/css/_schemes/Muse/index.styl","hash":"a0e2030a606c934fb2c5c7373aaae04a1caac4c5","modified":1482483431000},{"_id":"themes/next/source/css/_schemes/Pisces/_brand.styl","hash":"be22ad34f546a07f6d56b424338cdd898683eea4","modified":1482483431000},{"_id":"themes/next/source/css/_schemes/Pisces/_full-image.styl","hash":"938d39eedc6e3d33918c1145a5bf1e79991d3fcf","modified":1482483431000},{"_id":"themes/next/source/css/_schemes/Pisces/_layout.styl","hash":"7b206cd8921bc042f8e37a74aea1abc8a5ec8ab4","modified":1482483431000},{"_id":"themes/next/source/css/_schemes/Pisces/_menu.styl","hash":"d09280e5b79f3b573edb30f30c7a5f03ac640986","modified":1482483431000},{"_id":"themes/next/source/css/_schemes/Pisces/_posts.styl","hash":"2f878213cb24c5ddc18877f6d15ec5c5f57745ac","modified":1482483431000},{"_id":"themes/next/source/css/_schemes/Pisces/_sidebar.styl","hash":"d4b7bd610ca03dbb2f5b66631c0e84a79fb4660b","modified":1482483431000},{"_id":"themes/next/source/css/_schemes/Pisces/index.styl","hash":"1b10ba2d3ad0c063c418dc94a0b7e0db4b342c53","modified":1482483431000},{"_id":"themes/next/source/js/src/schemes/pisces.js","hash":"9ccee9189c910b8a264802d7b2ec305d12dedcd0","modified":1482483431000},{"_id":"themes/next/source/lib/fancybox/source/blank.gif","hash":"2daeaa8b5f19f0bc209d976c02bd6acb51b00b0a","modified":1482483431000},{"_id":"themes/next/source/lib/fancybox/source/fancybox_loading.gif","hash":"1a755fb2599f3a313cc6cfdb14df043f8c14a99c","modified":1482483431000},{"_id":"themes/next/source/lib/fancybox/source/fancybox_loading@2x.gif","hash":"273b123496a42ba45c3416adb027cd99745058b0","modified":1482483431000},{"_id":"themes/next/source/lib/fancybox/source/fancybox_overlay.png","hash":"b3a4ee645ba494f52840ef8412015ba0f465dbe0","modified":1482483431000},{"_id":"themes/next/source/lib/fancybox/source/fancybox_sprite.png","hash":"17df19f97628e77be09c352bf27425faea248251","modified":1482483431000},{"_id":"themes/next/source/lib/fancybox/source/fancybox_sprite@2x.png","hash":"30c58913f327e28f466a00f4c1ac8001b560aed8","modified":1482483431000},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.css","hash":"5f163444617b6cf267342f06ac166a237bb62df9","modified":1482483431000},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.js","hash":"1cf3d47b5ccb7cb6e9019c64f2a88d03a64853e4","modified":1482483431000},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.pack.js","hash":"53360764b429c212f424399384417ccc233bb3be","modified":1482483431000},{"_id":"themes/next/source/lib/fastclick/lib/fastclick.min.js","hash":"2cae0f5a6c5d6f3cb993015e6863f9483fc4de18","modified":1482483431000},{"_id":"themes/next/source/lib/fastclick/lib/fastclick.js","hash":"06cef196733a710e77ad7e386ced6963f092dc55","modified":1482483431000},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css","hash":"4eda182cbcc046dbf449aef97c02c230cf80a494","modified":1482483431000},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css.map","hash":"0189d278706509412bac4745f96c83984e1d59f4","modified":1482483431000},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.min.css","hash":"fb5b49426dee7f1508500e698d1b3c6b04c8fcce","modified":1482483431000},{"_id":"themes/next/source/lib/ua-parser-js/dist/ua-parser.min.js","hash":"38628e75e4412cc6f11074e03e1c6d257aae495b","modified":1482483431000},{"_id":"themes/next/source/lib/ua-parser-js/dist/ua-parser.pack.js","hash":"214dad442a92d36af77ed0ca1d9092b16687f02f","modified":1482483431000},{"_id":"themes/next/source/lib/font-awesome/fonts/FontAwesome.otf","hash":"1b22f17fdc38070de50e6d1ab3a32da71aa2d819","modified":1482483431000},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.eot","hash":"965ce8f688fedbeed504efd498bc9c1622d12362","modified":1482483431000},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff","hash":"6d7e6a5fc802b13694d8820fc0138037c0977d2e","modified":1482483431000},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff2","hash":"97e438cc545714309882fbceadbf344fcaddcec5","modified":1482483431000},{"_id":"themes/next/source/lib/velocity/velocity.js","hash":"9f08181baea0cc0e906703b7e5df9111b9ef3373","modified":1482483431000},{"_id":"themes/next/source/css/_common/components/footer/footer.styl","hash":"8994ffcce84deac0471532f270f97c44fea54dc0","modified":1482483431000},{"_id":"themes/next/source/css/_common/components/header/header.styl","hash":"ae1ca14e51de67b07dba8f61ec79ee0e2e344574","modified":1482483431000},{"_id":"themes/next/source/css/_common/components/header/headerband.styl","hash":"d27448f199fc2f9980b601bc22b87f08b5d64dd1","modified":1482483431000},{"_id":"themes/next/source/css/_common/components/header/menu.styl","hash":"c890ce7fe933abad7baf39764a01894924854e92","modified":1482483431000},{"_id":"themes/next/source/css/_common/components/header/site-meta.styl","hash":"6c00f6e0978f4d8f9a846a15579963728aaa6a17","modified":1482483431000},{"_id":"themes/next/source/css/_common/components/header/site-nav.styl","hash":"49c2b2c14a1e7fcc810c6be4b632975d0204c281","modified":1482483431000},{"_id":"themes/next/source/css/_common/components/highlight/diff.styl","hash":"96f32ea6c3265a3889e6abe57587f6e2a2a40dfb","modified":1482483431000},{"_id":"themes/next/source/css/_common/components/highlight/highlight.styl","hash":"4b7f81e1006e7acee3d1c840ccba155239f830cc","modified":1482483431000},{"_id":"themes/next/source/css/_common/components/highlight/theme.styl","hash":"b76387934fb6bb75212b23c1a194486892cc495e","modified":1482483431000},{"_id":"themes/next/source/css/_common/components/pages/archive.styl","hash":"7778920dd105fa4de3a7ab206eeba30b1a7bac45","modified":1482483431000},{"_id":"themes/next/source/css/_common/components/pages/categories.styl","hash":"4eff5b252d7b614e500fc7d52c97ce325e57d3ab","modified":1482483431000},{"_id":"themes/next/source/css/_common/components/pages/pages.styl","hash":"2039590632bba3943c39319d80ef630af7928185","modified":1482483431000},{"_id":"themes/next/source/css/_common/components/pages/post-detail.styl","hash":"9bf4362a4d0ae151ada84b219d39fbe5bb8c790e","modified":1482483431000},{"_id":"themes/next/source/css/_common/components/pages/schedule.styl","hash":"a82afbb72d83ee394aedc7b37ac0008a9823b4f4","modified":1482483431000},{"_id":"themes/next/source/css/_common/components/post/post-collapse.styl","hash":"8fae54591877a73dff0b29b2be2e8935e3c63575","modified":1482483431000},{"_id":"themes/next/source/css/_common/components/post/post-eof.styl","hash":"2cdc094ecf907a02fce25ad4a607cd5c40da0f2b","modified":1482483431000},{"_id":"themes/next/source/css/_common/components/post/post-expand.styl","hash":"b25132fe6a7ad67059a2c3afc60feabb479bdd75","modified":1482483431000},{"_id":"themes/next/source/css/_common/components/post/post-gallery.styl","hash":"387ce23bba52b22a586b2dfb4ec618fe1ffd3926","modified":1482483431000},{"_id":"themes/next/source/css/_common/components/post/post-meta.styl","hash":"7f1aab694caf603809e33cff82beea84cd0128fd","modified":1482483431000},{"_id":"themes/next/source/css/_common/components/post/post-more-link.styl","hash":"15063d79b5befc21820baf05d6f20cc1c1787477","modified":1482483431000},{"_id":"themes/next/source/css/_common/components/post/post-nav.styl","hash":"c6dab7661a6b8c678b21b7eb273cef7100f970f6","modified":1482483431000},{"_id":"themes/next/source/css/_common/components/post/post-reward.styl","hash":"e792c8dc41561c96d128e9b421187f1c3dc978a0","modified":1482483431000},{"_id":"themes/next/source/css/_common/components/post/post-tags.styl","hash":"a352ae5b1f8857393bf770d2e638bf15f0c9585d","modified":1482483431000},{"_id":"themes/next/source/css/_common/components/post/post-title.styl","hash":"963105a531403d7aad6d9e5e23e3bfabb8ec065a","modified":1482483431000},{"_id":"themes/next/source/css/_common/components/post/post-type.styl","hash":"10251257aceecb117233c9554dcf8ecfef8e2104","modified":1482483431000},{"_id":"themes/next/source/css/_common/components/post/post.styl","hash":"4eb18b12fa0ea6c35925d9a64f64e2a7dae8c7fd","modified":1482483431000},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-author-links.styl","hash":"2e7ec9aaa3293941106b1bdd09055246aa3c3dc6","modified":1482483431000},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-author.styl","hash":"920343e41c124221a17f050bbb989494d44f7a24","modified":1482483431000},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-blogroll.styl","hash":"c44f6a553ec7ea5508f2054a13be33a62a15d3a9","modified":1482483431000},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-feed-link.styl","hash":"9486ddd2cb255227db102d09a7df4cae0fabad72","modified":1482483431000},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-nav.styl","hash":"45fa7193435a8eae9960267438750b4c9fa9587f","modified":1482483431000},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-toc.styl","hash":"7690b9596ec3a49befbe529a5a2649abec0faf76","modified":1482483431000},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-toggle.styl","hash":"2d3abbc85b979a648e0e579e45f16a6eba49d1e7","modified":1482483431000},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar.styl","hash":"234facd038f144bd0fe09a31ed1357c5d74c517f","modified":1482483431000},{"_id":"themes/next/source/css/_common/components/sidebar/site-state.styl","hash":"3623e7fa4324ec1307370f33d8f287a9e20a5578","modified":1482483431000},{"_id":"themes/next/source/css/_common/components/tags/blockquote-center.styl","hash":"c2abe4d87148e23e15d49ee225bc650de60baf46","modified":1482483431000},{"_id":"themes/next/source/css/_common/components/tags/full-image.styl","hash":"618f73450cf541f88a4fddc3d22898aee49d105d","modified":1482483431000},{"_id":"themes/next/source/css/_common/components/tags/group-pictures.styl","hash":"4851b981020c5cbc354a1af9b831a2dcb3cf9d39","modified":1482483431000},{"_id":"themes/next/source/css/_common/components/tags/note.styl","hash":"74d0ba86f698165d13402670382a822c8736a556","modified":1482483431000},{"_id":"themes/next/source/css/_common/components/tags/tags.styl","hash":"3eb73cee103b810fa56901577ecb9c9bb1793cff","modified":1482483431000},{"_id":"themes/next/source/css/_common/components/third-party/algolia-search.styl","hash":"eba491ae624b4c843c8be4c94a044085dad4ba0f","modified":1482483431000},{"_id":"themes/next/source/css/_common/components/third-party/baidushare.styl","hash":"93b08815c4d17e2b96fef8530ec1f1064dede6ef","modified":1482483431000},{"_id":"themes/next/source/css/_common/components/third-party/busuanzi-counter.styl","hash":"b03f891883446f3a5548b7cc90d29c77e62f1053","modified":1482483431000},{"_id":"themes/next/source/css/_common/components/third-party/duoshuo.styl","hash":"2340dd9b3202c61d73cc708b790fac5adddbfc7f","modified":1482483431000},{"_id":"themes/next/source/css/_common/components/third-party/jiathis.styl","hash":"327b5f63d55ec26f7663185c1a778440588d9803","modified":1482483431000},{"_id":"themes/next/source/css/_common/components/third-party/localsearch.styl","hash":"637c6b32c58ecf40041be6e911471cd82671919b","modified":1482483431000},{"_id":"themes/next/source/css/_common/components/third-party/third-party.styl","hash":"5433b6bc9d8f0c4685e760b326445ac51245b0a8","modified":1482483431000},{"_id":"themes/next/source/css/_schemes/Mist/outline/outline.styl","hash":"5dc4859c66305f871e56cba78f64bfe3bf1b5f01","modified":1482483431000},{"_id":"themes/next/source/css/_schemes/Mist/sidebar/sidebar-blogroll.styl","hash":"8b8e8cbce98a9296c8fd77f512ae85d945f65d40","modified":1482483431000},{"_id":"themes/next/source/css/_schemes/Muse/sidebar/sidebar-blogroll.styl","hash":"8b8e8cbce98a9296c8fd77f512ae85d945f65d40","modified":1482483431000},{"_id":"themes/next/source/lib/fancybox/source/helpers/fancybox_buttons.png","hash":"e385b139516c6813dcd64b8fc431c364ceafe5f3","modified":1482483431000},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-buttons.css","hash":"1a9d8e5c22b371fcc69d4dbbb823d9c39f04c0c8","modified":1482483431000},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-buttons.js","hash":"91e41741c2e93f732c82aaacec4cfc6e3f3ec876","modified":1482483431000},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-media.js","hash":"3bdf69ed2469e4fb57f5a95f17300eef891ff90d","modified":1482483431000},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-thumbs.css","hash":"4ac329c16a5277592fc12a37cca3d72ca4ec292f","modified":1482483431000},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-thumbs.js","hash":"53e194f4a72e649c04fb586dd57762b8c022800b","modified":1482483431000},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.ttf","hash":"61d8d967807ef12598d81582fa95b9f600c3ee01","modified":1482483431000},{"_id":"themes/next/source/lib/algolia-instant-search/instantsearch.min.js","hash":"9ccc6f8144f54e86df9a3fd33a18368d81cf3a4f","modified":1482483431000},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.svg","hash":"c0522272bbaef2acb3d341912754d6ea2d0ecfc0","modified":1482483431000}],"Category":[{"name":"Ceph","_id":"cix1kgatl0004daeizjf911u4"},{"name":"mClock","_id":"cix1kgau6000sdaein02icmqs"},{"name":"Linux","_id":"cix1kgauk0019daeitdlivjil"},{"name":"Other","_id":"cix1kgav5002bdaei6xcpog4r"},{"name":"Python","_id":"cix1kgava002kdaei3i30bh0u"},{"name":"Vim","_id":"cix1kgavc002pdaeiyxbzpdy7"},{"name":"iSCSI","_id":"cix1kgave002udaein4lqmo2t"}],"Data":[],"Page":[{"date":"2016-12-05T20:18:13.000Z","type":"about","comments":0,"_content":"\n\n# 关于作者\n\nshanno，男，可以通过以下联系方式与我交流相关技术问题\n * Email: <a href=\"mailto:shanno@yeah.net\" title=\"shanno@yeah.net\">shanno@yeah.net</a>\n * Github: https://github.com/wuxiangwei\n","source":"about/index.md","raw":"---\ndate: 2016-12-06 04:18:13\ntype: \"about\"\ncomments: false\n---\n\n\n# 关于作者\n\nshanno，男，可以通过以下联系方式与我交流相关技术问题\n * Email: <a href=\"mailto:shanno@yeah.net\" title=\"shanno@yeah.net\">shanno@yeah.net</a>\n * Github: https://github.com/wuxiangwei\n","updated":"2016-12-15T07:18:40.000Z","path":"about/index.html","title":"","layout":"page","_id":"cix1kgaom0000daei4f76uhhn","content":"<h1 id=\"关于作者\"><a href=\"#关于作者\" class=\"headerlink\" title=\"关于作者\"></a>关于作者</h1><p>shanno，男，可以通过以下联系方式与我交流相关技术问题</p>\n<ul>\n<li>Email: <a href=\"mailto:shanno@yeah.net\" title=\"shanno@yeah.net\" target=\"_blank\" rel=\"external\">shanno@yeah.net</a></li>\n<li>Github: <a href=\"https://github.com/wuxiangwei\" target=\"_blank\" rel=\"external\">https://github.com/wuxiangwei</a></li>\n</ul>\n<hr>\n<p><strong>版权声明</strong></p>\n<p>本作品采用<a href=\"https://creativecommons.org/licenses/by/2.5/cn/\" target=\"_blank\" rel=\"external\">知识共享署名2.5 中国大陆许可协议</a>进行许可，欢迎转载，但转载请注明来自<a href=\"https://wuxiangwei.cn/\"><em>Shanno’s Blog</em></a>，并保持转载后文章内容的完整。本人保留所有版权相关权利，侵权必究。</p>\n","excerpt":"","more":"<h1 id=\"关于作者\"><a href=\"#关于作者\" class=\"headerlink\" title=\"关于作者\"></a>关于作者</h1><p>shanno，男，可以通过以下联系方式与我交流相关技术问题</p>\n<ul>\n<li>Email: <a href=\"mailto:shanno@yeah.net\" title=\"shanno@yeah.net\">shanno@yeah.net</a></li>\n<li>Github: <a href=\"https://github.com/wuxiangwei\">https://github.com/wuxiangwei</a></li>\n</ul>\n<hr>\n<p><strong>版权声明</strong></p>\n<p>本作品采用<a href=\"https://creativecommons.org/licenses/by/2.5/cn/\">知识共享署名2.5 中国大陆许可协议</a>进行许可，欢迎转载，但转载请注明来自<a href=\"https://wuxiangwei.cn/\"><em>Shanno’s Blog</em></a>，并保持转载后文章内容的完整。本人保留所有版权相关权利，侵权必究。</p>\n"},{"title":"categories","date":"2016-12-02T23:53:01.000Z","type":"categories","comments":0,"_content":"","source":"categories/index.md","raw":"---\ntitle: categories\ndate: 2016-12-03 07:53:01\ntype: \"categories\"\ncomments: false\n---\n","updated":"2016-12-15T07:18:40.000Z","path":"categories/index.html","layout":"page","_id":"cix1kgaop0001daei15qbuv7d","content":"","excerpt":"","more":""},{"title":"tags","date":"2016-12-02T23:49:55.000Z","type":"tags","comments":0,"_content":"","source":"tags/index.md","raw":"---\ntitle: tags\ndate: 2016-12-03 07:49:55\ntype: \"tags\"\ncomments: false\n---\n","updated":"2016-12-15T07:18:40.000Z","path":"tags/index.html","layout":"page","_id":"cix1kgaoq0002daei5u63j7ag","content":"","excerpt":"","more":""}],"Post":[{"title":"Ceph CRUSH算法与一致性Hash算法","date":"2014-09-05T12:23:29.000Z","toc":true,"_content":"\n## 引言\n\n数据分布是分布式存储系统的一个重要部分，数据分布算法至少要考虑以下三个因素：\n\n* 故障域隔离。同份数据的不同副本分布在不同的故障域，降低数据损坏的风险；\n* 负载均衡。数据能够均匀地分布在磁盘容量不等的存储节点，避免部分节点空闲部分节点超载，从而影响系统性能；\n* 控制节点加入离开时引起的数据迁移量。当节点离开时，最优的数据迁移是只有离线节点上的数据被迁移到其它节点，而正常工作的节点的数据不会发生迁移。\n\n对象存储中一致性Hash和Ceph的CRUSH算法是使用地比较多的数据分布算法。在Aamzon的Dyanmo键值存储系统中采用一致性Hash算法，并且对它做了很多优化。OpenStack的Swift对象存储系统也使用了一致性Hash算法。\n\n<!--more-->\n\n## 一致性Hash算法\n\n假设数据为*x*，存储节点数目为*N*。将数据分布到存储节点的最直接做法是，计算数据*x*的Hash值，并将结果同节点数目*N*取余数，余数就是数据x的目的存储节点。即目的存储节点为 *Hash(x) % N*。对数据计算Hash值的目的为了可以让数据均匀分布在N个节点中。这种做法的一个严重问题是，当加入新节点或则节点离开时，几乎所有数据都会受到影响，需要重新分布。因此，数据迁移量非常大。\n\n![](http://images.cnitblog.com/blog/571795/201409/051617221721380.jpg)\n\n\n一致性Hash算法将数据和存储节点映射到同个Hash空间，如上图所示。Hash环中的3存储节点把Hash空间划分成3个分区，每个存储节点负责一个分区上的数据。例如，落在分区[N2,N0]上的数据存储在节点N0。\n\n一致性Hash算法能够很好地控制节点加入离开导致的迁移数据的数量。如图(b)所示，当节点N0离开时，原来由它负责的[N2, N0]分区将同[N0, N1]分区合并成[N2, N1]分区，并且都由节点N1负责。也就是说，本来存储在节点N0上的数据都迁移到节点N1，而原来存储在N1和N2节点的数据不受影响。图(c)给出了当节点N3加入时，原来[N2, N0]分区分裂成[N3, N0]和[N2, N3]两个分区，其中[N3, N0]分区上是数据迁移到新加入的N3节点。\n\n### 虚拟节点\n\n一致性Hash的一个问题是，存储节点不能将Hash空间划分地足够均匀。如上图(a)所示，分区[N2, N0]的大小几乎是其它两个分区大小之和。这容易让负责该分区的节点N0负载过重。假设3个节点的磁盘容量相等，那么当节点N0的磁盘已经写满数据时其它两个节点上的磁盘还有很大的空闲空间，但此时系统已经无法继续向分区[N2, N0]写入数据，从而造成资源浪费。\n\n![](http://images.cnitblog.com/blog/571795/201409/051617496108430.jpg)\n\n\n虚拟节点是相对于物理存储节点而言的，虚拟节点负责的分区上的数据最终存储到其对应的物理节点。在一致性Hash中引入虚拟节点可以把Hash空间划分成更多的分区，从而让数据在存储节点上的分布更加均匀。如上图(b)所示，黄颜色的节点代表虚拟节点，Ni_0代表该虚拟节点对应于物理节点i的第0个虚拟节点。增加虚拟节点后，物理节点N0负责[N1_0, N0]和[N0, N0_0]两个分区，物理节点N1负责[N0_0, N1]和[N2_0, N1_0]两个分区，物理节点N2负责[N2, N1]和[N2_0, N2]两个分区，三个物理节点负责的总的数据量趋于平衡。\n\n实际应用中，可以根据物理节点的磁盘容量的大小来确定其对应的虚拟节点数目。虚拟节点数目越多，节点负责的数据区间也越大。\n\n### 分区与分区位置\n\n前文提到，当节点加入或者离开时，分区会相应地进行分裂或合并。这不对新写入的数据构成影响，但对已经写入到磁盘的数据需要重新计算Hash值以确定它是否需要迁移到其它节点。因为需要遍历磁盘中的所有数据，这个计算过程非常耗时。如下图(a)所示，分区是由落在Hash环上的虚拟节点*Ti*来划分的，并且分区位置(存储分区数据的节点)也同虚拟节点相关，即存储到其顺时针方向的第1个虚拟节点。\n\n\n![](http://images.cnitblog.com/blog/571795/201409/051618049539276.jpg)\n\n\n在Dynamo的论文中提出了分离分区和分区位置的方法来解决这个问题。该方法将Hash空间划分成固定的若干个分区，虚拟节点不再用于划分分区而用来确定分区的存储位置。如上图(b)所示，将Hash空间划分成[A,B]，[B,C], [C,D]和[D,A]四个固定的分区。虚拟节点用于确定分区位置，例如T1负责分区[B,C]，T2负责分区[C,D]，T0负责[D,A]和[A,B]两个分区。由于分区固定，因此迁移数据时可以很容易知道哪些数据需要迁移哪些数据不需要迁移。\n\n上图(b)中虚拟节点T0负责了[D,A]和[A,B]两个分区的数据，这是由分区数目和虚拟节点数目不相同导致的。为让分区分布地更加均匀，Dyanmo提出了维持分区数目和虚拟节点数目相等的方法。这样每个虚拟节点负责一个分区，在物理节点的磁盘容量都相同并且虚拟节点数目都相同的情况下，每个物理节点负责的分区大小是完全相同的，从而可以达到最佳的数据分布。\n\n## CRUSH算法\n\nCeph分布数据的过程：首先计算数据*x*的Hash值并将结果和PG数目取余，以得到数据*x*对应的*PG*编号。然后，通过CRUSH算法将PG映射到一组OSD中。最后把数据*x*存放到PG对应的OSD中。这个过程中包含了两次映射，第一次是数据*x*到PG的映射。如果把PG当作存储节点，那么这和文章开头提到的普通Hash算法一样。不同的是，PG是抽象的存储节点，它不会随着物理节点的加入或则离开而增加或减少，因此数据到PG的映射是稳定的。\n\n\n![](http://images.cnitblog.com/blog/571795/201409/051618214693665.jpg)\n\n\n\n在这个过程中，PG起到了两个作用：第一个作用是划分数据分区。每个PG管理的数据区间相同，因而数据能够均匀地分布到PG上；第二个作用是充当Dyanmo中Token的角色，即决定分区位置。实际上，这和Dynamo中固定分区数目，以及维持分区数目和虚拟节点数目相等的原则是同一回事。\n\n在没有多副本的情况下，Dynamo中分区的数据直接存储到Token，而每个Token对应唯一的一个物理存储节点。在多副本(假设副本数目为*N*)的情况下，分区的数据会存储到连续的*N*个Token中。但这会引入一个新问题：因为副本必须保持在不同的物理节点，但是如果这组Token中存在两个或多个Token对应到同个物理存储节点，那么就必须要跳过这样的节点。Dynamo采用Preference列表来记录每个分区对应的物理节点。然而，Dynmao论文中没有详述分区的Preference列表如何选取物理节点，以及选取物理节点时该如何隔离故障域等问题。\n\n> (osd0, osd1, osd2 … osdn) = CRUSH(x)\n\nCeph的PG担当起Dynamo中Token、固定分区以及Preference列表的角色，解决的是同样的问题。PG的Acting集合对应于Dynamo的Preference列表。CRUSH算法解决了Dynamo论文中未提及的问题。\n\n### OSD层级结构和权重大小\n\nCRUSH算法的目的是，为给定的PG(即分区)分配一组存储数据的OSD节点。选择OSD节点的过程，要考虑以下几个因素：\n\n1) PG在OSD间均匀分布。假设每个OSD的磁盘容量都相同，那么我们希望PG在每个OSD节点上是均匀分布的，也就是说每个OSD节点包含相同数目的PG。假如节点的磁盘容量不等，那么容量大的磁盘的节点能够处理更多数量的PG。\n2) PG的OSD分布在不同的故障域。因为PG的OSD列表用于保存数据的不同副本，副本分布在不同的OSD中可以降低数据损坏的风险。\n\n\n![](http://images.cnitblog.com/blog/571795/201409/051618360636140.jpg)\n\n\nCeph使用树型层级结构描述OSD的空间位置以及权重(同磁盘容量相关)大小。如上图所示，层级结构描述了OSD所在主机、主机所在机架以及机架所在机房等空间位置。这些空间位置隐含了故障区域，例如使用不同电源的不同的机架属于不同的故障域。CRUSH能够依据一定的规则将副本放置在不同的故障域。\n\nOSD节点在层级结构中也被称为Device，它位于层级结构的叶子节点，所有非叶子节点称为Bucket。Bucket拥有不同的类型，如上图所示，所有机架的类型为Rack，所有主机的类型为Host。使用者还可以自己定义Bucket的类型。Device节点的权重代表存储节点的性能，磁盘容量是影响权重大小的重要参数。Bucket节点的权重是其子节点的权重之和。\n\nCRUSH通过重复执行Take(bucketID)和Select(n, bucketType)两个操作选取副本位置。Take(bucketID)指定从给定的bucketID中选取副本位置，例如可以指定从某台机架上选取副本位置，以实现将不同的副本隔离在不同的故障域; Select(n, bucketType)则在给定的Bucket下选取*n*个类型为bucketType的Bucket，它选取Bucket主要考虑层级结构中节点的容量，以及当节点离线或者加入时的数据迁移量。\n\n### 算法流程\n\n![](http://images.cnitblog.com/blog/571795/201409/051618505162658.jpg)\n\n\n上图给出了CRUSH选取副本的流程图。\n\nbucket: Take操作指定的bucket；                  \ntype: Select操作指定的Bucket的类型；                \nrepnum: Select操作指定的副本数目；             \n\nrep：当前选择的副本编号；          \nx: 当前选择的PG编号；             \nitem: 代表当前被选中的Bucket；             \nc(r, x, in): 代表从Bucket in中为PG x选取第r个副本；             \ncollide: 代表当前选中的副本位置item已经被选中，即出现了冲突；             \nreject:  代表当前选中的副本位置item被拒绝，例如，在item已经处于out状态的情况下；             \n\n\nftotal:  在Descent域中选择的失败次数，即选择一个副本位置的总共的失败次数；             \nflocal:  在Local域中选择的失败次数；                  \nlocal_retries:  在Local域选择冲突时的尝试次数；                  \nlocal_fallback_retries:  允许在Local域的总共尝试次数为bucket.size + local_fallback_retires次，以保证遍历完Buckt的所有子节点；                   \ntries:  在Descent的最大尝试次数，超过这个次数则放弃这个副本。                \n\n![](http://images.cnitblog.com/blog/571795/201409/051619033448705.jpg)\n\n\n当Take操作指定的Bucket和Select操作指定的Bucket类型之间隔着几层Bucket时，算法直接深度优先地进入到目的Bucket的直接父母节点。例如，从根节点开始选择N个Host时，它会深度优先地查找到Rack类型的节点，并在这个节点下选取Host节点。为了方便表述，将Rack的所有子节点标记为Local域，将Take指定的Bucket的子节点标记为Descent域，如上图所示。\n\n选取过程中出现冲突、过载或者故障时，算法先在Local域内重新选择，尝试有限次数后，如果仍然找不到满足条件的Bucket，那就回到Descent域重新选择。每次重新选择时，修改副本数目为*r += ftotal*。因此每次选择失败都会递增ftotal，所以可以尽量避免选择时再次选到冲突的节点。\n\n### Bucket选取Item算法\n\n流程图中的*item=c(r,x,in)*从给定的Bucket in中选取一个子节点。\n\n### CRUSH rule和POOL的关系\n\n(待续)\n\n参考资料\n\n1、[Ceph的CRUSH数据分布算法介绍](http://way4ever.com/?p=122)        \n2、CRUSH:Controlled,Scalable,Decentralized Placement of Replicated Data           \n3、Dynamo:Amazon Highly Available Key-value Store\n\n","source":"_posts/Ceph/crush_hash.md","raw":"---\ntitle: \"Ceph CRUSH算法与一致性Hash算法\" \ndate: 2014-09-05 20:23:29 \ncategory: [Ceph]\ntags: [Ceph]\ntoc: true\n---\n\n## 引言\n\n数据分布是分布式存储系统的一个重要部分，数据分布算法至少要考虑以下三个因素：\n\n* 故障域隔离。同份数据的不同副本分布在不同的故障域，降低数据损坏的风险；\n* 负载均衡。数据能够均匀地分布在磁盘容量不等的存储节点，避免部分节点空闲部分节点超载，从而影响系统性能；\n* 控制节点加入离开时引起的数据迁移量。当节点离开时，最优的数据迁移是只有离线节点上的数据被迁移到其它节点，而正常工作的节点的数据不会发生迁移。\n\n对象存储中一致性Hash和Ceph的CRUSH算法是使用地比较多的数据分布算法。在Aamzon的Dyanmo键值存储系统中采用一致性Hash算法，并且对它做了很多优化。OpenStack的Swift对象存储系统也使用了一致性Hash算法。\n\n<!--more-->\n\n## 一致性Hash算法\n\n假设数据为*x*，存储节点数目为*N*。将数据分布到存储节点的最直接做法是，计算数据*x*的Hash值，并将结果同节点数目*N*取余数，余数就是数据x的目的存储节点。即目的存储节点为 *Hash(x) % N*。对数据计算Hash值的目的为了可以让数据均匀分布在N个节点中。这种做法的一个严重问题是，当加入新节点或则节点离开时，几乎所有数据都会受到影响，需要重新分布。因此，数据迁移量非常大。\n\n![](http://images.cnitblog.com/blog/571795/201409/051617221721380.jpg)\n\n\n一致性Hash算法将数据和存储节点映射到同个Hash空间，如上图所示。Hash环中的3存储节点把Hash空间划分成3个分区，每个存储节点负责一个分区上的数据。例如，落在分区[N2,N0]上的数据存储在节点N0。\n\n一致性Hash算法能够很好地控制节点加入离开导致的迁移数据的数量。如图(b)所示，当节点N0离开时，原来由它负责的[N2, N0]分区将同[N0, N1]分区合并成[N2, N1]分区，并且都由节点N1负责。也就是说，本来存储在节点N0上的数据都迁移到节点N1，而原来存储在N1和N2节点的数据不受影响。图(c)给出了当节点N3加入时，原来[N2, N0]分区分裂成[N3, N0]和[N2, N3]两个分区，其中[N3, N0]分区上是数据迁移到新加入的N3节点。\n\n### 虚拟节点\n\n一致性Hash的一个问题是，存储节点不能将Hash空间划分地足够均匀。如上图(a)所示，分区[N2, N0]的大小几乎是其它两个分区大小之和。这容易让负责该分区的节点N0负载过重。假设3个节点的磁盘容量相等，那么当节点N0的磁盘已经写满数据时其它两个节点上的磁盘还有很大的空闲空间，但此时系统已经无法继续向分区[N2, N0]写入数据，从而造成资源浪费。\n\n![](http://images.cnitblog.com/blog/571795/201409/051617496108430.jpg)\n\n\n虚拟节点是相对于物理存储节点而言的，虚拟节点负责的分区上的数据最终存储到其对应的物理节点。在一致性Hash中引入虚拟节点可以把Hash空间划分成更多的分区，从而让数据在存储节点上的分布更加均匀。如上图(b)所示，黄颜色的节点代表虚拟节点，Ni_0代表该虚拟节点对应于物理节点i的第0个虚拟节点。增加虚拟节点后，物理节点N0负责[N1_0, N0]和[N0, N0_0]两个分区，物理节点N1负责[N0_0, N1]和[N2_0, N1_0]两个分区，物理节点N2负责[N2, N1]和[N2_0, N2]两个分区，三个物理节点负责的总的数据量趋于平衡。\n\n实际应用中，可以根据物理节点的磁盘容量的大小来确定其对应的虚拟节点数目。虚拟节点数目越多，节点负责的数据区间也越大。\n\n### 分区与分区位置\n\n前文提到，当节点加入或者离开时，分区会相应地进行分裂或合并。这不对新写入的数据构成影响，但对已经写入到磁盘的数据需要重新计算Hash值以确定它是否需要迁移到其它节点。因为需要遍历磁盘中的所有数据，这个计算过程非常耗时。如下图(a)所示，分区是由落在Hash环上的虚拟节点*Ti*来划分的，并且分区位置(存储分区数据的节点)也同虚拟节点相关，即存储到其顺时针方向的第1个虚拟节点。\n\n\n![](http://images.cnitblog.com/blog/571795/201409/051618049539276.jpg)\n\n\n在Dynamo的论文中提出了分离分区和分区位置的方法来解决这个问题。该方法将Hash空间划分成固定的若干个分区，虚拟节点不再用于划分分区而用来确定分区的存储位置。如上图(b)所示，将Hash空间划分成[A,B]，[B,C], [C,D]和[D,A]四个固定的分区。虚拟节点用于确定分区位置，例如T1负责分区[B,C]，T2负责分区[C,D]，T0负责[D,A]和[A,B]两个分区。由于分区固定，因此迁移数据时可以很容易知道哪些数据需要迁移哪些数据不需要迁移。\n\n上图(b)中虚拟节点T0负责了[D,A]和[A,B]两个分区的数据，这是由分区数目和虚拟节点数目不相同导致的。为让分区分布地更加均匀，Dyanmo提出了维持分区数目和虚拟节点数目相等的方法。这样每个虚拟节点负责一个分区，在物理节点的磁盘容量都相同并且虚拟节点数目都相同的情况下，每个物理节点负责的分区大小是完全相同的，从而可以达到最佳的数据分布。\n\n## CRUSH算法\n\nCeph分布数据的过程：首先计算数据*x*的Hash值并将结果和PG数目取余，以得到数据*x*对应的*PG*编号。然后，通过CRUSH算法将PG映射到一组OSD中。最后把数据*x*存放到PG对应的OSD中。这个过程中包含了两次映射，第一次是数据*x*到PG的映射。如果把PG当作存储节点，那么这和文章开头提到的普通Hash算法一样。不同的是，PG是抽象的存储节点，它不会随着物理节点的加入或则离开而增加或减少，因此数据到PG的映射是稳定的。\n\n\n![](http://images.cnitblog.com/blog/571795/201409/051618214693665.jpg)\n\n\n\n在这个过程中，PG起到了两个作用：第一个作用是划分数据分区。每个PG管理的数据区间相同，因而数据能够均匀地分布到PG上；第二个作用是充当Dyanmo中Token的角色，即决定分区位置。实际上，这和Dynamo中固定分区数目，以及维持分区数目和虚拟节点数目相等的原则是同一回事。\n\n在没有多副本的情况下，Dynamo中分区的数据直接存储到Token，而每个Token对应唯一的一个物理存储节点。在多副本(假设副本数目为*N*)的情况下，分区的数据会存储到连续的*N*个Token中。但这会引入一个新问题：因为副本必须保持在不同的物理节点，但是如果这组Token中存在两个或多个Token对应到同个物理存储节点，那么就必须要跳过这样的节点。Dynamo采用Preference列表来记录每个分区对应的物理节点。然而，Dynmao论文中没有详述分区的Preference列表如何选取物理节点，以及选取物理节点时该如何隔离故障域等问题。\n\n> (osd0, osd1, osd2 … osdn) = CRUSH(x)\n\nCeph的PG担当起Dynamo中Token、固定分区以及Preference列表的角色，解决的是同样的问题。PG的Acting集合对应于Dynamo的Preference列表。CRUSH算法解决了Dynamo论文中未提及的问题。\n\n### OSD层级结构和权重大小\n\nCRUSH算法的目的是，为给定的PG(即分区)分配一组存储数据的OSD节点。选择OSD节点的过程，要考虑以下几个因素：\n\n1) PG在OSD间均匀分布。假设每个OSD的磁盘容量都相同，那么我们希望PG在每个OSD节点上是均匀分布的，也就是说每个OSD节点包含相同数目的PG。假如节点的磁盘容量不等，那么容量大的磁盘的节点能够处理更多数量的PG。\n2) PG的OSD分布在不同的故障域。因为PG的OSD列表用于保存数据的不同副本，副本分布在不同的OSD中可以降低数据损坏的风险。\n\n\n![](http://images.cnitblog.com/blog/571795/201409/051618360636140.jpg)\n\n\nCeph使用树型层级结构描述OSD的空间位置以及权重(同磁盘容量相关)大小。如上图所示，层级结构描述了OSD所在主机、主机所在机架以及机架所在机房等空间位置。这些空间位置隐含了故障区域，例如使用不同电源的不同的机架属于不同的故障域。CRUSH能够依据一定的规则将副本放置在不同的故障域。\n\nOSD节点在层级结构中也被称为Device，它位于层级结构的叶子节点，所有非叶子节点称为Bucket。Bucket拥有不同的类型，如上图所示，所有机架的类型为Rack，所有主机的类型为Host。使用者还可以自己定义Bucket的类型。Device节点的权重代表存储节点的性能，磁盘容量是影响权重大小的重要参数。Bucket节点的权重是其子节点的权重之和。\n\nCRUSH通过重复执行Take(bucketID)和Select(n, bucketType)两个操作选取副本位置。Take(bucketID)指定从给定的bucketID中选取副本位置，例如可以指定从某台机架上选取副本位置，以实现将不同的副本隔离在不同的故障域; Select(n, bucketType)则在给定的Bucket下选取*n*个类型为bucketType的Bucket，它选取Bucket主要考虑层级结构中节点的容量，以及当节点离线或者加入时的数据迁移量。\n\n### 算法流程\n\n![](http://images.cnitblog.com/blog/571795/201409/051618505162658.jpg)\n\n\n上图给出了CRUSH选取副本的流程图。\n\nbucket: Take操作指定的bucket；                  \ntype: Select操作指定的Bucket的类型；                \nrepnum: Select操作指定的副本数目；             \n\nrep：当前选择的副本编号；          \nx: 当前选择的PG编号；             \nitem: 代表当前被选中的Bucket；             \nc(r, x, in): 代表从Bucket in中为PG x选取第r个副本；             \ncollide: 代表当前选中的副本位置item已经被选中，即出现了冲突；             \nreject:  代表当前选中的副本位置item被拒绝，例如，在item已经处于out状态的情况下；             \n\n\nftotal:  在Descent域中选择的失败次数，即选择一个副本位置的总共的失败次数；             \nflocal:  在Local域中选择的失败次数；                  \nlocal_retries:  在Local域选择冲突时的尝试次数；                  \nlocal_fallback_retries:  允许在Local域的总共尝试次数为bucket.size + local_fallback_retires次，以保证遍历完Buckt的所有子节点；                   \ntries:  在Descent的最大尝试次数，超过这个次数则放弃这个副本。                \n\n![](http://images.cnitblog.com/blog/571795/201409/051619033448705.jpg)\n\n\n当Take操作指定的Bucket和Select操作指定的Bucket类型之间隔着几层Bucket时，算法直接深度优先地进入到目的Bucket的直接父母节点。例如，从根节点开始选择N个Host时，它会深度优先地查找到Rack类型的节点，并在这个节点下选取Host节点。为了方便表述，将Rack的所有子节点标记为Local域，将Take指定的Bucket的子节点标记为Descent域，如上图所示。\n\n选取过程中出现冲突、过载或者故障时，算法先在Local域内重新选择，尝试有限次数后，如果仍然找不到满足条件的Bucket，那就回到Descent域重新选择。每次重新选择时，修改副本数目为*r += ftotal*。因此每次选择失败都会递增ftotal，所以可以尽量避免选择时再次选到冲突的节点。\n\n### Bucket选取Item算法\n\n流程图中的*item=c(r,x,in)*从给定的Bucket in中选取一个子节点。\n\n### CRUSH rule和POOL的关系\n\n(待续)\n\n参考资料\n\n1、[Ceph的CRUSH数据分布算法介绍](http://way4ever.com/?p=122)        \n2、CRUSH:Controlled,Scalable,Decentralized Placement of Replicated Data           \n3、Dynamo:Amazon Highly Available Key-value Store\n\n","slug":"Ceph/crush_hash","published":1,"updated":"2016-12-15T07:18:40.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cix1kgath0003daeiknbk7v8m","content":"<h2 id=\"引言\"><a href=\"#引言\" class=\"headerlink\" title=\"引言\"></a>引言</h2><p>数据分布是分布式存储系统的一个重要部分，数据分布算法至少要考虑以下三个因素：</p>\n<ul>\n<li>故障域隔离。同份数据的不同副本分布在不同的故障域，降低数据损坏的风险；</li>\n<li>负载均衡。数据能够均匀地分布在磁盘容量不等的存储节点，避免部分节点空闲部分节点超载，从而影响系统性能；</li>\n<li>控制节点加入离开时引起的数据迁移量。当节点离开时，最优的数据迁移是只有离线节点上的数据被迁移到其它节点，而正常工作的节点的数据不会发生迁移。</li>\n</ul>\n<p>对象存储中一致性Hash和Ceph的CRUSH算法是使用地比较多的数据分布算法。在Aamzon的Dyanmo键值存储系统中采用一致性Hash算法，并且对它做了很多优化。OpenStack的Swift对象存储系统也使用了一致性Hash算法。</p>\n<a id=\"more\"></a>\n<h2 id=\"一致性Hash算法\"><a href=\"#一致性Hash算法\" class=\"headerlink\" title=\"一致性Hash算法\"></a>一致性Hash算法</h2><p>假设数据为<em>x</em>，存储节点数目为<em>N</em>。将数据分布到存储节点的最直接做法是，计算数据<em>x</em>的Hash值，并将结果同节点数目<em>N</em>取余数，余数就是数据x的目的存储节点。即目的存储节点为 <em>Hash(x) % N</em>。对数据计算Hash值的目的为了可以让数据均匀分布在N个节点中。这种做法的一个严重问题是，当加入新节点或则节点离开时，几乎所有数据都会受到影响，需要重新分布。因此，数据迁移量非常大。</p>\n<p><img src=\"http://images.cnitblog.com/blog/571795/201409/051617221721380.jpg\" alt=\"\"></p>\n<p>一致性Hash算法将数据和存储节点映射到同个Hash空间，如上图所示。Hash环中的3存储节点把Hash空间划分成3个分区，每个存储节点负责一个分区上的数据。例如，落在分区[N2,N0]上的数据存储在节点N0。</p>\n<p>一致性Hash算法能够很好地控制节点加入离开导致的迁移数据的数量。如图(b)所示，当节点N0离开时，原来由它负责的[N2, N0]分区将同[N0, N1]分区合并成[N2, N1]分区，并且都由节点N1负责。也就是说，本来存储在节点N0上的数据都迁移到节点N1，而原来存储在N1和N2节点的数据不受影响。图(c)给出了当节点N3加入时，原来[N2, N0]分区分裂成[N3, N0]和[N2, N3]两个分区，其中[N3, N0]分区上是数据迁移到新加入的N3节点。</p>\n<h3 id=\"虚拟节点\"><a href=\"#虚拟节点\" class=\"headerlink\" title=\"虚拟节点\"></a>虚拟节点</h3><p>一致性Hash的一个问题是，存储节点不能将Hash空间划分地足够均匀。如上图(a)所示，分区[N2, N0]的大小几乎是其它两个分区大小之和。这容易让负责该分区的节点N0负载过重。假设3个节点的磁盘容量相等，那么当节点N0的磁盘已经写满数据时其它两个节点上的磁盘还有很大的空闲空间，但此时系统已经无法继续向分区[N2, N0]写入数据，从而造成资源浪费。</p>\n<p><img src=\"http://images.cnitblog.com/blog/571795/201409/051617496108430.jpg\" alt=\"\"></p>\n<p>虚拟节点是相对于物理存储节点而言的，虚拟节点负责的分区上的数据最终存储到其对应的物理节点。在一致性Hash中引入虚拟节点可以把Hash空间划分成更多的分区，从而让数据在存储节点上的分布更加均匀。如上图(b)所示，黄颜色的节点代表虚拟节点，Ni_0代表该虚拟节点对应于物理节点i的第0个虚拟节点。增加虚拟节点后，物理节点N0负责[N1_0, N0]和[N0, N0_0]两个分区，物理节点N1负责[N0_0, N1]和[N2_0, N1_0]两个分区，物理节点N2负责[N2, N1]和[N2_0, N2]两个分区，三个物理节点负责的总的数据量趋于平衡。</p>\n<p>实际应用中，可以根据物理节点的磁盘容量的大小来确定其对应的虚拟节点数目。虚拟节点数目越多，节点负责的数据区间也越大。</p>\n<h3 id=\"分区与分区位置\"><a href=\"#分区与分区位置\" class=\"headerlink\" title=\"分区与分区位置\"></a>分区与分区位置</h3><p>前文提到，当节点加入或者离开时，分区会相应地进行分裂或合并。这不对新写入的数据构成影响，但对已经写入到磁盘的数据需要重新计算Hash值以确定它是否需要迁移到其它节点。因为需要遍历磁盘中的所有数据，这个计算过程非常耗时。如下图(a)所示，分区是由落在Hash环上的虚拟节点<em>Ti</em>来划分的，并且分区位置(存储分区数据的节点)也同虚拟节点相关，即存储到其顺时针方向的第1个虚拟节点。</p>\n<p><img src=\"http://images.cnitblog.com/blog/571795/201409/051618049539276.jpg\" alt=\"\"></p>\n<p>在Dynamo的论文中提出了分离分区和分区位置的方法来解决这个问题。该方法将Hash空间划分成固定的若干个分区，虚拟节点不再用于划分分区而用来确定分区的存储位置。如上图(b)所示，将Hash空间划分成[A,B]，[B,C], [C,D]和[D,A]四个固定的分区。虚拟节点用于确定分区位置，例如T1负责分区[B,C]，T2负责分区[C,D]，T0负责[D,A]和[A,B]两个分区。由于分区固定，因此迁移数据时可以很容易知道哪些数据需要迁移哪些数据不需要迁移。</p>\n<p>上图(b)中虚拟节点T0负责了[D,A]和[A,B]两个分区的数据，这是由分区数目和虚拟节点数目不相同导致的。为让分区分布地更加均匀，Dyanmo提出了维持分区数目和虚拟节点数目相等的方法。这样每个虚拟节点负责一个分区，在物理节点的磁盘容量都相同并且虚拟节点数目都相同的情况下，每个物理节点负责的分区大小是完全相同的，从而可以达到最佳的数据分布。</p>\n<h2 id=\"CRUSH算法\"><a href=\"#CRUSH算法\" class=\"headerlink\" title=\"CRUSH算法\"></a>CRUSH算法</h2><p>Ceph分布数据的过程：首先计算数据<em>x</em>的Hash值并将结果和PG数目取余，以得到数据<em>x</em>对应的<em>PG</em>编号。然后，通过CRUSH算法将PG映射到一组OSD中。最后把数据<em>x</em>存放到PG对应的OSD中。这个过程中包含了两次映射，第一次是数据<em>x</em>到PG的映射。如果把PG当作存储节点，那么这和文章开头提到的普通Hash算法一样。不同的是，PG是抽象的存储节点，它不会随着物理节点的加入或则离开而增加或减少，因此数据到PG的映射是稳定的。</p>\n<p><img src=\"http://images.cnitblog.com/blog/571795/201409/051618214693665.jpg\" alt=\"\"></p>\n<p>在这个过程中，PG起到了两个作用：第一个作用是划分数据分区。每个PG管理的数据区间相同，因而数据能够均匀地分布到PG上；第二个作用是充当Dyanmo中Token的角色，即决定分区位置。实际上，这和Dynamo中固定分区数目，以及维持分区数目和虚拟节点数目相等的原则是同一回事。</p>\n<p>在没有多副本的情况下，Dynamo中分区的数据直接存储到Token，而每个Token对应唯一的一个物理存储节点。在多副本(假设副本数目为<em>N</em>)的情况下，分区的数据会存储到连续的<em>N</em>个Token中。但这会引入一个新问题：因为副本必须保持在不同的物理节点，但是如果这组Token中存在两个或多个Token对应到同个物理存储节点，那么就必须要跳过这样的节点。Dynamo采用Preference列表来记录每个分区对应的物理节点。然而，Dynmao论文中没有详述分区的Preference列表如何选取物理节点，以及选取物理节点时该如何隔离故障域等问题。</p>\n<blockquote>\n<p>(osd0, osd1, osd2 … osdn) = CRUSH(x)</p>\n</blockquote>\n<p>Ceph的PG担当起Dynamo中Token、固定分区以及Preference列表的角色，解决的是同样的问题。PG的Acting集合对应于Dynamo的Preference列表。CRUSH算法解决了Dynamo论文中未提及的问题。</p>\n<h3 id=\"OSD层级结构和权重大小\"><a href=\"#OSD层级结构和权重大小\" class=\"headerlink\" title=\"OSD层级结构和权重大小\"></a>OSD层级结构和权重大小</h3><p>CRUSH算法的目的是，为给定的PG(即分区)分配一组存储数据的OSD节点。选择OSD节点的过程，要考虑以下几个因素：</p>\n<p>1) PG在OSD间均匀分布。假设每个OSD的磁盘容量都相同，那么我们希望PG在每个OSD节点上是均匀分布的，也就是说每个OSD节点包含相同数目的PG。假如节点的磁盘容量不等，那么容量大的磁盘的节点能够处理更多数量的PG。<br>2) PG的OSD分布在不同的故障域。因为PG的OSD列表用于保存数据的不同副本，副本分布在不同的OSD中可以降低数据损坏的风险。</p>\n<p><img src=\"http://images.cnitblog.com/blog/571795/201409/051618360636140.jpg\" alt=\"\"></p>\n<p>Ceph使用树型层级结构描述OSD的空间位置以及权重(同磁盘容量相关)大小。如上图所示，层级结构描述了OSD所在主机、主机所在机架以及机架所在机房等空间位置。这些空间位置隐含了故障区域，例如使用不同电源的不同的机架属于不同的故障域。CRUSH能够依据一定的规则将副本放置在不同的故障域。</p>\n<p>OSD节点在层级结构中也被称为Device，它位于层级结构的叶子节点，所有非叶子节点称为Bucket。Bucket拥有不同的类型，如上图所示，所有机架的类型为Rack，所有主机的类型为Host。使用者还可以自己定义Bucket的类型。Device节点的权重代表存储节点的性能，磁盘容量是影响权重大小的重要参数。Bucket节点的权重是其子节点的权重之和。</p>\n<p>CRUSH通过重复执行Take(bucketID)和Select(n, bucketType)两个操作选取副本位置。Take(bucketID)指定从给定的bucketID中选取副本位置，例如可以指定从某台机架上选取副本位置，以实现将不同的副本隔离在不同的故障域; Select(n, bucketType)则在给定的Bucket下选取<em>n</em>个类型为bucketType的Bucket，它选取Bucket主要考虑层级结构中节点的容量，以及当节点离线或者加入时的数据迁移量。</p>\n<h3 id=\"算法流程\"><a href=\"#算法流程\" class=\"headerlink\" title=\"算法流程\"></a>算法流程</h3><p><img src=\"http://images.cnitblog.com/blog/571795/201409/051618505162658.jpg\" alt=\"\"></p>\n<p>上图给出了CRUSH选取副本的流程图。</p>\n<p>bucket: Take操作指定的bucket；<br>type: Select操作指定的Bucket的类型；<br>repnum: Select操作指定的副本数目；             </p>\n<p>rep：当前选择的副本编号；<br>x: 当前选择的PG编号；<br>item: 代表当前被选中的Bucket；<br>c(r, x, in): 代表从Bucket in中为PG x选取第r个副本；<br>collide: 代表当前选中的副本位置item已经被选中，即出现了冲突；<br>reject:  代表当前选中的副本位置item被拒绝，例如，在item已经处于out状态的情况下；             </p>\n<p>ftotal:  在Descent域中选择的失败次数，即选择一个副本位置的总共的失败次数；<br>flocal:  在Local域中选择的失败次数；<br>local_retries:  在Local域选择冲突时的尝试次数；<br>local_fallback_retries:  允许在Local域的总共尝试次数为bucket.size + local_fallback_retires次，以保证遍历完Buckt的所有子节点；<br>tries:  在Descent的最大尝试次数，超过这个次数则放弃这个副本。                </p>\n<p><img src=\"http://images.cnitblog.com/blog/571795/201409/051619033448705.jpg\" alt=\"\"></p>\n<p>当Take操作指定的Bucket和Select操作指定的Bucket类型之间隔着几层Bucket时，算法直接深度优先地进入到目的Bucket的直接父母节点。例如，从根节点开始选择N个Host时，它会深度优先地查找到Rack类型的节点，并在这个节点下选取Host节点。为了方便表述，将Rack的所有子节点标记为Local域，将Take指定的Bucket的子节点标记为Descent域，如上图所示。</p>\n<p>选取过程中出现冲突、过载或者故障时，算法先在Local域内重新选择，尝试有限次数后，如果仍然找不到满足条件的Bucket，那就回到Descent域重新选择。每次重新选择时，修改副本数目为<em>r += ftotal</em>。因此每次选择失败都会递增ftotal，所以可以尽量避免选择时再次选到冲突的节点。</p>\n<h3 id=\"Bucket选取Item算法\"><a href=\"#Bucket选取Item算法\" class=\"headerlink\" title=\"Bucket选取Item算法\"></a>Bucket选取Item算法</h3><p>流程图中的<em>item=c(r,x,in)</em>从给定的Bucket in中选取一个子节点。</p>\n<h3 id=\"CRUSH-rule和POOL的关系\"><a href=\"#CRUSH-rule和POOL的关系\" class=\"headerlink\" title=\"CRUSH rule和POOL的关系\"></a>CRUSH rule和POOL的关系</h3><p>(待续)</p>\n<p>参考资料</p>\n<p>1、<a href=\"http://way4ever.com/?p=122\" target=\"_blank\" rel=\"external\">Ceph的CRUSH数据分布算法介绍</a><br>2、CRUSH:Controlled,Scalable,Decentralized Placement of Replicated Data<br>3、Dynamo:Amazon Highly Available Key-value Store</p>\n<hr>\n<p><strong>版权声明</strong></p>\n<p>本作品采用<a href=\"https://creativecommons.org/licenses/by/2.5/cn/\" target=\"_blank\" rel=\"external\">知识共享署名2.5 中国大陆许可协议</a>进行许可，欢迎转载，但转载请注明来自<a href=\"https://wuxiangwei.cn/\"><em>Shanno’s Blog</em></a>，并保持转载后文章内容的完整。本人保留所有版权相关权利，侵权必究。</p>\n","excerpt":"<h2 id=\"引言\"><a href=\"#引言\" class=\"headerlink\" title=\"引言\"></a>引言</h2><p>数据分布是分布式存储系统的一个重要部分，数据分布算法至少要考虑以下三个因素：</p>\n<ul>\n<li>故障域隔离。同份数据的不同副本分布在不同的故障域，降低数据损坏的风险；</li>\n<li>负载均衡。数据能够均匀地分布在磁盘容量不等的存储节点，避免部分节点空闲部分节点超载，从而影响系统性能；</li>\n<li>控制节点加入离开时引起的数据迁移量。当节点离开时，最优的数据迁移是只有离线节点上的数据被迁移到其它节点，而正常工作的节点的数据不会发生迁移。</li>\n</ul>\n<p>对象存储中一致性Hash和Ceph的CRUSH算法是使用地比较多的数据分布算法。在Aamzon的Dyanmo键值存储系统中采用一致性Hash算法，并且对它做了很多优化。OpenStack的Swift对象存储系统也使用了一致性Hash算法。</p>","more":"<h2 id=\"一致性Hash算法\"><a href=\"#一致性Hash算法\" class=\"headerlink\" title=\"一致性Hash算法\"></a>一致性Hash算法</h2><p>假设数据为<em>x</em>，存储节点数目为<em>N</em>。将数据分布到存储节点的最直接做法是，计算数据<em>x</em>的Hash值，并将结果同节点数目<em>N</em>取余数，余数就是数据x的目的存储节点。即目的存储节点为 <em>Hash(x) % N</em>。对数据计算Hash值的目的为了可以让数据均匀分布在N个节点中。这种做法的一个严重问题是，当加入新节点或则节点离开时，几乎所有数据都会受到影响，需要重新分布。因此，数据迁移量非常大。</p>\n<p><img src=\"http://images.cnitblog.com/blog/571795/201409/051617221721380.jpg\" alt=\"\"></p>\n<p>一致性Hash算法将数据和存储节点映射到同个Hash空间，如上图所示。Hash环中的3存储节点把Hash空间划分成3个分区，每个存储节点负责一个分区上的数据。例如，落在分区[N2,N0]上的数据存储在节点N0。</p>\n<p>一致性Hash算法能够很好地控制节点加入离开导致的迁移数据的数量。如图(b)所示，当节点N0离开时，原来由它负责的[N2, N0]分区将同[N0, N1]分区合并成[N2, N1]分区，并且都由节点N1负责。也就是说，本来存储在节点N0上的数据都迁移到节点N1，而原来存储在N1和N2节点的数据不受影响。图(c)给出了当节点N3加入时，原来[N2, N0]分区分裂成[N3, N0]和[N2, N3]两个分区，其中[N3, N0]分区上是数据迁移到新加入的N3节点。</p>\n<h3 id=\"虚拟节点\"><a href=\"#虚拟节点\" class=\"headerlink\" title=\"虚拟节点\"></a>虚拟节点</h3><p>一致性Hash的一个问题是，存储节点不能将Hash空间划分地足够均匀。如上图(a)所示，分区[N2, N0]的大小几乎是其它两个分区大小之和。这容易让负责该分区的节点N0负载过重。假设3个节点的磁盘容量相等，那么当节点N0的磁盘已经写满数据时其它两个节点上的磁盘还有很大的空闲空间，但此时系统已经无法继续向分区[N2, N0]写入数据，从而造成资源浪费。</p>\n<p><img src=\"http://images.cnitblog.com/blog/571795/201409/051617496108430.jpg\" alt=\"\"></p>\n<p>虚拟节点是相对于物理存储节点而言的，虚拟节点负责的分区上的数据最终存储到其对应的物理节点。在一致性Hash中引入虚拟节点可以把Hash空间划分成更多的分区，从而让数据在存储节点上的分布更加均匀。如上图(b)所示，黄颜色的节点代表虚拟节点，Ni_0代表该虚拟节点对应于物理节点i的第0个虚拟节点。增加虚拟节点后，物理节点N0负责[N1_0, N0]和[N0, N0_0]两个分区，物理节点N1负责[N0_0, N1]和[N2_0, N1_0]两个分区，物理节点N2负责[N2, N1]和[N2_0, N2]两个分区，三个物理节点负责的总的数据量趋于平衡。</p>\n<p>实际应用中，可以根据物理节点的磁盘容量的大小来确定其对应的虚拟节点数目。虚拟节点数目越多，节点负责的数据区间也越大。</p>\n<h3 id=\"分区与分区位置\"><a href=\"#分区与分区位置\" class=\"headerlink\" title=\"分区与分区位置\"></a>分区与分区位置</h3><p>前文提到，当节点加入或者离开时，分区会相应地进行分裂或合并。这不对新写入的数据构成影响，但对已经写入到磁盘的数据需要重新计算Hash值以确定它是否需要迁移到其它节点。因为需要遍历磁盘中的所有数据，这个计算过程非常耗时。如下图(a)所示，分区是由落在Hash环上的虚拟节点<em>Ti</em>来划分的，并且分区位置(存储分区数据的节点)也同虚拟节点相关，即存储到其顺时针方向的第1个虚拟节点。</p>\n<p><img src=\"http://images.cnitblog.com/blog/571795/201409/051618049539276.jpg\" alt=\"\"></p>\n<p>在Dynamo的论文中提出了分离分区和分区位置的方法来解决这个问题。该方法将Hash空间划分成固定的若干个分区，虚拟节点不再用于划分分区而用来确定分区的存储位置。如上图(b)所示，将Hash空间划分成[A,B]，[B,C], [C,D]和[D,A]四个固定的分区。虚拟节点用于确定分区位置，例如T1负责分区[B,C]，T2负责分区[C,D]，T0负责[D,A]和[A,B]两个分区。由于分区固定，因此迁移数据时可以很容易知道哪些数据需要迁移哪些数据不需要迁移。</p>\n<p>上图(b)中虚拟节点T0负责了[D,A]和[A,B]两个分区的数据，这是由分区数目和虚拟节点数目不相同导致的。为让分区分布地更加均匀，Dyanmo提出了维持分区数目和虚拟节点数目相等的方法。这样每个虚拟节点负责一个分区，在物理节点的磁盘容量都相同并且虚拟节点数目都相同的情况下，每个物理节点负责的分区大小是完全相同的，从而可以达到最佳的数据分布。</p>\n<h2 id=\"CRUSH算法\"><a href=\"#CRUSH算法\" class=\"headerlink\" title=\"CRUSH算法\"></a>CRUSH算法</h2><p>Ceph分布数据的过程：首先计算数据<em>x</em>的Hash值并将结果和PG数目取余，以得到数据<em>x</em>对应的<em>PG</em>编号。然后，通过CRUSH算法将PG映射到一组OSD中。最后把数据<em>x</em>存放到PG对应的OSD中。这个过程中包含了两次映射，第一次是数据<em>x</em>到PG的映射。如果把PG当作存储节点，那么这和文章开头提到的普通Hash算法一样。不同的是，PG是抽象的存储节点，它不会随着物理节点的加入或则离开而增加或减少，因此数据到PG的映射是稳定的。</p>\n<p><img src=\"http://images.cnitblog.com/blog/571795/201409/051618214693665.jpg\" alt=\"\"></p>\n<p>在这个过程中，PG起到了两个作用：第一个作用是划分数据分区。每个PG管理的数据区间相同，因而数据能够均匀地分布到PG上；第二个作用是充当Dyanmo中Token的角色，即决定分区位置。实际上，这和Dynamo中固定分区数目，以及维持分区数目和虚拟节点数目相等的原则是同一回事。</p>\n<p>在没有多副本的情况下，Dynamo中分区的数据直接存储到Token，而每个Token对应唯一的一个物理存储节点。在多副本(假设副本数目为<em>N</em>)的情况下，分区的数据会存储到连续的<em>N</em>个Token中。但这会引入一个新问题：因为副本必须保持在不同的物理节点，但是如果这组Token中存在两个或多个Token对应到同个物理存储节点，那么就必须要跳过这样的节点。Dynamo采用Preference列表来记录每个分区对应的物理节点。然而，Dynmao论文中没有详述分区的Preference列表如何选取物理节点，以及选取物理节点时该如何隔离故障域等问题。</p>\n<blockquote>\n<p>(osd0, osd1, osd2 … osdn) = CRUSH(x)</p>\n</blockquote>\n<p>Ceph的PG担当起Dynamo中Token、固定分区以及Preference列表的角色，解决的是同样的问题。PG的Acting集合对应于Dynamo的Preference列表。CRUSH算法解决了Dynamo论文中未提及的问题。</p>\n<h3 id=\"OSD层级结构和权重大小\"><a href=\"#OSD层级结构和权重大小\" class=\"headerlink\" title=\"OSD层级结构和权重大小\"></a>OSD层级结构和权重大小</h3><p>CRUSH算法的目的是，为给定的PG(即分区)分配一组存储数据的OSD节点。选择OSD节点的过程，要考虑以下几个因素：</p>\n<p>1) PG在OSD间均匀分布。假设每个OSD的磁盘容量都相同，那么我们希望PG在每个OSD节点上是均匀分布的，也就是说每个OSD节点包含相同数目的PG。假如节点的磁盘容量不等，那么容量大的磁盘的节点能够处理更多数量的PG。<br>2) PG的OSD分布在不同的故障域。因为PG的OSD列表用于保存数据的不同副本，副本分布在不同的OSD中可以降低数据损坏的风险。</p>\n<p><img src=\"http://images.cnitblog.com/blog/571795/201409/051618360636140.jpg\" alt=\"\"></p>\n<p>Ceph使用树型层级结构描述OSD的空间位置以及权重(同磁盘容量相关)大小。如上图所示，层级结构描述了OSD所在主机、主机所在机架以及机架所在机房等空间位置。这些空间位置隐含了故障区域，例如使用不同电源的不同的机架属于不同的故障域。CRUSH能够依据一定的规则将副本放置在不同的故障域。</p>\n<p>OSD节点在层级结构中也被称为Device，它位于层级结构的叶子节点，所有非叶子节点称为Bucket。Bucket拥有不同的类型，如上图所示，所有机架的类型为Rack，所有主机的类型为Host。使用者还可以自己定义Bucket的类型。Device节点的权重代表存储节点的性能，磁盘容量是影响权重大小的重要参数。Bucket节点的权重是其子节点的权重之和。</p>\n<p>CRUSH通过重复执行Take(bucketID)和Select(n, bucketType)两个操作选取副本位置。Take(bucketID)指定从给定的bucketID中选取副本位置，例如可以指定从某台机架上选取副本位置，以实现将不同的副本隔离在不同的故障域; Select(n, bucketType)则在给定的Bucket下选取<em>n</em>个类型为bucketType的Bucket，它选取Bucket主要考虑层级结构中节点的容量，以及当节点离线或者加入时的数据迁移量。</p>\n<h3 id=\"算法流程\"><a href=\"#算法流程\" class=\"headerlink\" title=\"算法流程\"></a>算法流程</h3><p><img src=\"http://images.cnitblog.com/blog/571795/201409/051618505162658.jpg\" alt=\"\"></p>\n<p>上图给出了CRUSH选取副本的流程图。</p>\n<p>bucket: Take操作指定的bucket；<br>type: Select操作指定的Bucket的类型；<br>repnum: Select操作指定的副本数目；             </p>\n<p>rep：当前选择的副本编号；<br>x: 当前选择的PG编号；<br>item: 代表当前被选中的Bucket；<br>c(r, x, in): 代表从Bucket in中为PG x选取第r个副本；<br>collide: 代表当前选中的副本位置item已经被选中，即出现了冲突；<br>reject:  代表当前选中的副本位置item被拒绝，例如，在item已经处于out状态的情况下；             </p>\n<p>ftotal:  在Descent域中选择的失败次数，即选择一个副本位置的总共的失败次数；<br>flocal:  在Local域中选择的失败次数；<br>local_retries:  在Local域选择冲突时的尝试次数；<br>local_fallback_retries:  允许在Local域的总共尝试次数为bucket.size + local_fallback_retires次，以保证遍历完Buckt的所有子节点；<br>tries:  在Descent的最大尝试次数，超过这个次数则放弃这个副本。                </p>\n<p><img src=\"http://images.cnitblog.com/blog/571795/201409/051619033448705.jpg\" alt=\"\"></p>\n<p>当Take操作指定的Bucket和Select操作指定的Bucket类型之间隔着几层Bucket时，算法直接深度优先地进入到目的Bucket的直接父母节点。例如，从根节点开始选择N个Host时，它会深度优先地查找到Rack类型的节点，并在这个节点下选取Host节点。为了方便表述，将Rack的所有子节点标记为Local域，将Take指定的Bucket的子节点标记为Descent域，如上图所示。</p>\n<p>选取过程中出现冲突、过载或者故障时，算法先在Local域内重新选择，尝试有限次数后，如果仍然找不到满足条件的Bucket，那就回到Descent域重新选择。每次重新选择时，修改副本数目为<em>r += ftotal</em>。因此每次选择失败都会递增ftotal，所以可以尽量避免选择时再次选到冲突的节点。</p>\n<h3 id=\"Bucket选取Item算法\"><a href=\"#Bucket选取Item算法\" class=\"headerlink\" title=\"Bucket选取Item算法\"></a>Bucket选取Item算法</h3><p>流程图中的<em>item=c(r,x,in)</em>从给定的Bucket in中选取一个子节点。</p>\n<h3 id=\"CRUSH-rule和POOL的关系\"><a href=\"#CRUSH-rule和POOL的关系\" class=\"headerlink\" title=\"CRUSH rule和POOL的关系\"></a>CRUSH rule和POOL的关系</h3><p>(待续)</p>\n<p>参考资料</p>\n<p>1、<a href=\"http://way4ever.com/?p=122\">Ceph的CRUSH数据分布算法介绍</a><br>2、CRUSH:Controlled,Scalable,Decentralized Placement of Replicated Data<br>3、Dynamo:Amazon Highly Available Key-value Store</p>\n<hr>\n<p><strong>版权声明</strong></p>\n<p>本作品采用<a href=\"https://creativecommons.org/licenses/by/2.5/cn/\">知识共享署名2.5 中国大陆许可协议</a>进行许可，欢迎转载，但转载请注明来自<a href=\"https://wuxiangwei.cn/\"><em>Shanno’s Blog</em></a>，并保持转载后文章内容的完整。本人保留所有版权相关权利，侵权必究。</p>"},{"title":"Ceph消息处理","date":"2014-10-09T13:10:31.000Z","toc":true,"_content":"\n\n总体上，Ceph的消息处理框架是发布者订阅者的设计结构。Messenger担当发布者的角色，Dispatcher担当订阅者的角色。Messenger将接收到的消息通知给已注册的Dispatcher，由Dispatcher完成具体的消息处理。\n\n<!--more-->\n\n在服务端，SimpleMessenger通过Accepter实例监听端口，接收来自客户端的连接。Accepter接受客户端的连接后，为该连接创建一个Pipe实例。Pipe实例负责具体消息的接收和发送，一个Pipe实例包含一个读线程和一个写线程。读线程读取到消息后，有三种分发消息的方法：\n\n1. **快速分发**，直接在Pipe的读线程中处理掉消息。可快速分发的消息在Dispatcher的*ms_can_fast_dispatch*中注册。\n2. **正常分发**，将消息放入DispatchQueue，由单独的线程按照消息的优先级从高到低进行分发处理。需要注意的是，属于同个SimpleMessenger实例的Pipe间使用同个DispatchQueue。\n3. **延迟分发**，为消息随机设置延迟时间，定时时间到时由单独的线程走快速分发或正常分发的流程分发消息。\n\nPipe的写线程将消息放入out_q队列，按照消息的优先级从高到低发送消息。另外，消息(Message)中携带了seq序列号，Pipe使用in_seq和out_seq记录它接收到和发送出去的消息的序列号。发送消息时，Pipe用out_seq设置消息的序列号；接收消息时，通过比较消息的序列号和in_seq来确定消息是否为旧消息，如果为旧消息则丢弃，否则使用消息的序列号更新in_seq。\n\n","source":"_posts/Ceph/message.md","raw":"---\ntitle: \"Ceph消息处理\"\ndate: 2014-10-09 21:10:31\ncategories: [Ceph]\ntoc: true\n---\n\n\n总体上，Ceph的消息处理框架是发布者订阅者的设计结构。Messenger担当发布者的角色，Dispatcher担当订阅者的角色。Messenger将接收到的消息通知给已注册的Dispatcher，由Dispatcher完成具体的消息处理。\n\n<!--more-->\n\n在服务端，SimpleMessenger通过Accepter实例监听端口，接收来自客户端的连接。Accepter接受客户端的连接后，为该连接创建一个Pipe实例。Pipe实例负责具体消息的接收和发送，一个Pipe实例包含一个读线程和一个写线程。读线程读取到消息后，有三种分发消息的方法：\n\n1. **快速分发**，直接在Pipe的读线程中处理掉消息。可快速分发的消息在Dispatcher的*ms_can_fast_dispatch*中注册。\n2. **正常分发**，将消息放入DispatchQueue，由单独的线程按照消息的优先级从高到低进行分发处理。需要注意的是，属于同个SimpleMessenger实例的Pipe间使用同个DispatchQueue。\n3. **延迟分发**，为消息随机设置延迟时间，定时时间到时由单独的线程走快速分发或正常分发的流程分发消息。\n\nPipe的写线程将消息放入out_q队列，按照消息的优先级从高到低发送消息。另外，消息(Message)中携带了seq序列号，Pipe使用in_seq和out_seq记录它接收到和发送出去的消息的序列号。发送消息时，Pipe用out_seq设置消息的序列号；接收消息时，通过比较消息的序列号和in_seq来确定消息是否为旧消息，如果为旧消息则丢弃，否则使用消息的序列号更新in_seq。\n\n","slug":"Ceph/message","published":1,"updated":"2016-12-15T07:18:40.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cix1kgato0008daeiigojygvl","content":"<p>总体上，Ceph的消息处理框架是发布者订阅者的设计结构。Messenger担当发布者的角色，Dispatcher担当订阅者的角色。Messenger将接收到的消息通知给已注册的Dispatcher，由Dispatcher完成具体的消息处理。</p>\n<a id=\"more\"></a>\n<p>在服务端，SimpleMessenger通过Accepter实例监听端口，接收来自客户端的连接。Accepter接受客户端的连接后，为该连接创建一个Pipe实例。Pipe实例负责具体消息的接收和发送，一个Pipe实例包含一个读线程和一个写线程。读线程读取到消息后，有三种分发消息的方法：</p>\n<ol>\n<li><strong>快速分发</strong>，直接在Pipe的读线程中处理掉消息。可快速分发的消息在Dispatcher的<em>ms_can_fast_dispatch</em>中注册。</li>\n<li><strong>正常分发</strong>，将消息放入DispatchQueue，由单独的线程按照消息的优先级从高到低进行分发处理。需要注意的是，属于同个SimpleMessenger实例的Pipe间使用同个DispatchQueue。</li>\n<li><strong>延迟分发</strong>，为消息随机设置延迟时间，定时时间到时由单独的线程走快速分发或正常分发的流程分发消息。</li>\n</ol>\n<p>Pipe的写线程将消息放入out_q队列，按照消息的优先级从高到低发送消息。另外，消息(Message)中携带了seq序列号，Pipe使用in_seq和out_seq记录它接收到和发送出去的消息的序列号。发送消息时，Pipe用out_seq设置消息的序列号；接收消息时，通过比较消息的序列号和in_seq来确定消息是否为旧消息，如果为旧消息则丢弃，否则使用消息的序列号更新in_seq。</p>\n<hr>\n<p><strong>版权声明</strong></p>\n<p>本作品采用<a href=\"https://creativecommons.org/licenses/by/2.5/cn/\" target=\"_blank\" rel=\"external\">知识共享署名2.5 中国大陆许可协议</a>进行许可，欢迎转载，但转载请注明来自<a href=\"https://wuxiangwei.cn/\"><em>Shanno’s Blog</em></a>，并保持转载后文章内容的完整。本人保留所有版权相关权利，侵权必究。</p>\n","excerpt":"<p>总体上，Ceph的消息处理框架是发布者订阅者的设计结构。Messenger担当发布者的角色，Dispatcher担当订阅者的角色。Messenger将接收到的消息通知给已注册的Dispatcher，由Dispatcher完成具体的消息处理。</p>","more":"<p>在服务端，SimpleMessenger通过Accepter实例监听端口，接收来自客户端的连接。Accepter接受客户端的连接后，为该连接创建一个Pipe实例。Pipe实例负责具体消息的接收和发送，一个Pipe实例包含一个读线程和一个写线程。读线程读取到消息后，有三种分发消息的方法：</p>\n<ol>\n<li><strong>快速分发</strong>，直接在Pipe的读线程中处理掉消息。可快速分发的消息在Dispatcher的<em>ms_can_fast_dispatch</em>中注册。</li>\n<li><strong>正常分发</strong>，将消息放入DispatchQueue，由单独的线程按照消息的优先级从高到低进行分发处理。需要注意的是，属于同个SimpleMessenger实例的Pipe间使用同个DispatchQueue。</li>\n<li><strong>延迟分发</strong>，为消息随机设置延迟时间，定时时间到时由单独的线程走快速分发或正常分发的流程分发消息。</li>\n</ol>\n<p>Pipe的写线程将消息放入out_q队列，按照消息的优先级从高到低发送消息。另外，消息(Message)中携带了seq序列号，Pipe使用in_seq和out_seq记录它接收到和发送出去的消息的序列号。发送消息时，Pipe用out_seq设置消息的序列号；接收消息时，通过比较消息的序列号和in_seq来确定消息是否为旧消息，如果为旧消息则丢弃，否则使用消息的序列号更新in_seq。</p>\n<hr>\n<p><strong>版权声明</strong></p>\n<p>本作品采用<a href=\"https://creativecommons.org/licenses/by/2.5/cn/\">知识共享署名2.5 中国大陆许可协议</a>进行许可，欢迎转载，但转载请注明来自<a href=\"https://wuxiangwei.cn/\"><em>Shanno’s Blog</em></a>，并保持转载后文章内容的完整。本人保留所有版权相关权利，侵权必究。</p>"},{"title":"Context设计模式","date":"2014-09-12T01:05:45.000Z","_content":"\nCeph实现中使用了大量派生于Context抽象类的子类，用法简单却很巧妙，我将它称为Context模式或者上下文模式。上下文模式像极了C的回调函数，而回调函数又像极了Template模式，但Template模式和Context模式似乎有所不同。\n\n<!--more-->\n\n回调函数的使用场景是，函数的实现者不知道何时调用函数，而函数的调用者不知道如何实现函数。回调的实现者实现由调用者提供的函数接口，并将其注册给调用者。调用者在适当的时机调用函数完成实现者指定的功能。\n\n从“你不用调用我，让我调用你”的角度看，Template模式是ODD中的回调函数。Template模式中，父类充当回调中的调用者角色，子类充当实现者角色。一般地，父类定义了程序的主要逻辑功能，而子类只用于填充逻辑中不稳定或者可以多样化的部分，这部分的内容通过多态机制来调用。\n\nTemplate模式中一个很重要的因素是，**调用者父类和实现者子类共享同一个上下文信息**。但是，如果父类是重量级的并且子类实现回调接口使用的上下文内容同父类的上下文风马牛不相及时，就有必要将二者分离。回头看，C的回调和Template模式之间的区别：**C回调的调用者和实现者是属于两个实体的，而Template模式将实现者和调用者放到了一个实体中。Context模式又从Template模式中将这两者分离开来**。\n\n```C++\nclass Context {\nprotected:\n  virtual void finish(int r) = 0; /// 派生类实现接口\n\npublic:\n  virtual void complete(int r) {  /// 调用者调用接口，函数执行结束后销毁对象\n    finish(r); /// r用于让子类判断是否执行，r为0时执行，不为0时放弃执行\n    delete this;\n  }\n};\n```\n从上文的分析可知，Context模式和Template模式的区别是，调用者和实现者是否共享上下文信息。Context类将实现者的上下文信息封装在子类中，提供给调用者*void complete(int r)*方法并且授予其是否执行回调的权利。因为将实现者的上下文信息封装在子类，因此可以不必像C回调那样设计各种不同的回调函数类型，从而统一了所有的回调接口。\n\n```C++\nclass C_SafeCond : public Context {\n  bool *done;     /// true after finish() has been called\npublic:\n  C_SafeCond(bool *d ) : done(d) { /// 主要这里引用的是done的地址\n    *done = false;\n  }\n  void finish(int r) { /// 执行complete后，设置done为true。\n    *done = true;\n  }\n}\n\n/// 从远程OSD中同步读取Object数据\nint librados::IoCtxImpl::operate_read(const object_t& oid, ::ObjectOperation *o, bufferlist *pbl, int flags)\n{\n  bool done;\n\n  Context *onack = new C_SafeCond(&mylock, &cond, &done, &r);\n\n  Objecter::Op *objecter_op = objecter->prepare_read_op(oid, oloc, *o, snap_seq, pbl, flags, onack, &ver);\n  objecter->op_submit(objecter_op); /// 发送读object请求，收到对该请求的回复后，执行onack回调\n\n  while (!done){  /// 线程阻塞直到onack被执行\n    cond.Wait(mylock);\n  }\n\n  return r;\n}\n```\n通常情况下，Context模式适用于异步执行环境。但上面的例子说明，它也可以用于实现同步等待。\n\n","source":"_posts/Ceph/ood-context.md","raw":"---\ntitle: Context设计模式\ndate: 2014-09-12 09:05:45\ncategories: [Ceph]\ntags: [OOD, Ceph]\n---\n\nCeph实现中使用了大量派生于Context抽象类的子类，用法简单却很巧妙，我将它称为Context模式或者上下文模式。上下文模式像极了C的回调函数，而回调函数又像极了Template模式，但Template模式和Context模式似乎有所不同。\n\n<!--more-->\n\n回调函数的使用场景是，函数的实现者不知道何时调用函数，而函数的调用者不知道如何实现函数。回调的实现者实现由调用者提供的函数接口，并将其注册给调用者。调用者在适当的时机调用函数完成实现者指定的功能。\n\n从“你不用调用我，让我调用你”的角度看，Template模式是ODD中的回调函数。Template模式中，父类充当回调中的调用者角色，子类充当实现者角色。一般地，父类定义了程序的主要逻辑功能，而子类只用于填充逻辑中不稳定或者可以多样化的部分，这部分的内容通过多态机制来调用。\n\nTemplate模式中一个很重要的因素是，**调用者父类和实现者子类共享同一个上下文信息**。但是，如果父类是重量级的并且子类实现回调接口使用的上下文内容同父类的上下文风马牛不相及时，就有必要将二者分离。回头看，C的回调和Template模式之间的区别：**C回调的调用者和实现者是属于两个实体的，而Template模式将实现者和调用者放到了一个实体中。Context模式又从Template模式中将这两者分离开来**。\n\n```C++\nclass Context {\nprotected:\n  virtual void finish(int r) = 0; /// 派生类实现接口\n\npublic:\n  virtual void complete(int r) {  /// 调用者调用接口，函数执行结束后销毁对象\n    finish(r); /// r用于让子类判断是否执行，r为0时执行，不为0时放弃执行\n    delete this;\n  }\n};\n```\n从上文的分析可知，Context模式和Template模式的区别是，调用者和实现者是否共享上下文信息。Context类将实现者的上下文信息封装在子类中，提供给调用者*void complete(int r)*方法并且授予其是否执行回调的权利。因为将实现者的上下文信息封装在子类，因此可以不必像C回调那样设计各种不同的回调函数类型，从而统一了所有的回调接口。\n\n```C++\nclass C_SafeCond : public Context {\n  bool *done;     /// true after finish() has been called\npublic:\n  C_SafeCond(bool *d ) : done(d) { /// 主要这里引用的是done的地址\n    *done = false;\n  }\n  void finish(int r) { /// 执行complete后，设置done为true。\n    *done = true;\n  }\n}\n\n/// 从远程OSD中同步读取Object数据\nint librados::IoCtxImpl::operate_read(const object_t& oid, ::ObjectOperation *o, bufferlist *pbl, int flags)\n{\n  bool done;\n\n  Context *onack = new C_SafeCond(&mylock, &cond, &done, &r);\n\n  Objecter::Op *objecter_op = objecter->prepare_read_op(oid, oloc, *o, snap_seq, pbl, flags, onack, &ver);\n  objecter->op_submit(objecter_op); /// 发送读object请求，收到对该请求的回复后，执行onack回调\n\n  while (!done){  /// 线程阻塞直到onack被执行\n    cond.Wait(mylock);\n  }\n\n  return r;\n}\n```\n通常情况下，Context模式适用于异步执行环境。但上面的例子说明，它也可以用于实现同步等待。\n\n","slug":"Ceph/ood-context","published":1,"updated":"2016-12-15T07:18:40.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cix1kgatr000adaei5qxqpn13","content":"<p>Ceph实现中使用了大量派生于Context抽象类的子类，用法简单却很巧妙，我将它称为Context模式或者上下文模式。上下文模式像极了C的回调函数，而回调函数又像极了Template模式，但Template模式和Context模式似乎有所不同。</p>\n<a id=\"more\"></a>\n<p>回调函数的使用场景是，函数的实现者不知道何时调用函数，而函数的调用者不知道如何实现函数。回调的实现者实现由调用者提供的函数接口，并将其注册给调用者。调用者在适当的时机调用函数完成实现者指定的功能。</p>\n<p>从“你不用调用我，让我调用你”的角度看，Template模式是ODD中的回调函数。Template模式中，父类充当回调中的调用者角色，子类充当实现者角色。一般地，父类定义了程序的主要逻辑功能，而子类只用于填充逻辑中不稳定或者可以多样化的部分，这部分的内容通过多态机制来调用。</p>\n<p>Template模式中一个很重要的因素是，<strong>调用者父类和实现者子类共享同一个上下文信息</strong>。但是，如果父类是重量级的并且子类实现回调接口使用的上下文内容同父类的上下文风马牛不相及时，就有必要将二者分离。回头看，C的回调和Template模式之间的区别：<strong>C回调的调用者和实现者是属于两个实体的，而Template模式将实现者和调用者放到了一个实体中。Context模式又从Template模式中将这两者分离开来</strong>。</p>\n<figure class=\"highlight c++\"><table><tr><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">class</span> Context &#123;</div><div class=\"line\"><span class=\"keyword\">protected</span>:</div><div class=\"line\">  <span class=\"function\"><span class=\"keyword\">virtual</span> <span class=\"keyword\">void</span> <span class=\"title\">finish</span><span class=\"params\">(<span class=\"keyword\">int</span> r)</span> </span>= <span class=\"number\">0</span>; <span class=\"comment\">/// 派生类实现接口</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"keyword\">public</span>:</div><div class=\"line\">  <span class=\"function\"><span class=\"keyword\">virtual</span> <span class=\"keyword\">void</span> <span class=\"title\">complete</span><span class=\"params\">(<span class=\"keyword\">int</span> r)</span> </span>&#123;  <span class=\"comment\">/// 调用者调用接口，函数执行结束后销毁对象</span></div><div class=\"line\">    finish(r); <span class=\"comment\">/// r用于让子类判断是否执行，r为0时执行，不为0时放弃执行</span></div><div class=\"line\">    <span class=\"keyword\">delete</span> <span class=\"keyword\">this</span>;</div><div class=\"line\">  &#125;</div><div class=\"line\">&#125;;</div></pre></td></tr></table></figure>\n<p>从上文的分析可知，Context模式和Template模式的区别是，调用者和实现者是否共享上下文信息。Context类将实现者的上下文信息封装在子类中，提供给调用者<em>void complete(int r)</em>方法并且授予其是否执行回调的权利。因为将实现者的上下文信息封装在子类，因此可以不必像C回调那样设计各种不同的回调函数类型，从而统一了所有的回调接口。</p>\n<figure class=\"highlight c++\"><table><tr><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">class</span> C_SafeCond : <span class=\"keyword\">public</span> Context &#123;</div><div class=\"line\">  <span class=\"keyword\">bool</span> *done;     <span class=\"comment\">/// true after finish() has been called</span></div><div class=\"line\"><span class=\"keyword\">public</span>:</div><div class=\"line\">  C_SafeCond(<span class=\"keyword\">bool</span> *d ) : done(d) &#123; <span class=\"comment\">/// 主要这里引用的是done的地址</span></div><div class=\"line\">    *done = <span class=\"literal\">false</span>;</div><div class=\"line\">  &#125;</div><div class=\"line\">  <span class=\"function\"><span class=\"keyword\">void</span> <span class=\"title\">finish</span><span class=\"params\">(<span class=\"keyword\">int</span> r)</span> </span>&#123; <span class=\"comment\">/// 执行complete后，设置done为true。</span></div><div class=\"line\">    *done = <span class=\"literal\">true</span>;</div><div class=\"line\">  &#125;</div><div class=\"line\">&#125;</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">/// 从远程OSD中同步读取Object数据</span></div><div class=\"line\"><span class=\"keyword\">int</span> librados::IoCtxImpl::operate_read(<span class=\"keyword\">const</span> <span class=\"keyword\">object_t</span>&amp; oid, ::ObjectOperation *o, bufferlist *pbl, <span class=\"keyword\">int</span> flags)</div><div class=\"line\">&#123;</div><div class=\"line\">  <span class=\"keyword\">bool</span> done;</div><div class=\"line\"></div><div class=\"line\">  Context *onack = <span class=\"keyword\">new</span> C_SafeCond(&amp;mylock, &amp;cond, &amp;done, &amp;r);</div><div class=\"line\"></div><div class=\"line\">  Objecter::Op *objecter_op = objecter-&gt;prepare_read_op(oid, oloc, *o, snap_seq, pbl, flags, onack, &amp;ver);</div><div class=\"line\">  objecter-&gt;op_submit(objecter_op); <span class=\"comment\">/// 发送读object请求，收到对该请求的回复后，执行onack回调</span></div><div class=\"line\"></div><div class=\"line\">  <span class=\"keyword\">while</span> (!done)&#123;  <span class=\"comment\">/// 线程阻塞直到onack被执行</span></div><div class=\"line\">    cond.Wait(mylock);</div><div class=\"line\">  &#125;</div><div class=\"line\"></div><div class=\"line\">  <span class=\"keyword\">return</span> r;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<p>通常情况下，Context模式适用于异步执行环境。但上面的例子说明，它也可以用于实现同步等待。</p>\n<hr>\n<p><strong>版权声明</strong></p>\n<p>本作品采用<a href=\"https://creativecommons.org/licenses/by/2.5/cn/\" target=\"_blank\" rel=\"external\">知识共享署名2.5 中国大陆许可协议</a>进行许可，欢迎转载，但转载请注明来自<a href=\"https://wuxiangwei.cn/\"><em>Shanno’s Blog</em></a>，并保持转载后文章内容的完整。本人保留所有版权相关权利，侵权必究。</p>\n","excerpt":"<p>Ceph实现中使用了大量派生于Context抽象类的子类，用法简单却很巧妙，我将它称为Context模式或者上下文模式。上下文模式像极了C的回调函数，而回调函数又像极了Template模式，但Template模式和Context模式似乎有所不同。</p>","more":"<p>回调函数的使用场景是，函数的实现者不知道何时调用函数，而函数的调用者不知道如何实现函数。回调的实现者实现由调用者提供的函数接口，并将其注册给调用者。调用者在适当的时机调用函数完成实现者指定的功能。</p>\n<p>从“你不用调用我，让我调用你”的角度看，Template模式是ODD中的回调函数。Template模式中，父类充当回调中的调用者角色，子类充当实现者角色。一般地，父类定义了程序的主要逻辑功能，而子类只用于填充逻辑中不稳定或者可以多样化的部分，这部分的内容通过多态机制来调用。</p>\n<p>Template模式中一个很重要的因素是，<strong>调用者父类和实现者子类共享同一个上下文信息</strong>。但是，如果父类是重量级的并且子类实现回调接口使用的上下文内容同父类的上下文风马牛不相及时，就有必要将二者分离。回头看，C的回调和Template模式之间的区别：<strong>C回调的调用者和实现者是属于两个实体的，而Template模式将实现者和调用者放到了一个实体中。Context模式又从Template模式中将这两者分离开来</strong>。</p>\n<figure class=\"highlight c++\"><table><tr><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">class</span> Context &#123;</div><div class=\"line\"><span class=\"keyword\">protected</span>:</div><div class=\"line\">  <span class=\"function\"><span class=\"keyword\">virtual</span> <span class=\"keyword\">void</span> <span class=\"title\">finish</span><span class=\"params\">(<span class=\"keyword\">int</span> r)</span> </span>= <span class=\"number\">0</span>; <span class=\"comment\">/// 派生类实现接口</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"keyword\">public</span>:</div><div class=\"line\">  <span class=\"function\"><span class=\"keyword\">virtual</span> <span class=\"keyword\">void</span> <span class=\"title\">complete</span><span class=\"params\">(<span class=\"keyword\">int</span> r)</span> </span>&#123;  <span class=\"comment\">/// 调用者调用接口，函数执行结束后销毁对象</span></div><div class=\"line\">    finish(r); <span class=\"comment\">/// r用于让子类判断是否执行，r为0时执行，不为0时放弃执行</span></div><div class=\"line\">    <span class=\"keyword\">delete</span> <span class=\"keyword\">this</span>;</div><div class=\"line\">  &#125;</div><div class=\"line\">&#125;;</div></pre></td></tr></table></figure>\n<p>从上文的分析可知，Context模式和Template模式的区别是，调用者和实现者是否共享上下文信息。Context类将实现者的上下文信息封装在子类中，提供给调用者<em>void complete(int r)</em>方法并且授予其是否执行回调的权利。因为将实现者的上下文信息封装在子类，因此可以不必像C回调那样设计各种不同的回调函数类型，从而统一了所有的回调接口。</p>\n<figure class=\"highlight c++\"><table><tr><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">class</span> C_SafeCond : <span class=\"keyword\">public</span> Context &#123;</div><div class=\"line\">  <span class=\"keyword\">bool</span> *done;     <span class=\"comment\">/// true after finish() has been called</span></div><div class=\"line\"><span class=\"keyword\">public</span>:</div><div class=\"line\">  C_SafeCond(<span class=\"keyword\">bool</span> *d ) : done(d) &#123; <span class=\"comment\">/// 主要这里引用的是done的地址</span></div><div class=\"line\">    *done = <span class=\"literal\">false</span>;</div><div class=\"line\">  &#125;</div><div class=\"line\">  <span class=\"function\"><span class=\"keyword\">void</span> <span class=\"title\">finish</span><span class=\"params\">(<span class=\"keyword\">int</span> r)</span> </span>&#123; <span class=\"comment\">/// 执行complete后，设置done为true。</span></div><div class=\"line\">    *done = <span class=\"literal\">true</span>;</div><div class=\"line\">  &#125;</div><div class=\"line\">&#125;</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">/// 从远程OSD中同步读取Object数据</span></div><div class=\"line\"><span class=\"keyword\">int</span> librados::IoCtxImpl::operate_read(<span class=\"keyword\">const</span> <span class=\"keyword\">object_t</span>&amp; oid, ::ObjectOperation *o, bufferlist *pbl, <span class=\"keyword\">int</span> flags)</div><div class=\"line\">&#123;</div><div class=\"line\">  <span class=\"keyword\">bool</span> done;</div><div class=\"line\"></div><div class=\"line\">  Context *onack = <span class=\"keyword\">new</span> C_SafeCond(&amp;mylock, &amp;cond, &amp;done, &amp;r);</div><div class=\"line\"></div><div class=\"line\">  Objecter::Op *objecter_op = objecter-&gt;prepare_read_op(oid, oloc, *o, snap_seq, pbl, flags, onack, &amp;ver);</div><div class=\"line\">  objecter-&gt;op_submit(objecter_op); <span class=\"comment\">/// 发送读object请求，收到对该请求的回复后，执行onack回调</span></div><div class=\"line\"></div><div class=\"line\">  <span class=\"keyword\">while</span> (!done)&#123;  <span class=\"comment\">/// 线程阻塞直到onack被执行</span></div><div class=\"line\">    cond.Wait(mylock);</div><div class=\"line\">  &#125;</div><div class=\"line\"></div><div class=\"line\">  <span class=\"keyword\">return</span> r;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<p>通常情况下，Context模式适用于异步执行环境。但上面的例子说明，它也可以用于实现同步等待。</p>\n<hr>\n<p><strong>版权声明</strong></p>\n<p>本作品采用<a href=\"https://creativecommons.org/licenses/by/2.5/cn/\">知识共享署名2.5 中国大陆许可协议</a>进行许可，欢迎转载，但转载请注明来自<a href=\"https://wuxiangwei.cn/\"><em>Shanno’s Blog</em></a>，并保持转载后文章内容的完整。本人保留所有版权相关权利，侵权必究。</p>"},{"title":"Ceph的Paxos算法实现","date":"2014-10-08T12:07:09.000Z","toc":true,"_content":"\nCeph使用Paxos算法来解决Monitor间数据的一致性问题。本文介绍的内容位于Leader选举之后，被推选为Leader的Montior节点从其它节点中收集状态信息，统一Quoruma成员的状态，然后进入正常工作的流程。\n\n<!--more-->\n\n##Recovery阶段\n\n在Leader选举成功后，Leader和Peon都进入Recovery阶段。该阶段的目的是为了保证新Quorum的所有成员状态一致，这些状态包括：最后一个批准(Committed)的提案，最后一个没批准的提案，最后一个接受(Acceppted)的提案。每个节点的这些状态都持久化到磁盘。对旧Quorum的所有成员来说，最后一个通过的提案应该都是相同的，但对不属于旧Quorum的成员来说，它的最后一个通过的提案是落后的。\n\n![](http://images.cnitblog.com/blog/571795/201410/081934320306301.jpg)\n\n同步已批准提案的方法是，Leader先向新Quorum的所有Peon节点发送OP_COLLECT消息，并在消息中携带Leader自己的第1个和最后1个批准的提案的值的版本号。Peon收到OP_COLLECT消息后，将自己的第1个和最后1个批准的提案的值的版本号返回给Leader，并且如果Peon的最后1个批准的版本号大于Leader最后一个批准的版本号时，将所有在大于Leader最后一个版本号的提案值发送给Leader。Leader将根据这些信息填补自己错过的提案。这样，当Leader接收到所有Peon对OP_COLLECT消息的回应后，也就将自己更新到了最新的状态。这时Leader又反过来将最新状态同步到其它节点。\n\n\n为获取新Quorum所有成员中的最大提案号，Leader在发送OP_COLLECT消息时，提出它知道的最大的提案号，并将该提案号附加在OP_COLLECT消息中。如果Peon已接受的最大提案号大于Leader提出的提案号，则拒绝接受Leader提出的提案号并将自己已接受的最大提案号通过OP_LAST消息发送给Leader。Leader收到OP_LAST消息后，发现自己的提案号不是最大时，就在Peon接受的最大提案号的基础上提出更大的提案号，重新进入Recovery阶段。这样，最终可以获取到最大的提案号。\n\n总而言之，Recovery阶段的目的是让新Quorum中所有节点处于一致状态。实现这一目的的方法分成两步：首先，在Leader节点收集所有节点的状态，通过比较得到最新状态；然后，Leader将最新状态同步给其它节点。有两个比较重要的状态，最后一次批准的提案和已接受的最大提案号。\n\n**注意** 区分提案号(proposal number)、提案值(value)、提案值的版本号(value version)这三个概念。提案号由Leader提出，为避免不同Leader提出的提案号不冲突，同个Leader提出的提案号是不连续的。提案的值的版本号是连续的。\n\n代码注释\n```\n函数\nvoid Paxos::leader_init()\nvoid Paxos::peon_init()\nvoid Paxos::collect(version_t oldpn)\nvoid Paxos::handle_collect(MMonPaxos *collect)\nvoid Paxos::handle_last(MMonPaxos *last)\nvoid Paxos::handle_commit(MMonPaxos *commit)\nPaxos属性\nuncommitted_v、uncommitted_pn、uncommitted_value\nlast_committed、accepted_pn\n配置\nOPTION(mon_lease, OPT_FLOAT, 5)     // Lease租期\n```\n\n## Lease阶段\n\nPaxos算法分成两个阶段，第一个阶段为Prepare阶段。在这阶段中，(a)Proposer选择它知道的**最大**提案号n，并向所有Acceptor发送Prepare消息。(b)Acceptor承诺不再接受编号小于n的提案，(c)并返回它**接受的编号小于n的提案中编号最大的提案**给Proposer。这个过程中，如果Proposer选择的不是最大的提案号，那么Acceptor将拒绝Proposer的提案，而Proposer遭到拒绝后会提出编号更大的提案。这样循环反复，Proposer最终可以提出编号**最大**的提案。另外，Acceptor返回**接受的编号小于n的提案中编号最大的提案**给Proposer的目的是为让Proposer决定新提出的提案的值。对Ceph而言，由于Leader可以控制提案的进度，运行一次Paxos算法只有一个提案在审批，每次算法Leader都能够由自己决定提案的值，所以Peon不必返回**接受的编号小于n的提案中编号最大的提案**。\n\nCeph中Paxos算法的实现，省略了Prepare阶段，并且Leader选举成功后每次执行算法使用同一个提案号。在Prepare阶段要完成(a)、(b)和(c)三件事，前两件事在Recovery阶段完成，Leader和Peon的**已接受的最大提案号**保持相同。最后一件事情，由于Leader的存在不需要做。 \n\nPaxos算法的第二阶段为Accept阶段。在这个阶段中，(d)Proposer根据在Prepare阶段中学习到的知识提出提案。(e)Acceptor根据接受到的提案的提案号决定拒绝还是接受。最后，(f)Proposer根据反馈情况决定提案是否得到批准。对Ceph来说，每次算法只有一个提案所以可以直接决定提案的值，因此不必关心(d)。对(e)和(f)的实现和标准Paxos算法保持一致。\n\n![](http://7vzu17.com1.z0.glb.clouddn.com/2014/10/08/ceph_paxos_02.jpg)\n\nCeph的Paxos存在如下几个状态：                 \n1) Recovery状态：Leader选举结束后进入该状态。该状态的目的是同步Quorum成员间的状态；            \n2) Active状态：即空闲状态，没有执行Paxos算法审批提案；              \n3) Updating状态：正在执行Paxos算法审批提案；          \n4) Updating Previous状态：正在执行Paxos算法审批旧提案，旧提案即Leader选举之前旧Leader提出但尚未批准的提案。\n\n\n\n代码注释\n```\n函数\nvoid Paxos::begin(bufferlist& v)\nvoid handle_begin(MMonPaxos *begin)\nvoid handle_accept(MMonPaxos *accept)\nvoid commit()\nvoid handle_commit(MMonPaxos *commit)\nPaxos属性\nproposals //提案列表\naccepted  // 接受(accept)新提案的节点，包括Leader和Peon\nnew_value\n```\n\n","source":"_posts/Ceph/paxos.md","raw":"title: Ceph的Paxos算法实现\ndate: 2014-10-08 20:07:09\ncategories: [Ceph]\ntags: [Ceph, Paxos]\ntoc: true\n---\n\nCeph使用Paxos算法来解决Monitor间数据的一致性问题。本文介绍的内容位于Leader选举之后，被推选为Leader的Montior节点从其它节点中收集状态信息，统一Quoruma成员的状态，然后进入正常工作的流程。\n\n<!--more-->\n\n##Recovery阶段\n\n在Leader选举成功后，Leader和Peon都进入Recovery阶段。该阶段的目的是为了保证新Quorum的所有成员状态一致，这些状态包括：最后一个批准(Committed)的提案，最后一个没批准的提案，最后一个接受(Acceppted)的提案。每个节点的这些状态都持久化到磁盘。对旧Quorum的所有成员来说，最后一个通过的提案应该都是相同的，但对不属于旧Quorum的成员来说，它的最后一个通过的提案是落后的。\n\n![](http://images.cnitblog.com/blog/571795/201410/081934320306301.jpg)\n\n同步已批准提案的方法是，Leader先向新Quorum的所有Peon节点发送OP_COLLECT消息，并在消息中携带Leader自己的第1个和最后1个批准的提案的值的版本号。Peon收到OP_COLLECT消息后，将自己的第1个和最后1个批准的提案的值的版本号返回给Leader，并且如果Peon的最后1个批准的版本号大于Leader最后一个批准的版本号时，将所有在大于Leader最后一个版本号的提案值发送给Leader。Leader将根据这些信息填补自己错过的提案。这样，当Leader接收到所有Peon对OP_COLLECT消息的回应后，也就将自己更新到了最新的状态。这时Leader又反过来将最新状态同步到其它节点。\n\n\n为获取新Quorum所有成员中的最大提案号，Leader在发送OP_COLLECT消息时，提出它知道的最大的提案号，并将该提案号附加在OP_COLLECT消息中。如果Peon已接受的最大提案号大于Leader提出的提案号，则拒绝接受Leader提出的提案号并将自己已接受的最大提案号通过OP_LAST消息发送给Leader。Leader收到OP_LAST消息后，发现自己的提案号不是最大时，就在Peon接受的最大提案号的基础上提出更大的提案号，重新进入Recovery阶段。这样，最终可以获取到最大的提案号。\n\n总而言之，Recovery阶段的目的是让新Quorum中所有节点处于一致状态。实现这一目的的方法分成两步：首先，在Leader节点收集所有节点的状态，通过比较得到最新状态；然后，Leader将最新状态同步给其它节点。有两个比较重要的状态，最后一次批准的提案和已接受的最大提案号。\n\n**注意** 区分提案号(proposal number)、提案值(value)、提案值的版本号(value version)这三个概念。提案号由Leader提出，为避免不同Leader提出的提案号不冲突，同个Leader提出的提案号是不连续的。提案的值的版本号是连续的。\n\n代码注释\n```\n函数\nvoid Paxos::leader_init()\nvoid Paxos::peon_init()\nvoid Paxos::collect(version_t oldpn)\nvoid Paxos::handle_collect(MMonPaxos *collect)\nvoid Paxos::handle_last(MMonPaxos *last)\nvoid Paxos::handle_commit(MMonPaxos *commit)\nPaxos属性\nuncommitted_v、uncommitted_pn、uncommitted_value\nlast_committed、accepted_pn\n配置\nOPTION(mon_lease, OPT_FLOAT, 5)     // Lease租期\n```\n\n## Lease阶段\n\nPaxos算法分成两个阶段，第一个阶段为Prepare阶段。在这阶段中，(a)Proposer选择它知道的**最大**提案号n，并向所有Acceptor发送Prepare消息。(b)Acceptor承诺不再接受编号小于n的提案，(c)并返回它**接受的编号小于n的提案中编号最大的提案**给Proposer。这个过程中，如果Proposer选择的不是最大的提案号，那么Acceptor将拒绝Proposer的提案，而Proposer遭到拒绝后会提出编号更大的提案。这样循环反复，Proposer最终可以提出编号**最大**的提案。另外，Acceptor返回**接受的编号小于n的提案中编号最大的提案**给Proposer的目的是为让Proposer决定新提出的提案的值。对Ceph而言，由于Leader可以控制提案的进度，运行一次Paxos算法只有一个提案在审批，每次算法Leader都能够由自己决定提案的值，所以Peon不必返回**接受的编号小于n的提案中编号最大的提案**。\n\nCeph中Paxos算法的实现，省略了Prepare阶段，并且Leader选举成功后每次执行算法使用同一个提案号。在Prepare阶段要完成(a)、(b)和(c)三件事，前两件事在Recovery阶段完成，Leader和Peon的**已接受的最大提案号**保持相同。最后一件事情，由于Leader的存在不需要做。 \n\nPaxos算法的第二阶段为Accept阶段。在这个阶段中，(d)Proposer根据在Prepare阶段中学习到的知识提出提案。(e)Acceptor根据接受到的提案的提案号决定拒绝还是接受。最后，(f)Proposer根据反馈情况决定提案是否得到批准。对Ceph来说，每次算法只有一个提案所以可以直接决定提案的值，因此不必关心(d)。对(e)和(f)的实现和标准Paxos算法保持一致。\n\n![](http://7vzu17.com1.z0.glb.clouddn.com/2014/10/08/ceph_paxos_02.jpg)\n\nCeph的Paxos存在如下几个状态：                 \n1) Recovery状态：Leader选举结束后进入该状态。该状态的目的是同步Quorum成员间的状态；            \n2) Active状态：即空闲状态，没有执行Paxos算法审批提案；              \n3) Updating状态：正在执行Paxos算法审批提案；          \n4) Updating Previous状态：正在执行Paxos算法审批旧提案，旧提案即Leader选举之前旧Leader提出但尚未批准的提案。\n\n\n\n代码注释\n```\n函数\nvoid Paxos::begin(bufferlist& v)\nvoid handle_begin(MMonPaxos *begin)\nvoid handle_accept(MMonPaxos *accept)\nvoid commit()\nvoid handle_commit(MMonPaxos *commit)\nPaxos属性\nproposals //提案列表\naccepted  // 接受(accept)新提案的节点，包括Leader和Peon\nnew_value\n```\n\n","slug":"Ceph/paxos","published":1,"updated":"2016-12-15T07:18:40.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cix1kgatt000fdaei3gv2k1fl","content":"<p>Ceph使用Paxos算法来解决Monitor间数据的一致性问题。本文介绍的内容位于Leader选举之后，被推选为Leader的Montior节点从其它节点中收集状态信息，统一Quoruma成员的状态，然后进入正常工作的流程。</p>\n<a id=\"more\"></a>\n<p>##Recovery阶段</p>\n<p>在Leader选举成功后，Leader和Peon都进入Recovery阶段。该阶段的目的是为了保证新Quorum的所有成员状态一致，这些状态包括：最后一个批准(Committed)的提案，最后一个没批准的提案，最后一个接受(Acceppted)的提案。每个节点的这些状态都持久化到磁盘。对旧Quorum的所有成员来说，最后一个通过的提案应该都是相同的，但对不属于旧Quorum的成员来说，它的最后一个通过的提案是落后的。</p>\n<p><img src=\"http://images.cnitblog.com/blog/571795/201410/081934320306301.jpg\" alt=\"\"></p>\n<p>同步已批准提案的方法是，Leader先向新Quorum的所有Peon节点发送OP_COLLECT消息，并在消息中携带Leader自己的第1个和最后1个批准的提案的值的版本号。Peon收到OP_COLLECT消息后，将自己的第1个和最后1个批准的提案的值的版本号返回给Leader，并且如果Peon的最后1个批准的版本号大于Leader最后一个批准的版本号时，将所有在大于Leader最后一个版本号的提案值发送给Leader。Leader将根据这些信息填补自己错过的提案。这样，当Leader接收到所有Peon对OP_COLLECT消息的回应后，也就将自己更新到了最新的状态。这时Leader又反过来将最新状态同步到其它节点。</p>\n<p>为获取新Quorum所有成员中的最大提案号，Leader在发送OP_COLLECT消息时，提出它知道的最大的提案号，并将该提案号附加在OP_COLLECT消息中。如果Peon已接受的最大提案号大于Leader提出的提案号，则拒绝接受Leader提出的提案号并将自己已接受的最大提案号通过OP_LAST消息发送给Leader。Leader收到OP_LAST消息后，发现自己的提案号不是最大时，就在Peon接受的最大提案号的基础上提出更大的提案号，重新进入Recovery阶段。这样，最终可以获取到最大的提案号。</p>\n<p>总而言之，Recovery阶段的目的是让新Quorum中所有节点处于一致状态。实现这一目的的方法分成两步：首先，在Leader节点收集所有节点的状态，通过比较得到最新状态；然后，Leader将最新状态同步给其它节点。有两个比较重要的状态，最后一次批准的提案和已接受的最大提案号。</p>\n<p><strong>注意</strong> 区分提案号(proposal number)、提案值(value)、提案值的版本号(value version)这三个概念。提案号由Leader提出，为避免不同Leader提出的提案号不冲突，同个Leader提出的提案号是不连续的。提案的值的版本号是连续的。</p>\n<p>代码注释<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">函数</div><div class=\"line\">void Paxos::leader_init()</div><div class=\"line\">void Paxos::peon_init()</div><div class=\"line\">void Paxos::collect(version_t oldpn)</div><div class=\"line\">void Paxos::handle_collect(MMonPaxos *collect)</div><div class=\"line\">void Paxos::handle_last(MMonPaxos *last)</div><div class=\"line\">void Paxos::handle_commit(MMonPaxos *commit)</div><div class=\"line\">Paxos属性</div><div class=\"line\">uncommitted_v、uncommitted_pn、uncommitted_value</div><div class=\"line\">last_committed、accepted_pn</div><div class=\"line\">配置</div><div class=\"line\">OPTION(mon_lease, OPT_FLOAT, 5)     // Lease租期</div></pre></td></tr></table></figure></p>\n<h2 id=\"Lease阶段\"><a href=\"#Lease阶段\" class=\"headerlink\" title=\"Lease阶段\"></a>Lease阶段</h2><p>Paxos算法分成两个阶段，第一个阶段为Prepare阶段。在这阶段中，(a)Proposer选择它知道的<strong>最大</strong>提案号n，并向所有Acceptor发送Prepare消息。(b)Acceptor承诺不再接受编号小于n的提案，(c)并返回它<strong>接受的编号小于n的提案中编号最大的提案</strong>给Proposer。这个过程中，如果Proposer选择的不是最大的提案号，那么Acceptor将拒绝Proposer的提案，而Proposer遭到拒绝后会提出编号更大的提案。这样循环反复，Proposer最终可以提出编号<strong>最大</strong>的提案。另外，Acceptor返回<strong>接受的编号小于n的提案中编号最大的提案</strong>给Proposer的目的是为让Proposer决定新提出的提案的值。对Ceph而言，由于Leader可以控制提案的进度，运行一次Paxos算法只有一个提案在审批，每次算法Leader都能够由自己决定提案的值，所以Peon不必返回<strong>接受的编号小于n的提案中编号最大的提案</strong>。</p>\n<p>Ceph中Paxos算法的实现，省略了Prepare阶段，并且Leader选举成功后每次执行算法使用同一个提案号。在Prepare阶段要完成(a)、(b)和(c)三件事，前两件事在Recovery阶段完成，Leader和Peon的<strong>已接受的最大提案号</strong>保持相同。最后一件事情，由于Leader的存在不需要做。 </p>\n<p>Paxos算法的第二阶段为Accept阶段。在这个阶段中，(d)Proposer根据在Prepare阶段中学习到的知识提出提案。(e)Acceptor根据接受到的提案的提案号决定拒绝还是接受。最后，(f)Proposer根据反馈情况决定提案是否得到批准。对Ceph来说，每次算法只有一个提案所以可以直接决定提案的值，因此不必关心(d)。对(e)和(f)的实现和标准Paxos算法保持一致。</p>\n<p><img src=\"http://7vzu17.com1.z0.glb.clouddn.com/2014/10/08/ceph_paxos_02.jpg\" alt=\"\"></p>\n<p>Ceph的Paxos存在如下几个状态：<br>1) Recovery状态：Leader选举结束后进入该状态。该状态的目的是同步Quorum成员间的状态；<br>2) Active状态：即空闲状态，没有执行Paxos算法审批提案；<br>3) Updating状态：正在执行Paxos算法审批提案；<br>4) Updating Previous状态：正在执行Paxos算法审批旧提案，旧提案即Leader选举之前旧Leader提出但尚未批准的提案。</p>\n<p>代码注释<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">函数</div><div class=\"line\">void Paxos::begin(bufferlist&amp; v)</div><div class=\"line\">void handle_begin(MMonPaxos *begin)</div><div class=\"line\">void handle_accept(MMonPaxos *accept)</div><div class=\"line\">void commit()</div><div class=\"line\">void handle_commit(MMonPaxos *commit)</div><div class=\"line\">Paxos属性</div><div class=\"line\">proposals //提案列表</div><div class=\"line\">accepted  // 接受(accept)新提案的节点，包括Leader和Peon</div><div class=\"line\">new_value</div></pre></td></tr></table></figure></p>\n<hr>\n<p><strong>版权声明</strong></p>\n<p>本作品采用<a href=\"https://creativecommons.org/licenses/by/2.5/cn/\" target=\"_blank\" rel=\"external\">知识共享署名2.5 中国大陆许可协议</a>进行许可，欢迎转载，但转载请注明来自<a href=\"https://wuxiangwei.cn/\"><em>Shanno’s Blog</em></a>，并保持转载后文章内容的完整。本人保留所有版权相关权利，侵权必究。</p>\n","excerpt":"<p>Ceph使用Paxos算法来解决Monitor间数据的一致性问题。本文介绍的内容位于Leader选举之后，被推选为Leader的Montior节点从其它节点中收集状态信息，统一Quoruma成员的状态，然后进入正常工作的流程。</p>","more":"<p>##Recovery阶段</p>\n<p>在Leader选举成功后，Leader和Peon都进入Recovery阶段。该阶段的目的是为了保证新Quorum的所有成员状态一致，这些状态包括：最后一个批准(Committed)的提案，最后一个没批准的提案，最后一个接受(Acceppted)的提案。每个节点的这些状态都持久化到磁盘。对旧Quorum的所有成员来说，最后一个通过的提案应该都是相同的，但对不属于旧Quorum的成员来说，它的最后一个通过的提案是落后的。</p>\n<p><img src=\"http://images.cnitblog.com/blog/571795/201410/081934320306301.jpg\" alt=\"\"></p>\n<p>同步已批准提案的方法是，Leader先向新Quorum的所有Peon节点发送OP_COLLECT消息，并在消息中携带Leader自己的第1个和最后1个批准的提案的值的版本号。Peon收到OP_COLLECT消息后，将自己的第1个和最后1个批准的提案的值的版本号返回给Leader，并且如果Peon的最后1个批准的版本号大于Leader最后一个批准的版本号时，将所有在大于Leader最后一个版本号的提案值发送给Leader。Leader将根据这些信息填补自己错过的提案。这样，当Leader接收到所有Peon对OP_COLLECT消息的回应后，也就将自己更新到了最新的状态。这时Leader又反过来将最新状态同步到其它节点。</p>\n<p>为获取新Quorum所有成员中的最大提案号，Leader在发送OP_COLLECT消息时，提出它知道的最大的提案号，并将该提案号附加在OP_COLLECT消息中。如果Peon已接受的最大提案号大于Leader提出的提案号，则拒绝接受Leader提出的提案号并将自己已接受的最大提案号通过OP_LAST消息发送给Leader。Leader收到OP_LAST消息后，发现自己的提案号不是最大时，就在Peon接受的最大提案号的基础上提出更大的提案号，重新进入Recovery阶段。这样，最终可以获取到最大的提案号。</p>\n<p>总而言之，Recovery阶段的目的是让新Quorum中所有节点处于一致状态。实现这一目的的方法分成两步：首先，在Leader节点收集所有节点的状态，通过比较得到最新状态；然后，Leader将最新状态同步给其它节点。有两个比较重要的状态，最后一次批准的提案和已接受的最大提案号。</p>\n<p><strong>注意</strong> 区分提案号(proposal number)、提案值(value)、提案值的版本号(value version)这三个概念。提案号由Leader提出，为避免不同Leader提出的提案号不冲突，同个Leader提出的提案号是不连续的。提案的值的版本号是连续的。</p>\n<p>代码注释<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">函数</div><div class=\"line\">void Paxos::leader_init()</div><div class=\"line\">void Paxos::peon_init()</div><div class=\"line\">void Paxos::collect(version_t oldpn)</div><div class=\"line\">void Paxos::handle_collect(MMonPaxos *collect)</div><div class=\"line\">void Paxos::handle_last(MMonPaxos *last)</div><div class=\"line\">void Paxos::handle_commit(MMonPaxos *commit)</div><div class=\"line\">Paxos属性</div><div class=\"line\">uncommitted_v、uncommitted_pn、uncommitted_value</div><div class=\"line\">last_committed、accepted_pn</div><div class=\"line\">配置</div><div class=\"line\">OPTION(mon_lease, OPT_FLOAT, 5)     // Lease租期</div></pre></td></tr></table></figure></p>\n<h2 id=\"Lease阶段\"><a href=\"#Lease阶段\" class=\"headerlink\" title=\"Lease阶段\"></a>Lease阶段</h2><p>Paxos算法分成两个阶段，第一个阶段为Prepare阶段。在这阶段中，(a)Proposer选择它知道的<strong>最大</strong>提案号n，并向所有Acceptor发送Prepare消息。(b)Acceptor承诺不再接受编号小于n的提案，(c)并返回它<strong>接受的编号小于n的提案中编号最大的提案</strong>给Proposer。这个过程中，如果Proposer选择的不是最大的提案号，那么Acceptor将拒绝Proposer的提案，而Proposer遭到拒绝后会提出编号更大的提案。这样循环反复，Proposer最终可以提出编号<strong>最大</strong>的提案。另外，Acceptor返回<strong>接受的编号小于n的提案中编号最大的提案</strong>给Proposer的目的是为让Proposer决定新提出的提案的值。对Ceph而言，由于Leader可以控制提案的进度，运行一次Paxos算法只有一个提案在审批，每次算法Leader都能够由自己决定提案的值，所以Peon不必返回<strong>接受的编号小于n的提案中编号最大的提案</strong>。</p>\n<p>Ceph中Paxos算法的实现，省略了Prepare阶段，并且Leader选举成功后每次执行算法使用同一个提案号。在Prepare阶段要完成(a)、(b)和(c)三件事，前两件事在Recovery阶段完成，Leader和Peon的<strong>已接受的最大提案号</strong>保持相同。最后一件事情，由于Leader的存在不需要做。 </p>\n<p>Paxos算法的第二阶段为Accept阶段。在这个阶段中，(d)Proposer根据在Prepare阶段中学习到的知识提出提案。(e)Acceptor根据接受到的提案的提案号决定拒绝还是接受。最后，(f)Proposer根据反馈情况决定提案是否得到批准。对Ceph来说，每次算法只有一个提案所以可以直接决定提案的值，因此不必关心(d)。对(e)和(f)的实现和标准Paxos算法保持一致。</p>\n<p><img src=\"http://7vzu17.com1.z0.glb.clouddn.com/2014/10/08/ceph_paxos_02.jpg\" alt=\"\"></p>\n<p>Ceph的Paxos存在如下几个状态：<br>1) Recovery状态：Leader选举结束后进入该状态。该状态的目的是同步Quorum成员间的状态；<br>2) Active状态：即空闲状态，没有执行Paxos算法审批提案；<br>3) Updating状态：正在执行Paxos算法审批提案；<br>4) Updating Previous状态：正在执行Paxos算法审批旧提案，旧提案即Leader选举之前旧Leader提出但尚未批准的提案。</p>\n<p>代码注释<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">函数</div><div class=\"line\">void Paxos::begin(bufferlist&amp; v)</div><div class=\"line\">void handle_begin(MMonPaxos *begin)</div><div class=\"line\">void handle_accept(MMonPaxos *accept)</div><div class=\"line\">void commit()</div><div class=\"line\">void handle_commit(MMonPaxos *commit)</div><div class=\"line\">Paxos属性</div><div class=\"line\">proposals //提案列表</div><div class=\"line\">accepted  // 接受(accept)新提案的节点，包括Leader和Peon</div><div class=\"line\">new_value</div></pre></td></tr></table></figure></p>\n<hr>\n<p><strong>版权声明</strong></p>\n<p>本作品采用<a href=\"https://creativecommons.org/licenses/by/2.5/cn/\">知识共享署名2.5 中国大陆许可协议</a>进行许可，欢迎转载，但转载请注明来自<a href=\"https://wuxiangwei.cn/\"><em>Shanno’s Blog</em></a>，并保持转载后文章内容的完整。本人保留所有版权相关权利，侵权必究。</p>"},{"title":"Paxos算法分析","date":"2014-09-30T11:50:33.000Z","toc":true,"_content":"\nPaxos算法是重要的一致性算法，在GFS、Ceph中都扮演重要角色。本文着重介绍算法的基本原理和推导过程，不涉及具体环境中的使用。如果希望了解Paxos在Ceph系统中的使用可以参考我的另外一篇博客。\n\n<!--more-->\n\n### 一致性问题\n\n![](http://7vzu17.com1.z0.glb.clouddn.com/2014/09/30/paxos/paxos_alg.jpg)\n\n如上图所示，服务器Ai(i=1,2,..5)组成存储集群，每份数据在5台服务器中各保留一个副本。当客户端C1和C2同时修改存储在集群中的同一份数据时，由于网络延迟的存在无法保证两个修改数据的请求到达每台服务器的先后顺序。也就是说，可能出现C1的请求先到达A1服务器，而C2的请求先到达A2服务器的情况。这种情况下，在A1和A2服务器上对数据的修改顺序不同，而修改顺序的不同直接导致了这两个副本数据的不一致。\n\n要保持副本一致只要保证在每台服务器上对数据修改的操作顺序相同。保证修改操作的顺序相同的最简单方法是采用Primary-Secondary模式，所有对数据的修改操作都先发送到Primary主机，然后由Primary主机分发给Secondary主机，也就是说修改操作的顺序由Primary主机决定。但这种方法存在单点故障问题。另一种方法是选举。当一台主机接收到修改数据的请求时并不直接修改数据，而是发起一次选举，只有赢得选举的请求才可以修改数据，并且修改操作在所有主机上进行。也就是说修改操作的顺序由集群中的所有主机共同决定。举个例子，如果在A1接收到C1请求的同时，A2也接收到了C2的请求，这时它们同时发起选举以执行自己的修改操作，选举的结果只有一个请求被批准。假如C2请求得到批准，那么所有主机上先执行C2请求。A1选举失败后重新发起选举，在得到批准后，所有主机执行C1的请求。从而，通过两轮选举确定了C1和C2请求在所有主机上的执行顺序。\n\n上面的第二个方法的关键是如何由集群中的所有节点共同选择一个值。\n\n### 选择一个值(Value)\n\n执行一次Paxos算法只能批准(chonsen)一个值，否则，仍旧无法确定值的先后顺序。                    \n提案得到批准的条件是，存在一个多数派，它的每个成员都接受(Accept)这个提案。**假定每个成员最多只能接受一个提案**，那么如果不是多数派，就会出现一次Paxos算法批准多个值的问题。例如，假设只要有2个节点接受提案就批准提案，当A1、A2同时提出提案，A1和A3接受A1的提案，A2和A4接受A2的提案，因为它们都满足被批准的条件，所以A1和A2的提案都得到批准。 因为任意两个多数派之间至少存在一个公共的成员，所以可以保证得到多数派支持的提案是唯一的。继续上面的例子，假设A5接受了A1的提案，那么就不能再接受A2的提案，从而A1的提案得到多数派的支持获得批准。\n\n**P1.每个Acceptor必须接受它接收到的第一个提案**。                   \n提出约束条件P1的原因是，我们希望在只有一个节点提出提案的情况下，这个提案可以得到批准。但P1会引入另外一个问题，就是可能无法形成多数派。例如，A1到A5每个节点在同一时刻提出提案，并且自己率先接受了提案。由于每个节点最多只能接受一个提案，因此每个提案都只有一个节点接受，无法形成多数派。为了可以形成多数派，放宽条件，**让每个Acceptor节点可以接受多个提案**。但这又会引入新问题，继续前面的例子，如果每个节点接受了所有接收到的提案，那么5个提案都会得到批准，从而又出现了执行一次算法批准多个提案的问题。\n\n**P2. 如果一个值为v的提案被批准，那么提案号更大的提案得到批准的条件是它的值也为v**。            \n为解决上述问题，提出了约束条件P2。在每个Acceptor可以接受多个提案的情况下，会导致多个提案得到批准。这是不允许的，但是如果每个被批准的提案的值都相同，那就相当于只批准了一个提案，这是可以接受的。我们直接对提案的结果进行约束，允许批准多个提案，但是这些提案的值必须相同，也就是必须同第一个被批准的提案的值相同。\n\n\n**P2a. 如果一个值为v的提案被批准，那么Acceptor只能接受提案号更大并且值为v的提案**。         \n事实上，只要满足P1和P2两个约束条件，就能够保证执行一次算法只批准一个提案了。但P2直接对结果进行约束，可操作性差。为此，需要寻找一个比P2更强的约束P2a，只要满足了P2a也就满足了P2。因为每个提案都需要Acceptor接受后才能够被批准，因此P2a对Acceptor进行约束，让它只接受提案号更大并且值为v的提案。考虑到P1也是对Acceptor的约束，分析下这两者有无冲突。假设在A1提出提案时，A2处于离线状态，并且由于消息丢失A3没有接收到A1的提案。因此，A1的提案只被A1、A4和A5接受，但这已经形成多数派，所以A1的提案被批准。随后，A2上线提出了提案号更大但值不同的提案，A3接收到A2的提案，并且这是它接收到的第一个提案，根据P1的约束，它应该无条件接受这个提案。但是，根据P2a约束，因为提案的值和已经被批准的提案的值不相同，所以不能接受A2的提案。P1和P2a出现了冲突。\n\n**P2b. 如果一个值为v的提案被批准，那么Proposer只能提出提案号更大并且值为v的提案**。                 \n上面导致P1和P2a冲突的根源是，A2上线后贸然提出了值和已经批准的提案的值不相同的提案。为此，我们对Proposer进行约束，让它提出提案号更大的提案的值同已经被批准的提案的值相同。这样就避免了P1和P1a的冲突。另一方面，因为所有被Acceptor接受的提案都是由Proposer提出的，所以满足了P2b约束就满足了P2a约束，满足了P2a约束就满足了P2约束。\n\n\n**P2c. 如果有编号为n值为v的提案，那么存在一个多数派S，(a)要么S中的成员都没有接受过编号小于n的提案，(b)要么S接受的所有编号小于n的提案中提案号最大的提案的值为v**。                 \n感觉从P2b到P2c有点跳跃，不过还是可以证明只要满足了P2c就能满足P2b。即要证明：如果一个编号为m值为v的提案被批准(P2b的假设条件)，在满足P2c的条件下，那么Proposer提出的编号为n(n>m)的提案的值为v。证明过程采用数学归纳法。\n\n当n=m+1时，采用反证法。即假设Proposer提出的编号为n的提案的值不为v而为w，那么必定会和P2c的条件相冲突。根据P2c，如果有编号为n值为w的提案，那么存在一个多数派S1，(a)要么S1中所有成员都没有接受过编号小于n的提案，(b)要么S1中所有接受到的编号小于n的提案中编号最大的提案的值为w。因为编号为m值为v的提案已经被批准，批准该提案的多数派C和S1之间至少存在一个公共成员，这个成员接受过编号为m值为n的提案，所以(a)不成立。因为小于n的最大编号是m，而编号m的提案的值为v不为w,所以(b)不成立。从而，假设不成立，即Proposer提出的编号为n的提案的值为v。\n\n当n>m+1时，采用数学归纳法，即P2b在编号为m+1到n-1时都成立，要证明在编号为n时P2b依旧成立。证明过程仍然采用反证法。假设Proposer提出的编号为n的提案的值不为v而为w。那么根据P2c，存在一个多数派S1，(a)要么S1没有接受过编号小于n的提案，(b)要么S1接受的编号小于n的所有提案中编号最大的提案的值为w。因为编号为m值为v的提案被批准，而批准该提案的多数派C和S1之间存在至少一个公共成员，所以(a)不成立。另外，因为编号从m到n-1的提案的值都为v，所以(b)也不成立。因此假设不成立，P2c包含P2b的结论成立。\n\n\n至此，我们只要满足P1和P2c两个约束条件，就可以达到执行一次算法只批准一个提案(可以是多个提案，但提案的值相同)的目的了。满足条件P1很容易，但为满足P2c，每个Proposer提出编号为n的提案时都要先学习所有Acceptor已经接受的编号小于n且最大的提案的值，并将新提案的值设置成该提案的值。学习Acceptor已经接受的提案容易，但是要预测Acceptor将会接受的提案就比较麻烦。对此，Paxos使用了承诺策略。当Proposer向Acceptor获取编号小于n的提案时，Acceptor一方面将自己接受的*所有编号小于n的提案中编号最大*的提案返回给Proposer，另一方面承诺不再接受编号小于n的提案。也就是说在Proposer学习过程中，Acceptor不会再接受编号小于n的提案，从而Proposer就不用去预测Acceptor在这段时间内会接受哪些编号小于n的提案了。\n\n为满足P2c约束，**Proposer提出提案的过程**分以下两个步骤：                    \n1、选择一个新的提案号n，向所有Acceptor发送**Prepare请求**，以获取：(a) 让Acceptor承诺不再接受编号小于n的提案；(b) Acceptor返回它接受的*所有编号小于n的提案中编号最大*的提案。       \n2、Proposer接受到所有Prepare请求的回应后，提出编号为n值为v的提案。值v是所有Acceptor已经接受的*所有编号小于n的提案中的编号最大*的提案的值。如果所有的Acceptor都没有接受过编号小于n的提案，那么v的值可以由Proposer自己指定。\n\n**P1a. 如果Acceptor承诺过不再接受编号小于m(m>n)的提案，则拒绝接受编号为n的提案，否则，接受编号为n的提案**。              \n当Proposer提出编号为n的提案后，向Acceptor发送**Accept请求**，以让Acceptor接受自己的提案。Acceptor根据P1a的原则来决定是否接受提案。\n\n### 算法流程\n\n为满足P1a和P2c两个约束，将算法的执行流程分成两个阶段：               \n**Phase1**       \n(a) Proposer选择一个提案号n，向所有的Acceptor发送Prepare消息；               \n(b) 如果Acceptor还没有承若过不接受编号小于m(m>n)的提案，则承诺不再接受编号小于n的提案，并返回它已经*接受的编号小于n的提案中编号最大*的提案。\n\n**Phase2**           \n(a) Proposer接受到所有Acceptor对Prepare请求的回应后，提出编号为n值为v的提案。值v是所有Acceptor接受到的所有*编号小于n的提案中编号最大*的提案的值。如果没有Acceptor接受过编号小于n的提案，则值v可由Proposer自己决定。提出提案后，向所有的Acceptor发送Accept消息，以期Acceptor接受提案。                          \n(b) 如果Acceptor没有向其它的Prepare请求承诺过不再接受编号小于m(m>n)的提案，则接受编号为n的提案。\n\n### 活锁问题\n\n考虑这样一种情况：              \nAi(i=1,2,...,5)都没有接受过任何提案，A1提出编号为n的Prepare请求并发送给其它节点，A2~A5节点接收到A1的Prepare请求后都承诺不再接受编号小于n的提案。但是，在A1还没有发出Accept请求的时候，A2向所有节点发送了编号为n+1的Prepare请求，由于n+1大于n，所以所有节点都承诺不再接受编号小于n+1的提案。当A1提出的编号为n提案发送到各个节点时，每个节点都会拒绝接受编号为n的提案。同样地，在A2的Accept请求还没送到Ai节点时，Ai节点又接受到编号为n+2的Prepare请求并承诺不再接受编号小于n+2的提案，因此A2的提案又无法获得批准。如此循环往复，始终无法批准提案。这就是活锁问题。\n\n产生活锁问题的原因在于，无法控制Proposer提出提案的时机，在一轮算法还没执行结束时就提出提案导致前面的提案被撤销。控制提案进度的方法是，选择一个Leader节点，只允许Leader节点提出提案。Leader节点可以将提案保存在队列中，等一个提案批准后再从队列中取出另外一个提案，这就避免了活锁问题。\n\n引入Leader后，Paxos算法似乎变回到了Primary-Secondary模式，值的执行顺序完全由Leader决定，并且Leader存在单点故障。但**Paxos算法的优势在于无论是Leader、Acceptor宕机都能保证正常工作**。 如果Leader宕机，那么就要执行选举算法从现有的节点中选举出一个Leader。新Leader将从其余节点中收集信息，恢复状态。如果Acceptor宕机，只要能够形成多数派，就可以保持算法正常执行。\n\n### 全局唯一提案号\n\n全局唯一的提案号是影响Paxos算法正常运转的关键。如果两个Proposer提出相同的提案号，并且该编号的提案得到批准，那么在某些节点上更新这个值，在另外一些节点上更新那个值，就会出现副本不一致的问题。在单机上提出唯一的编号比较容易，只要依次递增即可。但在不同的机器上提出不同的编号，就要开动脑筋了。\n\nCeph是这么计算提案号的：               \n**last_pn = (last_pn / 100 + 1)100 + rank**                \nlast_pn是Leader节点最近提出的提案号，rank是Leader节点的编号。也就是说，所有的提案号都是100的整数倍加上节点自己的编号。当rank值小于100时，可以保证每个节点提出的提案号都是全局唯一的，具体代码参考Paxos::get_new_proposal_number()函数。\n\n\n\n\n参考资料\n\n1. Paxos Made Simple             \n2. [Paxos算法](http://zh.wikipedia.org/zh/Paxos%E7%AE%97%E6%B3%95)             \n3. [Paxos算法2-算法过程](http://blog.csdn.net/chen77716/article/details/6170235)            \n\n","source":"_posts/Ceph/paxos_2.md","raw":"---\ntitle: Paxos算法分析\ndate: 2014-09-30 19:50:33\ncategories: [Ceph]\ntags: [Ceph, Paxos]\ntoc: true\n---\n\nPaxos算法是重要的一致性算法，在GFS、Ceph中都扮演重要角色。本文着重介绍算法的基本原理和推导过程，不涉及具体环境中的使用。如果希望了解Paxos在Ceph系统中的使用可以参考我的另外一篇博客。\n\n<!--more-->\n\n### 一致性问题\n\n![](http://7vzu17.com1.z0.glb.clouddn.com/2014/09/30/paxos/paxos_alg.jpg)\n\n如上图所示，服务器Ai(i=1,2,..5)组成存储集群，每份数据在5台服务器中各保留一个副本。当客户端C1和C2同时修改存储在集群中的同一份数据时，由于网络延迟的存在无法保证两个修改数据的请求到达每台服务器的先后顺序。也就是说，可能出现C1的请求先到达A1服务器，而C2的请求先到达A2服务器的情况。这种情况下，在A1和A2服务器上对数据的修改顺序不同，而修改顺序的不同直接导致了这两个副本数据的不一致。\n\n要保持副本一致只要保证在每台服务器上对数据修改的操作顺序相同。保证修改操作的顺序相同的最简单方法是采用Primary-Secondary模式，所有对数据的修改操作都先发送到Primary主机，然后由Primary主机分发给Secondary主机，也就是说修改操作的顺序由Primary主机决定。但这种方法存在单点故障问题。另一种方法是选举。当一台主机接收到修改数据的请求时并不直接修改数据，而是发起一次选举，只有赢得选举的请求才可以修改数据，并且修改操作在所有主机上进行。也就是说修改操作的顺序由集群中的所有主机共同决定。举个例子，如果在A1接收到C1请求的同时，A2也接收到了C2的请求，这时它们同时发起选举以执行自己的修改操作，选举的结果只有一个请求被批准。假如C2请求得到批准，那么所有主机上先执行C2请求。A1选举失败后重新发起选举，在得到批准后，所有主机执行C1的请求。从而，通过两轮选举确定了C1和C2请求在所有主机上的执行顺序。\n\n上面的第二个方法的关键是如何由集群中的所有节点共同选择一个值。\n\n### 选择一个值(Value)\n\n执行一次Paxos算法只能批准(chonsen)一个值，否则，仍旧无法确定值的先后顺序。                    \n提案得到批准的条件是，存在一个多数派，它的每个成员都接受(Accept)这个提案。**假定每个成员最多只能接受一个提案**，那么如果不是多数派，就会出现一次Paxos算法批准多个值的问题。例如，假设只要有2个节点接受提案就批准提案，当A1、A2同时提出提案，A1和A3接受A1的提案，A2和A4接受A2的提案，因为它们都满足被批准的条件，所以A1和A2的提案都得到批准。 因为任意两个多数派之间至少存在一个公共的成员，所以可以保证得到多数派支持的提案是唯一的。继续上面的例子，假设A5接受了A1的提案，那么就不能再接受A2的提案，从而A1的提案得到多数派的支持获得批准。\n\n**P1.每个Acceptor必须接受它接收到的第一个提案**。                   \n提出约束条件P1的原因是，我们希望在只有一个节点提出提案的情况下，这个提案可以得到批准。但P1会引入另外一个问题，就是可能无法形成多数派。例如，A1到A5每个节点在同一时刻提出提案，并且自己率先接受了提案。由于每个节点最多只能接受一个提案，因此每个提案都只有一个节点接受，无法形成多数派。为了可以形成多数派，放宽条件，**让每个Acceptor节点可以接受多个提案**。但这又会引入新问题，继续前面的例子，如果每个节点接受了所有接收到的提案，那么5个提案都会得到批准，从而又出现了执行一次算法批准多个提案的问题。\n\n**P2. 如果一个值为v的提案被批准，那么提案号更大的提案得到批准的条件是它的值也为v**。            \n为解决上述问题，提出了约束条件P2。在每个Acceptor可以接受多个提案的情况下，会导致多个提案得到批准。这是不允许的，但是如果每个被批准的提案的值都相同，那就相当于只批准了一个提案，这是可以接受的。我们直接对提案的结果进行约束，允许批准多个提案，但是这些提案的值必须相同，也就是必须同第一个被批准的提案的值相同。\n\n\n**P2a. 如果一个值为v的提案被批准，那么Acceptor只能接受提案号更大并且值为v的提案**。         \n事实上，只要满足P1和P2两个约束条件，就能够保证执行一次算法只批准一个提案了。但P2直接对结果进行约束，可操作性差。为此，需要寻找一个比P2更强的约束P2a，只要满足了P2a也就满足了P2。因为每个提案都需要Acceptor接受后才能够被批准，因此P2a对Acceptor进行约束，让它只接受提案号更大并且值为v的提案。考虑到P1也是对Acceptor的约束，分析下这两者有无冲突。假设在A1提出提案时，A2处于离线状态，并且由于消息丢失A3没有接收到A1的提案。因此，A1的提案只被A1、A4和A5接受，但这已经形成多数派，所以A1的提案被批准。随后，A2上线提出了提案号更大但值不同的提案，A3接收到A2的提案，并且这是它接收到的第一个提案，根据P1的约束，它应该无条件接受这个提案。但是，根据P2a约束，因为提案的值和已经被批准的提案的值不相同，所以不能接受A2的提案。P1和P2a出现了冲突。\n\n**P2b. 如果一个值为v的提案被批准，那么Proposer只能提出提案号更大并且值为v的提案**。                 \n上面导致P1和P2a冲突的根源是，A2上线后贸然提出了值和已经批准的提案的值不相同的提案。为此，我们对Proposer进行约束，让它提出提案号更大的提案的值同已经被批准的提案的值相同。这样就避免了P1和P1a的冲突。另一方面，因为所有被Acceptor接受的提案都是由Proposer提出的，所以满足了P2b约束就满足了P2a约束，满足了P2a约束就满足了P2约束。\n\n\n**P2c. 如果有编号为n值为v的提案，那么存在一个多数派S，(a)要么S中的成员都没有接受过编号小于n的提案，(b)要么S接受的所有编号小于n的提案中提案号最大的提案的值为v**。                 \n感觉从P2b到P2c有点跳跃，不过还是可以证明只要满足了P2c就能满足P2b。即要证明：如果一个编号为m值为v的提案被批准(P2b的假设条件)，在满足P2c的条件下，那么Proposer提出的编号为n(n>m)的提案的值为v。证明过程采用数学归纳法。\n\n当n=m+1时，采用反证法。即假设Proposer提出的编号为n的提案的值不为v而为w，那么必定会和P2c的条件相冲突。根据P2c，如果有编号为n值为w的提案，那么存在一个多数派S1，(a)要么S1中所有成员都没有接受过编号小于n的提案，(b)要么S1中所有接受到的编号小于n的提案中编号最大的提案的值为w。因为编号为m值为v的提案已经被批准，批准该提案的多数派C和S1之间至少存在一个公共成员，这个成员接受过编号为m值为n的提案，所以(a)不成立。因为小于n的最大编号是m，而编号m的提案的值为v不为w,所以(b)不成立。从而，假设不成立，即Proposer提出的编号为n的提案的值为v。\n\n当n>m+1时，采用数学归纳法，即P2b在编号为m+1到n-1时都成立，要证明在编号为n时P2b依旧成立。证明过程仍然采用反证法。假设Proposer提出的编号为n的提案的值不为v而为w。那么根据P2c，存在一个多数派S1，(a)要么S1没有接受过编号小于n的提案，(b)要么S1接受的编号小于n的所有提案中编号最大的提案的值为w。因为编号为m值为v的提案被批准，而批准该提案的多数派C和S1之间存在至少一个公共成员，所以(a)不成立。另外，因为编号从m到n-1的提案的值都为v，所以(b)也不成立。因此假设不成立，P2c包含P2b的结论成立。\n\n\n至此，我们只要满足P1和P2c两个约束条件，就可以达到执行一次算法只批准一个提案(可以是多个提案，但提案的值相同)的目的了。满足条件P1很容易，但为满足P2c，每个Proposer提出编号为n的提案时都要先学习所有Acceptor已经接受的编号小于n且最大的提案的值，并将新提案的值设置成该提案的值。学习Acceptor已经接受的提案容易，但是要预测Acceptor将会接受的提案就比较麻烦。对此，Paxos使用了承诺策略。当Proposer向Acceptor获取编号小于n的提案时，Acceptor一方面将自己接受的*所有编号小于n的提案中编号最大*的提案返回给Proposer，另一方面承诺不再接受编号小于n的提案。也就是说在Proposer学习过程中，Acceptor不会再接受编号小于n的提案，从而Proposer就不用去预测Acceptor在这段时间内会接受哪些编号小于n的提案了。\n\n为满足P2c约束，**Proposer提出提案的过程**分以下两个步骤：                    \n1、选择一个新的提案号n，向所有Acceptor发送**Prepare请求**，以获取：(a) 让Acceptor承诺不再接受编号小于n的提案；(b) Acceptor返回它接受的*所有编号小于n的提案中编号最大*的提案。       \n2、Proposer接受到所有Prepare请求的回应后，提出编号为n值为v的提案。值v是所有Acceptor已经接受的*所有编号小于n的提案中的编号最大*的提案的值。如果所有的Acceptor都没有接受过编号小于n的提案，那么v的值可以由Proposer自己指定。\n\n**P1a. 如果Acceptor承诺过不再接受编号小于m(m>n)的提案，则拒绝接受编号为n的提案，否则，接受编号为n的提案**。              \n当Proposer提出编号为n的提案后，向Acceptor发送**Accept请求**，以让Acceptor接受自己的提案。Acceptor根据P1a的原则来决定是否接受提案。\n\n### 算法流程\n\n为满足P1a和P2c两个约束，将算法的执行流程分成两个阶段：               \n**Phase1**       \n(a) Proposer选择一个提案号n，向所有的Acceptor发送Prepare消息；               \n(b) 如果Acceptor还没有承若过不接受编号小于m(m>n)的提案，则承诺不再接受编号小于n的提案，并返回它已经*接受的编号小于n的提案中编号最大*的提案。\n\n**Phase2**           \n(a) Proposer接受到所有Acceptor对Prepare请求的回应后，提出编号为n值为v的提案。值v是所有Acceptor接受到的所有*编号小于n的提案中编号最大*的提案的值。如果没有Acceptor接受过编号小于n的提案，则值v可由Proposer自己决定。提出提案后，向所有的Acceptor发送Accept消息，以期Acceptor接受提案。                          \n(b) 如果Acceptor没有向其它的Prepare请求承诺过不再接受编号小于m(m>n)的提案，则接受编号为n的提案。\n\n### 活锁问题\n\n考虑这样一种情况：              \nAi(i=1,2,...,5)都没有接受过任何提案，A1提出编号为n的Prepare请求并发送给其它节点，A2~A5节点接收到A1的Prepare请求后都承诺不再接受编号小于n的提案。但是，在A1还没有发出Accept请求的时候，A2向所有节点发送了编号为n+1的Prepare请求，由于n+1大于n，所以所有节点都承诺不再接受编号小于n+1的提案。当A1提出的编号为n提案发送到各个节点时，每个节点都会拒绝接受编号为n的提案。同样地，在A2的Accept请求还没送到Ai节点时，Ai节点又接受到编号为n+2的Prepare请求并承诺不再接受编号小于n+2的提案，因此A2的提案又无法获得批准。如此循环往复，始终无法批准提案。这就是活锁问题。\n\n产生活锁问题的原因在于，无法控制Proposer提出提案的时机，在一轮算法还没执行结束时就提出提案导致前面的提案被撤销。控制提案进度的方法是，选择一个Leader节点，只允许Leader节点提出提案。Leader节点可以将提案保存在队列中，等一个提案批准后再从队列中取出另外一个提案，这就避免了活锁问题。\n\n引入Leader后，Paxos算法似乎变回到了Primary-Secondary模式，值的执行顺序完全由Leader决定，并且Leader存在单点故障。但**Paxos算法的优势在于无论是Leader、Acceptor宕机都能保证正常工作**。 如果Leader宕机，那么就要执行选举算法从现有的节点中选举出一个Leader。新Leader将从其余节点中收集信息，恢复状态。如果Acceptor宕机，只要能够形成多数派，就可以保持算法正常执行。\n\n### 全局唯一提案号\n\n全局唯一的提案号是影响Paxos算法正常运转的关键。如果两个Proposer提出相同的提案号，并且该编号的提案得到批准，那么在某些节点上更新这个值，在另外一些节点上更新那个值，就会出现副本不一致的问题。在单机上提出唯一的编号比较容易，只要依次递增即可。但在不同的机器上提出不同的编号，就要开动脑筋了。\n\nCeph是这么计算提案号的：               \n**last_pn = (last_pn / 100 + 1)100 + rank**                \nlast_pn是Leader节点最近提出的提案号，rank是Leader节点的编号。也就是说，所有的提案号都是100的整数倍加上节点自己的编号。当rank值小于100时，可以保证每个节点提出的提案号都是全局唯一的，具体代码参考Paxos::get_new_proposal_number()函数。\n\n\n\n\n参考资料\n\n1. Paxos Made Simple             \n2. [Paxos算法](http://zh.wikipedia.org/zh/Paxos%E7%AE%97%E6%B3%95)             \n3. [Paxos算法2-算法过程](http://blog.csdn.net/chen77716/article/details/6170235)            \n\n","slug":"Ceph/paxos_2","published":1,"updated":"2016-12-15T07:18:40.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cix1kgaty000kdaei67pcsy9y","content":"<p>Paxos算法是重要的一致性算法，在GFS、Ceph中都扮演重要角色。本文着重介绍算法的基本原理和推导过程，不涉及具体环境中的使用。如果希望了解Paxos在Ceph系统中的使用可以参考我的另外一篇博客。</p>\n<a id=\"more\"></a>\n<h3 id=\"一致性问题\"><a href=\"#一致性问题\" class=\"headerlink\" title=\"一致性问题\"></a>一致性问题</h3><p><img src=\"http://7vzu17.com1.z0.glb.clouddn.com/2014/09/30/paxos/paxos_alg.jpg\" alt=\"\"></p>\n<p>如上图所示，服务器Ai(i=1,2,..5)组成存储集群，每份数据在5台服务器中各保留一个副本。当客户端C1和C2同时修改存储在集群中的同一份数据时，由于网络延迟的存在无法保证两个修改数据的请求到达每台服务器的先后顺序。也就是说，可能出现C1的请求先到达A1服务器，而C2的请求先到达A2服务器的情况。这种情况下，在A1和A2服务器上对数据的修改顺序不同，而修改顺序的不同直接导致了这两个副本数据的不一致。</p>\n<p>要保持副本一致只要保证在每台服务器上对数据修改的操作顺序相同。保证修改操作的顺序相同的最简单方法是采用Primary-Secondary模式，所有对数据的修改操作都先发送到Primary主机，然后由Primary主机分发给Secondary主机，也就是说修改操作的顺序由Primary主机决定。但这种方法存在单点故障问题。另一种方法是选举。当一台主机接收到修改数据的请求时并不直接修改数据，而是发起一次选举，只有赢得选举的请求才可以修改数据，并且修改操作在所有主机上进行。也就是说修改操作的顺序由集群中的所有主机共同决定。举个例子，如果在A1接收到C1请求的同时，A2也接收到了C2的请求，这时它们同时发起选举以执行自己的修改操作，选举的结果只有一个请求被批准。假如C2请求得到批准，那么所有主机上先执行C2请求。A1选举失败后重新发起选举，在得到批准后，所有主机执行C1的请求。从而，通过两轮选举确定了C1和C2请求在所有主机上的执行顺序。</p>\n<p>上面的第二个方法的关键是如何由集群中的所有节点共同选择一个值。</p>\n<h3 id=\"选择一个值-Value\"><a href=\"#选择一个值-Value\" class=\"headerlink\" title=\"选择一个值(Value)\"></a>选择一个值(Value)</h3><p>执行一次Paxos算法只能批准(chonsen)一个值，否则，仍旧无法确定值的先后顺序。<br>提案得到批准的条件是，存在一个多数派，它的每个成员都接受(Accept)这个提案。<strong>假定每个成员最多只能接受一个提案</strong>，那么如果不是多数派，就会出现一次Paxos算法批准多个值的问题。例如，假设只要有2个节点接受提案就批准提案，当A1、A2同时提出提案，A1和A3接受A1的提案，A2和A4接受A2的提案，因为它们都满足被批准的条件，所以A1和A2的提案都得到批准。 因为任意两个多数派之间至少存在一个公共的成员，所以可以保证得到多数派支持的提案是唯一的。继续上面的例子，假设A5接受了A1的提案，那么就不能再接受A2的提案，从而A1的提案得到多数派的支持获得批准。</p>\n<p><strong>P1.每个Acceptor必须接受它接收到的第一个提案</strong>。<br>提出约束条件P1的原因是，我们希望在只有一个节点提出提案的情况下，这个提案可以得到批准。但P1会引入另外一个问题，就是可能无法形成多数派。例如，A1到A5每个节点在同一时刻提出提案，并且自己率先接受了提案。由于每个节点最多只能接受一个提案，因此每个提案都只有一个节点接受，无法形成多数派。为了可以形成多数派，放宽条件，<strong>让每个Acceptor节点可以接受多个提案</strong>。但这又会引入新问题，继续前面的例子，如果每个节点接受了所有接收到的提案，那么5个提案都会得到批准，从而又出现了执行一次算法批准多个提案的问题。</p>\n<p><strong>P2. 如果一个值为v的提案被批准，那么提案号更大的提案得到批准的条件是它的值也为v</strong>。<br>为解决上述问题，提出了约束条件P2。在每个Acceptor可以接受多个提案的情况下，会导致多个提案得到批准。这是不允许的，但是如果每个被批准的提案的值都相同，那就相当于只批准了一个提案，这是可以接受的。我们直接对提案的结果进行约束，允许批准多个提案，但是这些提案的值必须相同，也就是必须同第一个被批准的提案的值相同。</p>\n<p><strong>P2a. 如果一个值为v的提案被批准，那么Acceptor只能接受提案号更大并且值为v的提案</strong>。<br>事实上，只要满足P1和P2两个约束条件，就能够保证执行一次算法只批准一个提案了。但P2直接对结果进行约束，可操作性差。为此，需要寻找一个比P2更强的约束P2a，只要满足了P2a也就满足了P2。因为每个提案都需要Acceptor接受后才能够被批准，因此P2a对Acceptor进行约束，让它只接受提案号更大并且值为v的提案。考虑到P1也是对Acceptor的约束，分析下这两者有无冲突。假设在A1提出提案时，A2处于离线状态，并且由于消息丢失A3没有接收到A1的提案。因此，A1的提案只被A1、A4和A5接受，但这已经形成多数派，所以A1的提案被批准。随后，A2上线提出了提案号更大但值不同的提案，A3接收到A2的提案，并且这是它接收到的第一个提案，根据P1的约束，它应该无条件接受这个提案。但是，根据P2a约束，因为提案的值和已经被批准的提案的值不相同，所以不能接受A2的提案。P1和P2a出现了冲突。</p>\n<p><strong>P2b. 如果一个值为v的提案被批准，那么Proposer只能提出提案号更大并且值为v的提案</strong>。<br>上面导致P1和P2a冲突的根源是，A2上线后贸然提出了值和已经批准的提案的值不相同的提案。为此，我们对Proposer进行约束，让它提出提案号更大的提案的值同已经被批准的提案的值相同。这样就避免了P1和P1a的冲突。另一方面，因为所有被Acceptor接受的提案都是由Proposer提出的，所以满足了P2b约束就满足了P2a约束，满足了P2a约束就满足了P2约束。</p>\n<p><strong>P2c. 如果有编号为n值为v的提案，那么存在一个多数派S，(a)要么S中的成员都没有接受过编号小于n的提案，(b)要么S接受的所有编号小于n的提案中提案号最大的提案的值为v</strong>。<br>感觉从P2b到P2c有点跳跃，不过还是可以证明只要满足了P2c就能满足P2b。即要证明：如果一个编号为m值为v的提案被批准(P2b的假设条件)，在满足P2c的条件下，那么Proposer提出的编号为n(n&gt;m)的提案的值为v。证明过程采用数学归纳法。</p>\n<p>当n=m+1时，采用反证法。即假设Proposer提出的编号为n的提案的值不为v而为w，那么必定会和P2c的条件相冲突。根据P2c，如果有编号为n值为w的提案，那么存在一个多数派S1，(a)要么S1中所有成员都没有接受过编号小于n的提案，(b)要么S1中所有接受到的编号小于n的提案中编号最大的提案的值为w。因为编号为m值为v的提案已经被批准，批准该提案的多数派C和S1之间至少存在一个公共成员，这个成员接受过编号为m值为n的提案，所以(a)不成立。因为小于n的最大编号是m，而编号m的提案的值为v不为w,所以(b)不成立。从而，假设不成立，即Proposer提出的编号为n的提案的值为v。</p>\n<p>当n&gt;m+1时，采用数学归纳法，即P2b在编号为m+1到n-1时都成立，要证明在编号为n时P2b依旧成立。证明过程仍然采用反证法。假设Proposer提出的编号为n的提案的值不为v而为w。那么根据P2c，存在一个多数派S1，(a)要么S1没有接受过编号小于n的提案，(b)要么S1接受的编号小于n的所有提案中编号最大的提案的值为w。因为编号为m值为v的提案被批准，而批准该提案的多数派C和S1之间存在至少一个公共成员，所以(a)不成立。另外，因为编号从m到n-1的提案的值都为v，所以(b)也不成立。因此假设不成立，P2c包含P2b的结论成立。</p>\n<p>至此，我们只要满足P1和P2c两个约束条件，就可以达到执行一次算法只批准一个提案(可以是多个提案，但提案的值相同)的目的了。满足条件P1很容易，但为满足P2c，每个Proposer提出编号为n的提案时都要先学习所有Acceptor已经接受的编号小于n且最大的提案的值，并将新提案的值设置成该提案的值。学习Acceptor已经接受的提案容易，但是要预测Acceptor将会接受的提案就比较麻烦。对此，Paxos使用了承诺策略。当Proposer向Acceptor获取编号小于n的提案时，Acceptor一方面将自己接受的<em>所有编号小于n的提案中编号最大</em>的提案返回给Proposer，另一方面承诺不再接受编号小于n的提案。也就是说在Proposer学习过程中，Acceptor不会再接受编号小于n的提案，从而Proposer就不用去预测Acceptor在这段时间内会接受哪些编号小于n的提案了。</p>\n<p>为满足P2c约束，<strong>Proposer提出提案的过程</strong>分以下两个步骤：<br>1、选择一个新的提案号n，向所有Acceptor发送<strong>Prepare请求</strong>，以获取：(a) 让Acceptor承诺不再接受编号小于n的提案；(b) Acceptor返回它接受的<em>所有编号小于n的提案中编号最大</em>的提案。<br>2、Proposer接受到所有Prepare请求的回应后，提出编号为n值为v的提案。值v是所有Acceptor已经接受的<em>所有编号小于n的提案中的编号最大</em>的提案的值。如果所有的Acceptor都没有接受过编号小于n的提案，那么v的值可以由Proposer自己指定。</p>\n<p><strong>P1a. 如果Acceptor承诺过不再接受编号小于m(m&gt;n)的提案，则拒绝接受编号为n的提案，否则，接受编号为n的提案</strong>。<br>当Proposer提出编号为n的提案后，向Acceptor发送<strong>Accept请求</strong>，以让Acceptor接受自己的提案。Acceptor根据P1a的原则来决定是否接受提案。</p>\n<h3 id=\"算法流程\"><a href=\"#算法流程\" class=\"headerlink\" title=\"算法流程\"></a>算法流程</h3><p>为满足P1a和P2c两个约束，将算法的执行流程分成两个阶段：<br><strong>Phase1</strong><br>(a) Proposer选择一个提案号n，向所有的Acceptor发送Prepare消息；<br>(b) 如果Acceptor还没有承若过不接受编号小于m(m&gt;n)的提案，则承诺不再接受编号小于n的提案，并返回它已经<em>接受的编号小于n的提案中编号最大</em>的提案。</p>\n<p><strong>Phase2</strong><br>(a) Proposer接受到所有Acceptor对Prepare请求的回应后，提出编号为n值为v的提案。值v是所有Acceptor接受到的所有<em>编号小于n的提案中编号最大</em>的提案的值。如果没有Acceptor接受过编号小于n的提案，则值v可由Proposer自己决定。提出提案后，向所有的Acceptor发送Accept消息，以期Acceptor接受提案。<br>(b) 如果Acceptor没有向其它的Prepare请求承诺过不再接受编号小于m(m&gt;n)的提案，则接受编号为n的提案。</p>\n<h3 id=\"活锁问题\"><a href=\"#活锁问题\" class=\"headerlink\" title=\"活锁问题\"></a>活锁问题</h3><p>考虑这样一种情况：<br>Ai(i=1,2,…,5)都没有接受过任何提案，A1提出编号为n的Prepare请求并发送给其它节点，A2~A5节点接收到A1的Prepare请求后都承诺不再接受编号小于n的提案。但是，在A1还没有发出Accept请求的时候，A2向所有节点发送了编号为n+1的Prepare请求，由于n+1大于n，所以所有节点都承诺不再接受编号小于n+1的提案。当A1提出的编号为n提案发送到各个节点时，每个节点都会拒绝接受编号为n的提案。同样地，在A2的Accept请求还没送到Ai节点时，Ai节点又接受到编号为n+2的Prepare请求并承诺不再接受编号小于n+2的提案，因此A2的提案又无法获得批准。如此循环往复，始终无法批准提案。这就是活锁问题。</p>\n<p>产生活锁问题的原因在于，无法控制Proposer提出提案的时机，在一轮算法还没执行结束时就提出提案导致前面的提案被撤销。控制提案进度的方法是，选择一个Leader节点，只允许Leader节点提出提案。Leader节点可以将提案保存在队列中，等一个提案批准后再从队列中取出另外一个提案，这就避免了活锁问题。</p>\n<p>引入Leader后，Paxos算法似乎变回到了Primary-Secondary模式，值的执行顺序完全由Leader决定，并且Leader存在单点故障。但<strong>Paxos算法的优势在于无论是Leader、Acceptor宕机都能保证正常工作</strong>。 如果Leader宕机，那么就要执行选举算法从现有的节点中选举出一个Leader。新Leader将从其余节点中收集信息，恢复状态。如果Acceptor宕机，只要能够形成多数派，就可以保持算法正常执行。</p>\n<h3 id=\"全局唯一提案号\"><a href=\"#全局唯一提案号\" class=\"headerlink\" title=\"全局唯一提案号\"></a>全局唯一提案号</h3><p>全局唯一的提案号是影响Paxos算法正常运转的关键。如果两个Proposer提出相同的提案号，并且该编号的提案得到批准，那么在某些节点上更新这个值，在另外一些节点上更新那个值，就会出现副本不一致的问题。在单机上提出唯一的编号比较容易，只要依次递增即可。但在不同的机器上提出不同的编号，就要开动脑筋了。</p>\n<p>Ceph是这么计算提案号的：<br><strong>last_pn = (last_pn / 100 + 1)100 + rank</strong><br>last_pn是Leader节点最近提出的提案号，rank是Leader节点的编号。也就是说，所有的提案号都是100的整数倍加上节点自己的编号。当rank值小于100时，可以保证每个节点提出的提案号都是全局唯一的，具体代码参考Paxos::get_new_proposal_number()函数。</p>\n<p>参考资料</p>\n<ol>\n<li>Paxos Made Simple             </li>\n<li><a href=\"http://zh.wikipedia.org/zh/Paxos%E7%AE%97%E6%B3%95\" target=\"_blank\" rel=\"external\">Paxos算法</a>             </li>\n<li><a href=\"http://blog.csdn.net/chen77716/article/details/6170235\" target=\"_blank\" rel=\"external\">Paxos算法2-算法过程</a>            </li>\n</ol>\n<hr>\n<p><strong>版权声明</strong></p>\n<p>本作品采用<a href=\"https://creativecommons.org/licenses/by/2.5/cn/\" target=\"_blank\" rel=\"external\">知识共享署名2.5 中国大陆许可协议</a>进行许可，欢迎转载，但转载请注明来自<a href=\"https://wuxiangwei.cn/\"><em>Shanno’s Blog</em></a>，并保持转载后文章内容的完整。本人保留所有版权相关权利，侵权必究。</p>\n","excerpt":"<p>Paxos算法是重要的一致性算法，在GFS、Ceph中都扮演重要角色。本文着重介绍算法的基本原理和推导过程，不涉及具体环境中的使用。如果希望了解Paxos在Ceph系统中的使用可以参考我的另外一篇博客。</p>","more":"<h3 id=\"一致性问题\"><a href=\"#一致性问题\" class=\"headerlink\" title=\"一致性问题\"></a>一致性问题</h3><p><img src=\"http://7vzu17.com1.z0.glb.clouddn.com/2014/09/30/paxos/paxos_alg.jpg\" alt=\"\"></p>\n<p>如上图所示，服务器Ai(i=1,2,..5)组成存储集群，每份数据在5台服务器中各保留一个副本。当客户端C1和C2同时修改存储在集群中的同一份数据时，由于网络延迟的存在无法保证两个修改数据的请求到达每台服务器的先后顺序。也就是说，可能出现C1的请求先到达A1服务器，而C2的请求先到达A2服务器的情况。这种情况下，在A1和A2服务器上对数据的修改顺序不同，而修改顺序的不同直接导致了这两个副本数据的不一致。</p>\n<p>要保持副本一致只要保证在每台服务器上对数据修改的操作顺序相同。保证修改操作的顺序相同的最简单方法是采用Primary-Secondary模式，所有对数据的修改操作都先发送到Primary主机，然后由Primary主机分发给Secondary主机，也就是说修改操作的顺序由Primary主机决定。但这种方法存在单点故障问题。另一种方法是选举。当一台主机接收到修改数据的请求时并不直接修改数据，而是发起一次选举，只有赢得选举的请求才可以修改数据，并且修改操作在所有主机上进行。也就是说修改操作的顺序由集群中的所有主机共同决定。举个例子，如果在A1接收到C1请求的同时，A2也接收到了C2的请求，这时它们同时发起选举以执行自己的修改操作，选举的结果只有一个请求被批准。假如C2请求得到批准，那么所有主机上先执行C2请求。A1选举失败后重新发起选举，在得到批准后，所有主机执行C1的请求。从而，通过两轮选举确定了C1和C2请求在所有主机上的执行顺序。</p>\n<p>上面的第二个方法的关键是如何由集群中的所有节点共同选择一个值。</p>\n<h3 id=\"选择一个值-Value\"><a href=\"#选择一个值-Value\" class=\"headerlink\" title=\"选择一个值(Value)\"></a>选择一个值(Value)</h3><p>执行一次Paxos算法只能批准(chonsen)一个值，否则，仍旧无法确定值的先后顺序。<br>提案得到批准的条件是，存在一个多数派，它的每个成员都接受(Accept)这个提案。<strong>假定每个成员最多只能接受一个提案</strong>，那么如果不是多数派，就会出现一次Paxos算法批准多个值的问题。例如，假设只要有2个节点接受提案就批准提案，当A1、A2同时提出提案，A1和A3接受A1的提案，A2和A4接受A2的提案，因为它们都满足被批准的条件，所以A1和A2的提案都得到批准。 因为任意两个多数派之间至少存在一个公共的成员，所以可以保证得到多数派支持的提案是唯一的。继续上面的例子，假设A5接受了A1的提案，那么就不能再接受A2的提案，从而A1的提案得到多数派的支持获得批准。</p>\n<p><strong>P1.每个Acceptor必须接受它接收到的第一个提案</strong>。<br>提出约束条件P1的原因是，我们希望在只有一个节点提出提案的情况下，这个提案可以得到批准。但P1会引入另外一个问题，就是可能无法形成多数派。例如，A1到A5每个节点在同一时刻提出提案，并且自己率先接受了提案。由于每个节点最多只能接受一个提案，因此每个提案都只有一个节点接受，无法形成多数派。为了可以形成多数派，放宽条件，<strong>让每个Acceptor节点可以接受多个提案</strong>。但这又会引入新问题，继续前面的例子，如果每个节点接受了所有接收到的提案，那么5个提案都会得到批准，从而又出现了执行一次算法批准多个提案的问题。</p>\n<p><strong>P2. 如果一个值为v的提案被批准，那么提案号更大的提案得到批准的条件是它的值也为v</strong>。<br>为解决上述问题，提出了约束条件P2。在每个Acceptor可以接受多个提案的情况下，会导致多个提案得到批准。这是不允许的，但是如果每个被批准的提案的值都相同，那就相当于只批准了一个提案，这是可以接受的。我们直接对提案的结果进行约束，允许批准多个提案，但是这些提案的值必须相同，也就是必须同第一个被批准的提案的值相同。</p>\n<p><strong>P2a. 如果一个值为v的提案被批准，那么Acceptor只能接受提案号更大并且值为v的提案</strong>。<br>事实上，只要满足P1和P2两个约束条件，就能够保证执行一次算法只批准一个提案了。但P2直接对结果进行约束，可操作性差。为此，需要寻找一个比P2更强的约束P2a，只要满足了P2a也就满足了P2。因为每个提案都需要Acceptor接受后才能够被批准，因此P2a对Acceptor进行约束，让它只接受提案号更大并且值为v的提案。考虑到P1也是对Acceptor的约束，分析下这两者有无冲突。假设在A1提出提案时，A2处于离线状态，并且由于消息丢失A3没有接收到A1的提案。因此，A1的提案只被A1、A4和A5接受，但这已经形成多数派，所以A1的提案被批准。随后，A2上线提出了提案号更大但值不同的提案，A3接收到A2的提案，并且这是它接收到的第一个提案，根据P1的约束，它应该无条件接受这个提案。但是，根据P2a约束，因为提案的值和已经被批准的提案的值不相同，所以不能接受A2的提案。P1和P2a出现了冲突。</p>\n<p><strong>P2b. 如果一个值为v的提案被批准，那么Proposer只能提出提案号更大并且值为v的提案</strong>。<br>上面导致P1和P2a冲突的根源是，A2上线后贸然提出了值和已经批准的提案的值不相同的提案。为此，我们对Proposer进行约束，让它提出提案号更大的提案的值同已经被批准的提案的值相同。这样就避免了P1和P1a的冲突。另一方面，因为所有被Acceptor接受的提案都是由Proposer提出的，所以满足了P2b约束就满足了P2a约束，满足了P2a约束就满足了P2约束。</p>\n<p><strong>P2c. 如果有编号为n值为v的提案，那么存在一个多数派S，(a)要么S中的成员都没有接受过编号小于n的提案，(b)要么S接受的所有编号小于n的提案中提案号最大的提案的值为v</strong>。<br>感觉从P2b到P2c有点跳跃，不过还是可以证明只要满足了P2c就能满足P2b。即要证明：如果一个编号为m值为v的提案被批准(P2b的假设条件)，在满足P2c的条件下，那么Proposer提出的编号为n(n&gt;m)的提案的值为v。证明过程采用数学归纳法。</p>\n<p>当n=m+1时，采用反证法。即假设Proposer提出的编号为n的提案的值不为v而为w，那么必定会和P2c的条件相冲突。根据P2c，如果有编号为n值为w的提案，那么存在一个多数派S1，(a)要么S1中所有成员都没有接受过编号小于n的提案，(b)要么S1中所有接受到的编号小于n的提案中编号最大的提案的值为w。因为编号为m值为v的提案已经被批准，批准该提案的多数派C和S1之间至少存在一个公共成员，这个成员接受过编号为m值为n的提案，所以(a)不成立。因为小于n的最大编号是m，而编号m的提案的值为v不为w,所以(b)不成立。从而，假设不成立，即Proposer提出的编号为n的提案的值为v。</p>\n<p>当n&gt;m+1时，采用数学归纳法，即P2b在编号为m+1到n-1时都成立，要证明在编号为n时P2b依旧成立。证明过程仍然采用反证法。假设Proposer提出的编号为n的提案的值不为v而为w。那么根据P2c，存在一个多数派S1，(a)要么S1没有接受过编号小于n的提案，(b)要么S1接受的编号小于n的所有提案中编号最大的提案的值为w。因为编号为m值为v的提案被批准，而批准该提案的多数派C和S1之间存在至少一个公共成员，所以(a)不成立。另外，因为编号从m到n-1的提案的值都为v，所以(b)也不成立。因此假设不成立，P2c包含P2b的结论成立。</p>\n<p>至此，我们只要满足P1和P2c两个约束条件，就可以达到执行一次算法只批准一个提案(可以是多个提案，但提案的值相同)的目的了。满足条件P1很容易，但为满足P2c，每个Proposer提出编号为n的提案时都要先学习所有Acceptor已经接受的编号小于n且最大的提案的值，并将新提案的值设置成该提案的值。学习Acceptor已经接受的提案容易，但是要预测Acceptor将会接受的提案就比较麻烦。对此，Paxos使用了承诺策略。当Proposer向Acceptor获取编号小于n的提案时，Acceptor一方面将自己接受的<em>所有编号小于n的提案中编号最大</em>的提案返回给Proposer，另一方面承诺不再接受编号小于n的提案。也就是说在Proposer学习过程中，Acceptor不会再接受编号小于n的提案，从而Proposer就不用去预测Acceptor在这段时间内会接受哪些编号小于n的提案了。</p>\n<p>为满足P2c约束，<strong>Proposer提出提案的过程</strong>分以下两个步骤：<br>1、选择一个新的提案号n，向所有Acceptor发送<strong>Prepare请求</strong>，以获取：(a) 让Acceptor承诺不再接受编号小于n的提案；(b) Acceptor返回它接受的<em>所有编号小于n的提案中编号最大</em>的提案。<br>2、Proposer接受到所有Prepare请求的回应后，提出编号为n值为v的提案。值v是所有Acceptor已经接受的<em>所有编号小于n的提案中的编号最大</em>的提案的值。如果所有的Acceptor都没有接受过编号小于n的提案，那么v的值可以由Proposer自己指定。</p>\n<p><strong>P1a. 如果Acceptor承诺过不再接受编号小于m(m&gt;n)的提案，则拒绝接受编号为n的提案，否则，接受编号为n的提案</strong>。<br>当Proposer提出编号为n的提案后，向Acceptor发送<strong>Accept请求</strong>，以让Acceptor接受自己的提案。Acceptor根据P1a的原则来决定是否接受提案。</p>\n<h3 id=\"算法流程\"><a href=\"#算法流程\" class=\"headerlink\" title=\"算法流程\"></a>算法流程</h3><p>为满足P1a和P2c两个约束，将算法的执行流程分成两个阶段：<br><strong>Phase1</strong><br>(a) Proposer选择一个提案号n，向所有的Acceptor发送Prepare消息；<br>(b) 如果Acceptor还没有承若过不接受编号小于m(m&gt;n)的提案，则承诺不再接受编号小于n的提案，并返回它已经<em>接受的编号小于n的提案中编号最大</em>的提案。</p>\n<p><strong>Phase2</strong><br>(a) Proposer接受到所有Acceptor对Prepare请求的回应后，提出编号为n值为v的提案。值v是所有Acceptor接受到的所有<em>编号小于n的提案中编号最大</em>的提案的值。如果没有Acceptor接受过编号小于n的提案，则值v可由Proposer自己决定。提出提案后，向所有的Acceptor发送Accept消息，以期Acceptor接受提案。<br>(b) 如果Acceptor没有向其它的Prepare请求承诺过不再接受编号小于m(m&gt;n)的提案，则接受编号为n的提案。</p>\n<h3 id=\"活锁问题\"><a href=\"#活锁问题\" class=\"headerlink\" title=\"活锁问题\"></a>活锁问题</h3><p>考虑这样一种情况：<br>Ai(i=1,2,…,5)都没有接受过任何提案，A1提出编号为n的Prepare请求并发送给其它节点，A2~A5节点接收到A1的Prepare请求后都承诺不再接受编号小于n的提案。但是，在A1还没有发出Accept请求的时候，A2向所有节点发送了编号为n+1的Prepare请求，由于n+1大于n，所以所有节点都承诺不再接受编号小于n+1的提案。当A1提出的编号为n提案发送到各个节点时，每个节点都会拒绝接受编号为n的提案。同样地，在A2的Accept请求还没送到Ai节点时，Ai节点又接受到编号为n+2的Prepare请求并承诺不再接受编号小于n+2的提案，因此A2的提案又无法获得批准。如此循环往复，始终无法批准提案。这就是活锁问题。</p>\n<p>产生活锁问题的原因在于，无法控制Proposer提出提案的时机，在一轮算法还没执行结束时就提出提案导致前面的提案被撤销。控制提案进度的方法是，选择一个Leader节点，只允许Leader节点提出提案。Leader节点可以将提案保存在队列中，等一个提案批准后再从队列中取出另外一个提案，这就避免了活锁问题。</p>\n<p>引入Leader后，Paxos算法似乎变回到了Primary-Secondary模式，值的执行顺序完全由Leader决定，并且Leader存在单点故障。但<strong>Paxos算法的优势在于无论是Leader、Acceptor宕机都能保证正常工作</strong>。 如果Leader宕机，那么就要执行选举算法从现有的节点中选举出一个Leader。新Leader将从其余节点中收集信息，恢复状态。如果Acceptor宕机，只要能够形成多数派，就可以保持算法正常执行。</p>\n<h3 id=\"全局唯一提案号\"><a href=\"#全局唯一提案号\" class=\"headerlink\" title=\"全局唯一提案号\"></a>全局唯一提案号</h3><p>全局唯一的提案号是影响Paxos算法正常运转的关键。如果两个Proposer提出相同的提案号，并且该编号的提案得到批准，那么在某些节点上更新这个值，在另外一些节点上更新那个值，就会出现副本不一致的问题。在单机上提出唯一的编号比较容易，只要依次递增即可。但在不同的机器上提出不同的编号，就要开动脑筋了。</p>\n<p>Ceph是这么计算提案号的：<br><strong>last_pn = (last_pn / 100 + 1)100 + rank</strong><br>last_pn是Leader节点最近提出的提案号，rank是Leader节点的编号。也就是说，所有的提案号都是100的整数倍加上节点自己的编号。当rank值小于100时，可以保证每个节点提出的提案号都是全局唯一的，具体代码参考Paxos::get_new_proposal_number()函数。</p>\n<p>参考资料</p>\n<ol>\n<li>Paxos Made Simple             </li>\n<li><a href=\"http://zh.wikipedia.org/zh/Paxos%E7%AE%97%E6%B3%95\">Paxos算法</a>             </li>\n<li><a href=\"http://blog.csdn.net/chen77716/article/details/6170235\">Paxos算法2-算法过程</a>            </li>\n</ol>\n<hr>\n<p><strong>版权声明</strong></p>\n<p>本作品采用<a href=\"https://creativecommons.org/licenses/by/2.5/cn/\">知识共享署名2.5 中国大陆许可协议</a>进行许可，欢迎转载，但转载请注明来自<a href=\"https://wuxiangwei.cn/\"><em>Shanno’s Blog</em></a>，并保持转载后文章内容的完整。本人保留所有版权相关权利，侵权必究。</p>"},{"title":"Ceph线程池","date":"2014-08-04T12:23:29.000Z","_content":"\n线程池ThreadPool的实现符合*生产者-消费者*模型，这个模型解除生产者消费者间的耦合关系，生产者可以专注处理制造产品的逻辑而不用关心产品的消费，消费者亦然。当然，生产者消费者之间需要一个连接的纽带，那就是产品接口。产品接口是对这两者的约束，生产者生产的产品要符合产品的接口，消费者依据产品接口来消费。\n\n<!--more-->\n\nThread类是ThreadPool中的消费者，它封装*pthread* API函数，对外提供*Thread::entry()*作为线程的入口函数。Thread只是对消费者的抽象，WorkThread才是ThreadPool的具体消费者，它实现Thread接口函数，设置线程的入口函数为ThreadPool的*worker()*方法。\n\nWorkQueue\\_类在ThreadPool中代表产品接口，对外表现为一个队列，队列中存储待处理的数据元素。消费者WorkThread对它的处理主要分成三个步骤：首先调用*\\_void\\_dequeue()*方法获取队列元素，然后通过*\\_void\\_process()*方法处理元素，最后使用*\\_void\\_process\\_finish()*方法进行收尾工作。具体调用过程可以参看WorkTread的线程入口函数，也就是*ThreadPool::work()*方法。\n\n```C\ntemplate<typename T, typename U=T>\nclass WorkQueueVal: public WorkQueue_{\n    virtual void _enqueue(T) = 0;\n    virtual U _dequeue() = 0;    // 向WorkQueueVal加入的元素的类型为T，而读出的元素的类型为U。\n};\n```\n\nWorkQueueVal<T,U>是WorkQueue的纯虚子类。它包含两个列表，一个为*to_process*用于保存待处理的元素，另一个为*to_finish*用于保存已经处理过的元素。在*\\_void_dequeue()*时从其子类中取出元素放入*to_process*列表，以便在*\\_void\\_process()*时从中取出元素进行处理，并把处理后的元素放入*to_finish*列表。最后，在ThreadWork线程调用*\\_void_process_finish()*函数时，从*to_finish*列表中取出元素做最后的处理。此外，需要注意的是WorkQueueVal<T, U>添加元素时使用的类型是T，而读取元素时的类型变成了U，需要子类自己负责转换。或许，这就是WorkQueueVal<T, U>类的设计目的。\n\n","source":"_posts/Ceph/thread_pool.md","raw":"title: \"Ceph线程池\" \ndate: 2014-08-04 20:23:29 \ncategories: [Ceph]\ntags: [Ceph]\n---\n\n线程池ThreadPool的实现符合*生产者-消费者*模型，这个模型解除生产者消费者间的耦合关系，生产者可以专注处理制造产品的逻辑而不用关心产品的消费，消费者亦然。当然，生产者消费者之间需要一个连接的纽带，那就是产品接口。产品接口是对这两者的约束，生产者生产的产品要符合产品的接口，消费者依据产品接口来消费。\n\n<!--more-->\n\nThread类是ThreadPool中的消费者，它封装*pthread* API函数，对外提供*Thread::entry()*作为线程的入口函数。Thread只是对消费者的抽象，WorkThread才是ThreadPool的具体消费者，它实现Thread接口函数，设置线程的入口函数为ThreadPool的*worker()*方法。\n\nWorkQueue\\_类在ThreadPool中代表产品接口，对外表现为一个队列，队列中存储待处理的数据元素。消费者WorkThread对它的处理主要分成三个步骤：首先调用*\\_void\\_dequeue()*方法获取队列元素，然后通过*\\_void\\_process()*方法处理元素，最后使用*\\_void\\_process\\_finish()*方法进行收尾工作。具体调用过程可以参看WorkTread的线程入口函数，也就是*ThreadPool::work()*方法。\n\n```C\ntemplate<typename T, typename U=T>\nclass WorkQueueVal: public WorkQueue_{\n    virtual void _enqueue(T) = 0;\n    virtual U _dequeue() = 0;    // 向WorkQueueVal加入的元素的类型为T，而读出的元素的类型为U。\n};\n```\n\nWorkQueueVal<T,U>是WorkQueue的纯虚子类。它包含两个列表，一个为*to_process*用于保存待处理的元素，另一个为*to_finish*用于保存已经处理过的元素。在*\\_void_dequeue()*时从其子类中取出元素放入*to_process*列表，以便在*\\_void\\_process()*时从中取出元素进行处理，并把处理后的元素放入*to_finish*列表。最后，在ThreadWork线程调用*\\_void_process_finish()*函数时，从*to_finish*列表中取出元素做最后的处理。此外，需要注意的是WorkQueueVal<T, U>添加元素时使用的类型是T，而读取元素时的类型变成了U，需要子类自己负责转换。或许，这就是WorkQueueVal<T, U>类的设计目的。\n\n","slug":"Ceph/thread_pool","published":1,"updated":"2016-12-15T07:18:40.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cix1kgau1000odaei3xt886ar","content":"<p>线程池ThreadPool的实现符合<em>生产者-消费者</em>模型，这个模型解除生产者消费者间的耦合关系，生产者可以专注处理制造产品的逻辑而不用关心产品的消费，消费者亦然。当然，生产者消费者之间需要一个连接的纽带，那就是产品接口。产品接口是对这两者的约束，生产者生产的产品要符合产品的接口，消费者依据产品接口来消费。</p>\n<a id=\"more\"></a>\n<p>Thread类是ThreadPool中的消费者，它封装<em>pthread</em> API函数，对外提供<em>Thread::entry()</em>作为线程的入口函数。Thread只是对消费者的抽象，WorkThread才是ThreadPool的具体消费者，它实现Thread接口函数，设置线程的入口函数为ThreadPool的<em>worker()</em>方法。</p>\n<p>WorkQueue_类在ThreadPool中代表产品接口，对外表现为一个队列，队列中存储待处理的数据元素。消费者WorkThread对它的处理主要分成三个步骤：首先调用<em>_void_dequeue()</em>方法获取队列元素，然后通过<em>_void_process()</em>方法处理元素，最后使用<em>_void_process_finish()</em>方法进行收尾工作。具体调用过程可以参看WorkTread的线程入口函数，也就是<em>ThreadPool::work()</em>方法。</p>\n<figure class=\"highlight c\"><table><tr><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">template</span>&lt;<span class=\"keyword\">typename</span> T, <span class=\"keyword\">typename</span> U=T&gt;</div><div class=\"line\"><span class=\"keyword\">class</span> WorkQueueVal: <span class=\"keyword\">public</span> WorkQueue_&#123;</div><div class=\"line\">    <span class=\"keyword\">virtual</span> <span class=\"keyword\">void</span> _enqueue(T) = <span class=\"number\">0</span>;</div><div class=\"line\">    <span class=\"keyword\">virtual</span> U _dequeue() = <span class=\"number\">0</span>;    <span class=\"comment\">// 向WorkQueueVal加入的元素的类型为T，而读出的元素的类型为U。</span></div><div class=\"line\">&#125;;</div></pre></td></tr></table></figure>\n<p>WorkQueueVal<t,u>是WorkQueue的纯虚子类。它包含两个列表，一个为<em>to_process</em>用于保存待处理的元素，另一个为<em>to_finish</em>用于保存已经处理过的元素。在<em>_void_dequeue()</em>时从其子类中取出元素放入<em>to_process</em>列表，以便在<em>_void_process()</em>时从中取出元素进行处理，并把处理后的元素放入<em>to_finish</em>列表。最后，在ThreadWork线程调用<em>_void_process_finish()</em>函数时，从<em>to_finish</em>列表中取出元素做最后的处理。此外，需要注意的是WorkQueueVal<t, u=\"\">添加元素时使用的类型是T，而读取元素时的类型变成了U，需要子类自己负责转换。或许，这就是WorkQueueVal<t, u=\"\">类的设计目的。</t,></t,></t,u></p>\n<hr>\n<p><strong>版权声明</strong></p>\n<p>本作品采用<a href=\"https://creativecommons.org/licenses/by/2.5/cn/\" target=\"_blank\" rel=\"external\">知识共享署名2.5 中国大陆许可协议</a>进行许可，欢迎转载，但转载请注明来自<a href=\"https://wuxiangwei.cn/\"><em>Shanno’s Blog</em></a>，并保持转载后文章内容的完整。本人保留所有版权相关权利，侵权必究。</p>\n","excerpt":"<p>线程池ThreadPool的实现符合<em>生产者-消费者</em>模型，这个模型解除生产者消费者间的耦合关系，生产者可以专注处理制造产品的逻辑而不用关心产品的消费，消费者亦然。当然，生产者消费者之间需要一个连接的纽带，那就是产品接口。产品接口是对这两者的约束，生产者生产的产品要符合产品的接口，消费者依据产品接口来消费。</p>","more":"<p>Thread类是ThreadPool中的消费者，它封装<em>pthread</em> API函数，对外提供<em>Thread::entry()</em>作为线程的入口函数。Thread只是对消费者的抽象，WorkThread才是ThreadPool的具体消费者，它实现Thread接口函数，设置线程的入口函数为ThreadPool的<em>worker()</em>方法。</p>\n<p>WorkQueue_类在ThreadPool中代表产品接口，对外表现为一个队列，队列中存储待处理的数据元素。消费者WorkThread对它的处理主要分成三个步骤：首先调用<em>_void_dequeue()</em>方法获取队列元素，然后通过<em>_void_process()</em>方法处理元素，最后使用<em>_void_process_finish()</em>方法进行收尾工作。具体调用过程可以参看WorkTread的线程入口函数，也就是<em>ThreadPool::work()</em>方法。</p>\n<figure class=\"highlight c\"><table><tr><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">template</span>&lt;<span class=\"keyword\">typename</span> T, <span class=\"keyword\">typename</span> U=T&gt;</div><div class=\"line\"><span class=\"keyword\">class</span> WorkQueueVal: <span class=\"keyword\">public</span> WorkQueue_&#123;</div><div class=\"line\">    <span class=\"keyword\">virtual</span> <span class=\"keyword\">void</span> _enqueue(T) = <span class=\"number\">0</span>;</div><div class=\"line\">    <span class=\"keyword\">virtual</span> U _dequeue() = <span class=\"number\">0</span>;    <span class=\"comment\">// 向WorkQueueVal加入的元素的类型为T，而读出的元素的类型为U。</span></div><div class=\"line\">&#125;;</div></pre></td></tr></table></figure>\n<p>WorkQueueVal<T,U>是WorkQueue的纯虚子类。它包含两个列表，一个为<em>to_process</em>用于保存待处理的元素，另一个为<em>to_finish</em>用于保存已经处理过的元素。在<em>_void_dequeue()</em>时从其子类中取出元素放入<em>to_process</em>列表，以便在<em>_void_process()</em>时从中取出元素进行处理，并把处理后的元素放入<em>to_finish</em>列表。最后，在ThreadWork线程调用<em>_void_process_finish()</em>函数时，从<em>to_finish</em>列表中取出元素做最后的处理。此外，需要注意的是WorkQueueVal<T, U>添加元素时使用的类型是T，而读取元素时的类型变成了U，需要子类自己负责转换。或许，这就是WorkQueueVal<T, U>类的设计目的。</p>\n<hr>\n<p><strong>版权声明</strong></p>\n<p>本作品采用<a href=\"https://creativecommons.org/licenses/by/2.5/cn/\">知识共享署名2.5 中国大陆许可协议</a>进行许可，欢迎转载，但转载请注明来自<a href=\"https://wuxiangwei.cn/\"><em>Shanno’s Blog</em></a>，并保持转载后文章内容的完整。本人保留所有版权相关权利，侵权必究。</p>"},{"title":"dmClock源码分析之编译","date":"2016-11-10T06:01:26.000Z","toc":true,"_content":"\n编译步骤参考README.md文件。\n\n**问题1**\n```\nCMake Error at /usr/share/cmake-2.8/Modules/FindPackageHandleStandardArgs.cmake:108 (message):\n  Could NOT find gtest (missing: GTEST_LIBRARY GTEST_MAIN_LIBRARY\n  GTEST_INCLUDE_DIRS)\n```\n\n解决方法\n``` shell\napt-get install libgtest-dev \ncd /usr/src/gtest/\ncmake CMakeLists.txt \nmake\ncp *.a /usr/lib/\n```\n参考 [Getting started with Google Test (GTest) on Ubuntu](https://www.eriksmistad.no/getting-started-with-google-test-on-ubuntu/)\n\n\n**问题2**\n```\nCMake Error at /usr/share/cmake-2.8/Modules/FindBoost.cmake:1131 (message):\n  Unable to find the requested Boost libraries.\n```\n\n解决方法\n``` shell\napt-get install libboost-dev\n```\n","source":"_posts/DmClock/MCLOCK_compile.md","raw":"---\ntitle: \"dmClock源码分析之编译\"\ndate: 2016-11-10 14:01:26\ncategories: [mClock]\ntags: [mClock]\ntoc: true\n---\n\n编译步骤参考README.md文件。\n\n**问题1**\n```\nCMake Error at /usr/share/cmake-2.8/Modules/FindPackageHandleStandardArgs.cmake:108 (message):\n  Could NOT find gtest (missing: GTEST_LIBRARY GTEST_MAIN_LIBRARY\n  GTEST_INCLUDE_DIRS)\n```\n\n解决方法\n``` shell\napt-get install libgtest-dev \ncd /usr/src/gtest/\ncmake CMakeLists.txt \nmake\ncp *.a /usr/lib/\n```\n参考 [Getting started with Google Test (GTest) on Ubuntu](https://www.eriksmistad.no/getting-started-with-google-test-on-ubuntu/)\n\n\n**问题2**\n```\nCMake Error at /usr/share/cmake-2.8/Modules/FindBoost.cmake:1131 (message):\n  Unable to find the requested Boost libraries.\n```\n\n解决方法\n``` shell\napt-get install libboost-dev\n```\n","slug":"DmClock/MCLOCK_compile","published":1,"updated":"2016-12-15T07:18:40.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cix1kgau4000rdaeiwh5ya24f","content":"<p>编译步骤参考README.md文件。</p>\n<p><strong>问题1</strong><br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">CMake Error at /usr/share/cmake-2.8/Modules/FindPackageHandleStandardArgs.cmake:108 (message):</div><div class=\"line\">  Could NOT find gtest (missing: GTEST_LIBRARY GTEST_MAIN_LIBRARY</div><div class=\"line\">  GTEST_INCLUDE_DIRS)</div></pre></td></tr></table></figure></p>\n<p>解决方法<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">apt-get install libgtest-dev </div><div class=\"line\">cd /usr/src/gtest/</div><div class=\"line\">cmake CMakeLists.txt </div><div class=\"line\">make</div><div class=\"line\">cp *.a /usr/lib/</div></pre></td></tr></table></figure></p>\n<p>参考 <a href=\"https://www.eriksmistad.no/getting-started-with-google-test-on-ubuntu/\" target=\"_blank\" rel=\"external\">Getting started with Google Test (GTest) on Ubuntu</a></p>\n<p><strong>问题2</strong><br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">CMake Error at /usr/share/cmake-2.8/Modules/FindBoost.cmake:1131 (message):</div><div class=\"line\">  Unable to find the requested Boost libraries.</div></pre></td></tr></table></figure></p>\n<p>解决方法<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">apt-get install libboost-dev</div></pre></td></tr></table></figure></p>\n<hr>\n<p><strong>版权声明</strong></p>\n<p>本作品采用<a href=\"https://creativecommons.org/licenses/by/2.5/cn/\" target=\"_blank\" rel=\"external\">知识共享署名2.5 中国大陆许可协议</a>进行许可，欢迎转载，但转载请注明来自<a href=\"https://wuxiangwei.cn/\"><em>Shanno’s Blog</em></a>，并保持转载后文章内容的完整。本人保留所有版权相关权利，侵权必究。</p>\n","excerpt":"","more":"<p>编译步骤参考README.md文件。</p>\n<p><strong>问题1</strong><br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">CMake Error at /usr/share/cmake-2.8/Modules/FindPackageHandleStandardArgs.cmake:108 (message):</div><div class=\"line\">  Could NOT find gtest (missing: GTEST_LIBRARY GTEST_MAIN_LIBRARY</div><div class=\"line\">  GTEST_INCLUDE_DIRS)</div></pre></td></tr></table></figure></p>\n<p>解决方法<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">apt-get install libgtest-dev </div><div class=\"line\">cd /usr/src/gtest/</div><div class=\"line\">cmake CMakeLists.txt </div><div class=\"line\">make</div><div class=\"line\">cp *.a /usr/lib/</div></pre></td></tr></table></figure></p>\n<p>参考 <a href=\"https://www.eriksmistad.no/getting-started-with-google-test-on-ubuntu/\">Getting started with Google Test (GTest) on Ubuntu</a></p>\n<p><strong>问题2</strong><br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">CMake Error at /usr/share/cmake-2.8/Modules/FindBoost.cmake:1131 (message):</div><div class=\"line\">  Unable to find the requested Boost libraries.</div></pre></td></tr></table></figure></p>\n<p>解决方法<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">apt-get install libboost-dev</div></pre></td></tr></table></figure></p>\n<hr>\n<p><strong>版权声明</strong></p>\n<p>本作品采用<a href=\"https://creativecommons.org/licenses/by/2.5/cn/\">知识共享署名2.5 中国大陆许可协议</a>进行许可，欢迎转载，但转载请注明来自<a href=\"https://wuxiangwei.cn/\"><em>Shanno’s Blog</em></a>，并保持转载后文章内容的完整。本人保留所有版权相关权利，侵权必究。</p>\n"},{"title":"dmClock源码分析之DMC模块","date":"2016-11-17T07:01:26.000Z","toc":true,"_content":"\n# Tag计算\n\nTag值的计算依赖于两类参数，一类是用户配置的上限(limit)、下限(reservation)以及权重(weight)，另一类是动态统计出来的delta和rho。根据dmClock算法的描述，delta和rho这两个参数由客户端来统计，并将其夹带到请求中发送给服务端。\n\n<!--more-->\n\n## 客户端\n\n从客户端发送请求到请求到达服务端的内部队列的函数调用过程，后文记为**过程A**：\n```\nSimulatedClient::run_req() --> SimulatedClient::submit_f() --> SimulatedServer::post() --> \nPushPriorityQueue::add_request() --> PushPriorityQueue::schedule_request() --> \nPushPriorityQueue::submit_request() --> PushPriorityQueue::submit_top_request() --> \nPriorityQueueBase::pop_process_request() --> PushPriorityQueue::handle_f() --> SimulatedServer::inner_post()\n```\n\n从服务端发送响应到客户端接收到响应的函数调用过程，后文记为**过程B**：\n```\nSimulatedServer::run() --> SimulatedServer::client_resp_f() --> \nSimulatedClient::receive_response() --> SimulatedClient::run_resp() --> ServiceTracker::track_resp()\n```\n\n根据dmClock算法，发送给给定服务端的请求中携带的delta的定义为：\n> 从最近一次发送给该服务端请求的时刻到当前时刻这段时间内，客户发送给其它服务端的请求的数目。\n\n为计算delta，对任意一台服务端发送请求时记录下此时全局已发送的请求数目，计算delta时只要将当前的全局已发送请求数目减去最近一次发送时的全局已发送请求数目即可。算法实现中使用响应来近似请求，具体做法如下：\n\n对给定的服务端，使用t1代表最近一次客户向其发送请求的时刻，t2代表当前时刻，即本次向其发送请求的时刻。算法实现中用到的关键数据结构：\n\n``` C++\n// 一个服务端的响应统计情况\nstruct ServerInfo {\n    Counter delta_prev_req;  // t1时刻的全局响应数目\n    Counter rho_prev_req;  // t1时刻的全局reservation响应数目\n    uint32_t my_delta;  // >t1时间段内，属于本服务端的响应数目\n    uint32_t my_rho;  // >t1的时间段内，属于本服务端的reservation响应数目\n};\n\ntemplate<typename S>\nclass ServiceTracker {\n    Counter delta_counter;  // 所有服务端的响应数目\n    Counter rho_counter;  // 所有服务端的Reservation响应数目\n    std::map<S, ServerInfo> server_map;  // 每个服务端独立的响应统计情况\n};\n```\n\n**过程B**每接收到一个响应，递增全局的delta_counter以及特定服务端的my_delta。对rho也一样，详细过程参考ServiceTracker::track_resp函数。\n**过程A**在准备请求阶段计算delta，公式如下：\n\n``` C++\ndelta = 1 + delta_counter - it->second.delta_prev_req - it->second.my_delta\n```\n\n其中，it代表ServiceTracker::server_map的迭代器。计算delta时之所以减去my_delta，是因为在t1~t2这个时间段内除了能够接收到其它服务端的响应外还会接收到来自自己服务端的响应，因此要减去自己服务端的响应个数。\n\n## 服务端\n\n从客户端发送请求到请求到达服务端的优先级队列的函数调用过程：\n\n```\nSimulatedClient::run_req() --> SimulatedClient::submit_f() --> SimulatedServer::post() --> \nPushPriorityQueue::add_request() --> PushPriorityQueue::do_add_request()\n```\n在这过程中，请求被添加上了一个伪Tag。之所以假是因为Tag中除当前时间外其它的内容都为0。具体参看PushPriorityQueue::do_add_request函数。\n\n从请求开始调度到请求送到服务端的内部队列的函数调用过程：\n```\nPushPriorityQueue::schedule_request() --> PushPriorityQueue::submit_request() -->\nPushPriorityQueue::submit_top_request() --> PriorityQueueBase::pop_process_request() --> \nPushPriorityQueue::handle_f() --> SimulatedServer::inner_post()\n```\nPriorityQueueBase::pop_process_request开始计算请求的tag值，具体计算过程在RequestTag::tag_calc函数完成，计算过程和算法描述基本一致。注意，对第一个请求，RequestTag计算Tag时，依赖的前个请求为默认的prev_tag(0.0, 0.0, 0.0, TimeZero)，这样第一个请求的各个Tag值将为请求到达的时间。\n\n\n### 数据结构\n\n#### DMC客户\n\n``` C++\n// C: 客户ID\n// R: 请求类型\n// B: B叉堆，堆的分叉数\ntemplate<typename C, typename R, uint B>\nclass PriorityQueueBase {\n     std::map<C,ClientRecRef> client_map;  // 每个客户的记录\n};\n\nclass ClientRec {\n    uint32_t cur_rho;\n    uint32_t cur_delta;\n    ClientInfo info;  // 包含limit、reservation、proportion参数及其倒数\n\n    RequestTag  prev_tag;  // 前一个请求的Tag值\n    std::deque<ClientReq> requests;  // 请求队列\n};\n```\n服务端中每个客户即ClientRec类的一个实例，客户的请求保存到ClientRec::requests队列，该队列是FCFS队列。也就说，同个客户端的请求的相对顺序保持不变。dmClock改变的是不同客户端间的处理顺序。\n\n\n#### 请求堆\n\n``` C++\n// I: 元素类型\n// T: 元素类型 \n// C: 用于比较两个T类型的参数大小的函数类\n// K: 堆的分叉，K=2代表2叉堆\n// heap_info: T的成员变量\ntemplate<typenmae I, typename T, IndIntruHeapData T::*heap_info, typename C, uint K=2>\nclass IndIntruHeap {\nprotected:\n    std::vector<I> data;  // 存储元素的容器\n    index_t count;  // 元素个数\n    C comparator;  // 重建堆时，比较两元素的优先顺序\npublic:\n    void push(const I& item);  // 加入新元素\n    void adjust(T& item);\n\nprotected:\n    // 从下至上重建堆\n    void shift_up(index_t i);\n    // 由上到下重建堆\n    // 如果i为最后1个元素，不调整\n    // \n    typename std::enable_if<(K>2)&&EnableBool,void>::type sift_down(index_t i);\n    // 给定元素被修改后，调整其在堆中的位置\n    // 如果i为根节点，则向下调整\n    // 如果i大于其parent节点，则向上调整\n    // 如果i小于其parent节点，则向下调整\n    void sift(index_t i);\n};\n```\n参考[浅谈算法和数据结构: 五 优先级队列与堆排序](http://www.cnblogs.com/yangecnu/archive/2014/03/02/3577568.html) 获取更多关于堆的内容。\n\n\n``` C++\nvoid IndIntruHeap::sift_up(index_t i) {\n    // 入参i代表元素在IndIntruHeap::data中的位置（或者说，数组下标）\n    while (i > 0) {\n        index_t pi = parent(i);  // i的父节点位置\n        if (!comparator(*data[i], *data[pi])) {\n            // data[i] 优先级低于 data[pi]，保持相对结构不变\n            break;\n        }\n\n        // data[i] 优先级低于 data[pi]，将data[i]上移\n        // 因为pi的左节点的优先级低于pi，而pi的优先级低于i\n        // 所以pi的左节点的优先级也低于i，i可以作为其父节点，不必再进行比较\n        std::swap(data[i], data[pi]);\n        // 修改数据的位置信息\n        intru_data_of(data[i]) = i;\n        intru_data_of(data[pi]) = pi;\n        // data[i] 继续与其新的父节点进行比较\n        i = pi;\n   }\n} \n\n// 此处的std::enable_if可以理解为返回值函数重载，实际上是模板偏特化\n// 代码中实现了连个shift_down函数，用以区分K==2和K>2两种情况，这两种情况分别调用两个不同的函数\ntemplate<bool EnableBool=true>\ntypename std::enable_if<(K>2)&&EnableBool,void>::type IndIntruHeap::sift_down(index_t i) {\n    // count为元素数目，i大于它为非法\n    if (i >= count) return;\n    while (true) {\n        // li为i最左边的子节点\n        index_t li = lhs(i);\n\n        if (li < count) {\n            // ri为i最右边的子节点\n            index_t ri = std::min(rhs(i), count - 1);\n\n            // 查找优先级最高的子节点\n            index_t min_i = li;\n            for (index_t k = li + 1; k <= ri; ++k) {\n                if (comparator(*data[k], *data[min_i])) {\n                    min_i = k;\n                }\n            }\n\n            if (comparator(*data[min_i], *data[i])) {\n                // i优先级低于子节点min_i，交换data[i]和data[min_i]的位置\n                // 因为min_i已经是i所有子节点中优先级最高的，因此它可以作为其余子节点的父节点\n                std::swap(data[i], data[min_i]);\n                intru_data_of(data[i]) = i;\n                intru_data_of(data[min_i]) = min_i;\n                // i继续和新的子节点进行比较\n                i = min_i;\n            } else {\n                // i节点比所有的子节点优先级都高，退出\n                break;\n            }\n        } else {\n            // i节点已是叶子节点\n            break;\n        }\n    }\n} // sift_down \n```\n\n### 优先级比较\n\n``` C++\n// 请求的Tag，包含预留(reservation)、上限(limit)和权重(proportion)。\nclass RequestTag {\n    double reservation;  // 预留\n    double proportion;  // 权重\n    double limit;  // 上限\n    bool ready;  // 请求是否允许在Weight-based阶段被处理，当Limit低于当前时间（请求调度）\n    Time arrival;  // 接收到请求的时间\n};\n```\n请求调度的Weight-based阶段，先从Limit堆中筛选出能够在这个阶段被处理的请求，设置请求的ready字段为True。筛选的条件是请求的L Tag低于当前时间。筛选结束后再从Ready堆中选择请求。详情参考 PriorityQueueBase::do_next_request函数。\n\n``` C++\n// tag_field代表RequestTag类的一个成员变量\ntemplate<double RequestTag::*tag_field, ReadyOption ready_opt, bool use_prop_delta>\nstruct ClientCompare {\n    bool operator()(const ClientRec& n1, const ClientRec& n2) const {\n        // n1没有请求，n2有请求，返回false\n        // n1没有请求，n2也没有请求，返回false，保持堆结构不变\n        // n1有请求，n2没有请求，返回true\n        // n1有请求，n1也有请求\n    }\n};\n```\n比较客户n1和客户n2的Next请求的优先级。根据mClock算法，Tag值越小优先级应该越高。该函数的本意是，判断“n1的Next请求的优先级高于n2？”。下文使用tag1和tag2分别表示n1和n2的Next请求的Tag值。\n\n对**reservation堆**，依据“谁小谁优先级高”的原则，若tag1小于tag2，则返回True；\n对**ready堆**，如果t1、t2两者的ready状态相同（都为true或者都为false），那么遵循“谁小谁优先级高”的原则；如果t1、t2的两者的ready状态不同（一个为true一个为false），那么依据“**谁ready谁优先级高**”的原则；\n对**limit堆**，如果t1、t2两者的ready状态相同，那么遵循“谁小谁优先级高”的原则。如果两者的ready状态不同，按照“**谁没ready谁优先级高**”的原则，对已经Ready的请求将在请求调度中直接处理掉。\n\n### Tag校准\n\n根据mClock算法，当有**新VM**启动时所有**老VM**请求的P Tag都要向左偏移一个位置（即P Tag减去偏移量），以避免新VM请求偏低影响公平性。偏移量为所有老VM的剩余请求中最小的P Tag减去当前时间（新VM第一个请求的到达时间）。反过来思考，将所有老VM的剩余请求在P Tag时间轴上向左偏移和将新VM的请求向右偏移的结果是相同的，一样能够消除不公平性。但这在代码实现上就简单了，只要在新VM中记录偏移量，并在Tag比较时加上偏移量即可，不必对所有老VM的剩余请求大动干戈。\n\n``` C++\ntemplate<typename C, typename R, uint B>\nclass PriorityQueueBase {\n    c::IndIntruHeap<ClientRecRef, ClientRec, &ClientRec::reserv_heap_data,\n                    ClientCompare<&RequestTag::reservation, ReadyOption::ignore, false>,\n                    B> resv_heap;\n    c::IndIntruHeap<ClientRecRef, ClientRec, &ClientRec::lim_heap_data,\n                    ClientCompare<&RequestTag::limit, ReadyOption::lowers, false>,\n                    B> limit_heap;\n    c::IndIntruHeap<ClientRecRef, ClientRec, &ClientRec::ready_heap_data,\n                    ClientCompare<&RequestTag::proportion, ReadyOption::raises, true>,\n                    B> ready_heap;\n};\n```\n算法实现中分两步完成Tag校准：\n\n1. 接收请求时，检查Client是否空闲，若空闲则计算偏移量，并将偏移量保存到ClientRec::prop_delta变量。参考PriorityQueueBase::do_add_request函数的*if (client.idle)*分支；\n2. 比较P Tag时， 为请求的P Tag加上Client的prop_delta后再比较。参考ClientCompare的operator()函数。\n\n另外，注意只有P Tag需要校准。ClientCompare模板的最后一个参数代表是否使用prop_delta，只有ready_heap变量为True。\n\n### 判断客户为空闲\n\n客户是否为空闲，在Tag校准中非常重要，只有发现空闲的客户时才需要进行校准。以下是算法实现中，判断客户空闲的一些手段：\n- 服务端第一次接收到来自该客户的请求，就认为该客户为空闲，参考 PriorityQueueBase::do_add_request函数\n- 定时检查，将超过idle_age(10分钟)时间内没有新增请求的客户标记为idle, 参考 PriorityQueueBase::do_clean函数\n\n对定时检查策略，除了将客户标记为idle状态外还会清理长久没有发送请求的客户。默认情况下，定时检查的时间间隔为6分钟，超过10分钟没新请求的客户将被标记为idle状态，超过15分钟没有新请求的客户将被从服务端清理。\n\n### 请求调度\n\n请求调度，从堆结构中获取待处理请求的函数调用过程：\n\n```\nPushPriorityQueue::schedule_request() --> PushPriorityQueue::next_request --> PriorityQueueBase::do_next_request()\n```\nDmClock算法的请求调度流程在PriorityQueueBase::do_next_request函数中实现。\n\n**递减剩余请求的R Tag**\n根据DmClock算法，在Weight-based阶段每处理一个请求时，为避免影响R tag的排序，需要将该客户的剩余请求的R tag值递减 1/ri。这个过程在函数PushPriorityQueue::submit_request()以及PriorityQueueBase::reduce_reservation_tags函数中实现。\n","source":"_posts/DmClock/MCLOCK_dmclock.md","raw":"---\ntitle: \"dmClock源码分析之DMC模块\"\ndate: 2016-11-17 15:01:26\ncategories: [mClock]\ntags: [mClock]\ntoc: true\n---\n\n# Tag计算\n\nTag值的计算依赖于两类参数，一类是用户配置的上限(limit)、下限(reservation)以及权重(weight)，另一类是动态统计出来的delta和rho。根据dmClock算法的描述，delta和rho这两个参数由客户端来统计，并将其夹带到请求中发送给服务端。\n\n<!--more-->\n\n## 客户端\n\n从客户端发送请求到请求到达服务端的内部队列的函数调用过程，后文记为**过程A**：\n```\nSimulatedClient::run_req() --> SimulatedClient::submit_f() --> SimulatedServer::post() --> \nPushPriorityQueue::add_request() --> PushPriorityQueue::schedule_request() --> \nPushPriorityQueue::submit_request() --> PushPriorityQueue::submit_top_request() --> \nPriorityQueueBase::pop_process_request() --> PushPriorityQueue::handle_f() --> SimulatedServer::inner_post()\n```\n\n从服务端发送响应到客户端接收到响应的函数调用过程，后文记为**过程B**：\n```\nSimulatedServer::run() --> SimulatedServer::client_resp_f() --> \nSimulatedClient::receive_response() --> SimulatedClient::run_resp() --> ServiceTracker::track_resp()\n```\n\n根据dmClock算法，发送给给定服务端的请求中携带的delta的定义为：\n> 从最近一次发送给该服务端请求的时刻到当前时刻这段时间内，客户发送给其它服务端的请求的数目。\n\n为计算delta，对任意一台服务端发送请求时记录下此时全局已发送的请求数目，计算delta时只要将当前的全局已发送请求数目减去最近一次发送时的全局已发送请求数目即可。算法实现中使用响应来近似请求，具体做法如下：\n\n对给定的服务端，使用t1代表最近一次客户向其发送请求的时刻，t2代表当前时刻，即本次向其发送请求的时刻。算法实现中用到的关键数据结构：\n\n``` C++\n// 一个服务端的响应统计情况\nstruct ServerInfo {\n    Counter delta_prev_req;  // t1时刻的全局响应数目\n    Counter rho_prev_req;  // t1时刻的全局reservation响应数目\n    uint32_t my_delta;  // >t1时间段内，属于本服务端的响应数目\n    uint32_t my_rho;  // >t1的时间段内，属于本服务端的reservation响应数目\n};\n\ntemplate<typename S>\nclass ServiceTracker {\n    Counter delta_counter;  // 所有服务端的响应数目\n    Counter rho_counter;  // 所有服务端的Reservation响应数目\n    std::map<S, ServerInfo> server_map;  // 每个服务端独立的响应统计情况\n};\n```\n\n**过程B**每接收到一个响应，递增全局的delta_counter以及特定服务端的my_delta。对rho也一样，详细过程参考ServiceTracker::track_resp函数。\n**过程A**在准备请求阶段计算delta，公式如下：\n\n``` C++\ndelta = 1 + delta_counter - it->second.delta_prev_req - it->second.my_delta\n```\n\n其中，it代表ServiceTracker::server_map的迭代器。计算delta时之所以减去my_delta，是因为在t1~t2这个时间段内除了能够接收到其它服务端的响应外还会接收到来自自己服务端的响应，因此要减去自己服务端的响应个数。\n\n## 服务端\n\n从客户端发送请求到请求到达服务端的优先级队列的函数调用过程：\n\n```\nSimulatedClient::run_req() --> SimulatedClient::submit_f() --> SimulatedServer::post() --> \nPushPriorityQueue::add_request() --> PushPriorityQueue::do_add_request()\n```\n在这过程中，请求被添加上了一个伪Tag。之所以假是因为Tag中除当前时间外其它的内容都为0。具体参看PushPriorityQueue::do_add_request函数。\n\n从请求开始调度到请求送到服务端的内部队列的函数调用过程：\n```\nPushPriorityQueue::schedule_request() --> PushPriorityQueue::submit_request() -->\nPushPriorityQueue::submit_top_request() --> PriorityQueueBase::pop_process_request() --> \nPushPriorityQueue::handle_f() --> SimulatedServer::inner_post()\n```\nPriorityQueueBase::pop_process_request开始计算请求的tag值，具体计算过程在RequestTag::tag_calc函数完成，计算过程和算法描述基本一致。注意，对第一个请求，RequestTag计算Tag时，依赖的前个请求为默认的prev_tag(0.0, 0.0, 0.0, TimeZero)，这样第一个请求的各个Tag值将为请求到达的时间。\n\n\n### 数据结构\n\n#### DMC客户\n\n``` C++\n// C: 客户ID\n// R: 请求类型\n// B: B叉堆，堆的分叉数\ntemplate<typename C, typename R, uint B>\nclass PriorityQueueBase {\n     std::map<C,ClientRecRef> client_map;  // 每个客户的记录\n};\n\nclass ClientRec {\n    uint32_t cur_rho;\n    uint32_t cur_delta;\n    ClientInfo info;  // 包含limit、reservation、proportion参数及其倒数\n\n    RequestTag  prev_tag;  // 前一个请求的Tag值\n    std::deque<ClientReq> requests;  // 请求队列\n};\n```\n服务端中每个客户即ClientRec类的一个实例，客户的请求保存到ClientRec::requests队列，该队列是FCFS队列。也就说，同个客户端的请求的相对顺序保持不变。dmClock改变的是不同客户端间的处理顺序。\n\n\n#### 请求堆\n\n``` C++\n// I: 元素类型\n// T: 元素类型 \n// C: 用于比较两个T类型的参数大小的函数类\n// K: 堆的分叉，K=2代表2叉堆\n// heap_info: T的成员变量\ntemplate<typenmae I, typename T, IndIntruHeapData T::*heap_info, typename C, uint K=2>\nclass IndIntruHeap {\nprotected:\n    std::vector<I> data;  // 存储元素的容器\n    index_t count;  // 元素个数\n    C comparator;  // 重建堆时，比较两元素的优先顺序\npublic:\n    void push(const I& item);  // 加入新元素\n    void adjust(T& item);\n\nprotected:\n    // 从下至上重建堆\n    void shift_up(index_t i);\n    // 由上到下重建堆\n    // 如果i为最后1个元素，不调整\n    // \n    typename std::enable_if<(K>2)&&EnableBool,void>::type sift_down(index_t i);\n    // 给定元素被修改后，调整其在堆中的位置\n    // 如果i为根节点，则向下调整\n    // 如果i大于其parent节点，则向上调整\n    // 如果i小于其parent节点，则向下调整\n    void sift(index_t i);\n};\n```\n参考[浅谈算法和数据结构: 五 优先级队列与堆排序](http://www.cnblogs.com/yangecnu/archive/2014/03/02/3577568.html) 获取更多关于堆的内容。\n\n\n``` C++\nvoid IndIntruHeap::sift_up(index_t i) {\n    // 入参i代表元素在IndIntruHeap::data中的位置（或者说，数组下标）\n    while (i > 0) {\n        index_t pi = parent(i);  // i的父节点位置\n        if (!comparator(*data[i], *data[pi])) {\n            // data[i] 优先级低于 data[pi]，保持相对结构不变\n            break;\n        }\n\n        // data[i] 优先级低于 data[pi]，将data[i]上移\n        // 因为pi的左节点的优先级低于pi，而pi的优先级低于i\n        // 所以pi的左节点的优先级也低于i，i可以作为其父节点，不必再进行比较\n        std::swap(data[i], data[pi]);\n        // 修改数据的位置信息\n        intru_data_of(data[i]) = i;\n        intru_data_of(data[pi]) = pi;\n        // data[i] 继续与其新的父节点进行比较\n        i = pi;\n   }\n} \n\n// 此处的std::enable_if可以理解为返回值函数重载，实际上是模板偏特化\n// 代码中实现了连个shift_down函数，用以区分K==2和K>2两种情况，这两种情况分别调用两个不同的函数\ntemplate<bool EnableBool=true>\ntypename std::enable_if<(K>2)&&EnableBool,void>::type IndIntruHeap::sift_down(index_t i) {\n    // count为元素数目，i大于它为非法\n    if (i >= count) return;\n    while (true) {\n        // li为i最左边的子节点\n        index_t li = lhs(i);\n\n        if (li < count) {\n            // ri为i最右边的子节点\n            index_t ri = std::min(rhs(i), count - 1);\n\n            // 查找优先级最高的子节点\n            index_t min_i = li;\n            for (index_t k = li + 1; k <= ri; ++k) {\n                if (comparator(*data[k], *data[min_i])) {\n                    min_i = k;\n                }\n            }\n\n            if (comparator(*data[min_i], *data[i])) {\n                // i优先级低于子节点min_i，交换data[i]和data[min_i]的位置\n                // 因为min_i已经是i所有子节点中优先级最高的，因此它可以作为其余子节点的父节点\n                std::swap(data[i], data[min_i]);\n                intru_data_of(data[i]) = i;\n                intru_data_of(data[min_i]) = min_i;\n                // i继续和新的子节点进行比较\n                i = min_i;\n            } else {\n                // i节点比所有的子节点优先级都高，退出\n                break;\n            }\n        } else {\n            // i节点已是叶子节点\n            break;\n        }\n    }\n} // sift_down \n```\n\n### 优先级比较\n\n``` C++\n// 请求的Tag，包含预留(reservation)、上限(limit)和权重(proportion)。\nclass RequestTag {\n    double reservation;  // 预留\n    double proportion;  // 权重\n    double limit;  // 上限\n    bool ready;  // 请求是否允许在Weight-based阶段被处理，当Limit低于当前时间（请求调度）\n    Time arrival;  // 接收到请求的时间\n};\n```\n请求调度的Weight-based阶段，先从Limit堆中筛选出能够在这个阶段被处理的请求，设置请求的ready字段为True。筛选的条件是请求的L Tag低于当前时间。筛选结束后再从Ready堆中选择请求。详情参考 PriorityQueueBase::do_next_request函数。\n\n``` C++\n// tag_field代表RequestTag类的一个成员变量\ntemplate<double RequestTag::*tag_field, ReadyOption ready_opt, bool use_prop_delta>\nstruct ClientCompare {\n    bool operator()(const ClientRec& n1, const ClientRec& n2) const {\n        // n1没有请求，n2有请求，返回false\n        // n1没有请求，n2也没有请求，返回false，保持堆结构不变\n        // n1有请求，n2没有请求，返回true\n        // n1有请求，n1也有请求\n    }\n};\n```\n比较客户n1和客户n2的Next请求的优先级。根据mClock算法，Tag值越小优先级应该越高。该函数的本意是，判断“n1的Next请求的优先级高于n2？”。下文使用tag1和tag2分别表示n1和n2的Next请求的Tag值。\n\n对**reservation堆**，依据“谁小谁优先级高”的原则，若tag1小于tag2，则返回True；\n对**ready堆**，如果t1、t2两者的ready状态相同（都为true或者都为false），那么遵循“谁小谁优先级高”的原则；如果t1、t2的两者的ready状态不同（一个为true一个为false），那么依据“**谁ready谁优先级高**”的原则；\n对**limit堆**，如果t1、t2两者的ready状态相同，那么遵循“谁小谁优先级高”的原则。如果两者的ready状态不同，按照“**谁没ready谁优先级高**”的原则，对已经Ready的请求将在请求调度中直接处理掉。\n\n### Tag校准\n\n根据mClock算法，当有**新VM**启动时所有**老VM**请求的P Tag都要向左偏移一个位置（即P Tag减去偏移量），以避免新VM请求偏低影响公平性。偏移量为所有老VM的剩余请求中最小的P Tag减去当前时间（新VM第一个请求的到达时间）。反过来思考，将所有老VM的剩余请求在P Tag时间轴上向左偏移和将新VM的请求向右偏移的结果是相同的，一样能够消除不公平性。但这在代码实现上就简单了，只要在新VM中记录偏移量，并在Tag比较时加上偏移量即可，不必对所有老VM的剩余请求大动干戈。\n\n``` C++\ntemplate<typename C, typename R, uint B>\nclass PriorityQueueBase {\n    c::IndIntruHeap<ClientRecRef, ClientRec, &ClientRec::reserv_heap_data,\n                    ClientCompare<&RequestTag::reservation, ReadyOption::ignore, false>,\n                    B> resv_heap;\n    c::IndIntruHeap<ClientRecRef, ClientRec, &ClientRec::lim_heap_data,\n                    ClientCompare<&RequestTag::limit, ReadyOption::lowers, false>,\n                    B> limit_heap;\n    c::IndIntruHeap<ClientRecRef, ClientRec, &ClientRec::ready_heap_data,\n                    ClientCompare<&RequestTag::proportion, ReadyOption::raises, true>,\n                    B> ready_heap;\n};\n```\n算法实现中分两步完成Tag校准：\n\n1. 接收请求时，检查Client是否空闲，若空闲则计算偏移量，并将偏移量保存到ClientRec::prop_delta变量。参考PriorityQueueBase::do_add_request函数的*if (client.idle)*分支；\n2. 比较P Tag时， 为请求的P Tag加上Client的prop_delta后再比较。参考ClientCompare的operator()函数。\n\n另外，注意只有P Tag需要校准。ClientCompare模板的最后一个参数代表是否使用prop_delta，只有ready_heap变量为True。\n\n### 判断客户为空闲\n\n客户是否为空闲，在Tag校准中非常重要，只有发现空闲的客户时才需要进行校准。以下是算法实现中，判断客户空闲的一些手段：\n- 服务端第一次接收到来自该客户的请求，就认为该客户为空闲，参考 PriorityQueueBase::do_add_request函数\n- 定时检查，将超过idle_age(10分钟)时间内没有新增请求的客户标记为idle, 参考 PriorityQueueBase::do_clean函数\n\n对定时检查策略，除了将客户标记为idle状态外还会清理长久没有发送请求的客户。默认情况下，定时检查的时间间隔为6分钟，超过10分钟没新请求的客户将被标记为idle状态，超过15分钟没有新请求的客户将被从服务端清理。\n\n### 请求调度\n\n请求调度，从堆结构中获取待处理请求的函数调用过程：\n\n```\nPushPriorityQueue::schedule_request() --> PushPriorityQueue::next_request --> PriorityQueueBase::do_next_request()\n```\nDmClock算法的请求调度流程在PriorityQueueBase::do_next_request函数中实现。\n\n**递减剩余请求的R Tag**\n根据DmClock算法，在Weight-based阶段每处理一个请求时，为避免影响R tag的排序，需要将该客户的剩余请求的R tag值递减 1/ri。这个过程在函数PushPriorityQueue::submit_request()以及PriorityQueueBase::reduce_reservation_tags函数中实现。\n","slug":"DmClock/MCLOCK_dmclock","published":1,"updated":"2016-12-15T07:18:40.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cix1kgau7000wdaeil6pf5ja6","content":"<h1 id=\"Tag计算\"><a href=\"#Tag计算\" class=\"headerlink\" title=\"Tag计算\"></a>Tag计算</h1><p>Tag值的计算依赖于两类参数，一类是用户配置的上限(limit)、下限(reservation)以及权重(weight)，另一类是动态统计出来的delta和rho。根据dmClock算法的描述，delta和rho这两个参数由客户端来统计，并将其夹带到请求中发送给服务端。</p>\n<a id=\"more\"></a>\n<h2 id=\"客户端\"><a href=\"#客户端\" class=\"headerlink\" title=\"客户端\"></a>客户端</h2><p>从客户端发送请求到请求到达服务端的内部队列的函数调用过程，后文记为<strong>过程A</strong>：<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">SimulatedClient::run_req() --&gt; SimulatedClient::submit_f() --&gt; SimulatedServer::post() --&gt; </div><div class=\"line\">PushPriorityQueue::add_request() --&gt; PushPriorityQueue::schedule_request() --&gt; </div><div class=\"line\">PushPriorityQueue::submit_request() --&gt; PushPriorityQueue::submit_top_request() --&gt; </div><div class=\"line\">PriorityQueueBase::pop_process_request() --&gt; PushPriorityQueue::handle_f() --&gt; SimulatedServer::inner_post()</div></pre></td></tr></table></figure></p>\n<p>从服务端发送响应到客户端接收到响应的函数调用过程，后文记为<strong>过程B</strong>：<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">SimulatedServer::run() --&gt; SimulatedServer::client_resp_f() --&gt; </div><div class=\"line\">SimulatedClient::receive_response() --&gt; SimulatedClient::run_resp() --&gt; ServiceTracker::track_resp()</div></pre></td></tr></table></figure></p>\n<p>根据dmClock算法，发送给给定服务端的请求中携带的delta的定义为：</p>\n<blockquote>\n<p>从最近一次发送给该服务端请求的时刻到当前时刻这段时间内，客户发送给其它服务端的请求的数目。</p>\n</blockquote>\n<p>为计算delta，对任意一台服务端发送请求时记录下此时全局已发送的请求数目，计算delta时只要将当前的全局已发送请求数目减去最近一次发送时的全局已发送请求数目即可。算法实现中使用响应来近似请求，具体做法如下：</p>\n<p>对给定的服务端，使用t1代表最近一次客户向其发送请求的时刻，t2代表当前时刻，即本次向其发送请求的时刻。算法实现中用到的关键数据结构：</p>\n<figure class=\"highlight c++\"><table><tr><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\">// 一个服务端的响应统计情况</span></div><div class=\"line\"><span class=\"keyword\">struct</span> ServerInfo &#123;</div><div class=\"line\">    Counter delta_prev_req;  <span class=\"comment\">// t1时刻的全局响应数目</span></div><div class=\"line\">    Counter rho_prev_req;  <span class=\"comment\">// t1时刻的全局reservation响应数目</span></div><div class=\"line\">    <span class=\"keyword\">uint32_t</span> my_delta;  <span class=\"comment\">// &gt;t1时间段内，属于本服务端的响应数目</span></div><div class=\"line\">    <span class=\"keyword\">uint32_t</span> my_rho;  <span class=\"comment\">// &gt;t1的时间段内，属于本服务端的reservation响应数目</span></div><div class=\"line\">&#125;;</div><div class=\"line\"></div><div class=\"line\"><span class=\"keyword\">template</span>&lt;<span class=\"keyword\">typename</span> S&gt;</div><div class=\"line\"><span class=\"keyword\">class</span> ServiceTracker &#123;</div><div class=\"line\">    Counter delta_counter;  <span class=\"comment\">// 所有服务端的响应数目</span></div><div class=\"line\">    Counter rho_counter;  <span class=\"comment\">// 所有服务端的Reservation响应数目</span></div><div class=\"line\">    <span class=\"built_in\">std</span>::<span class=\"built_in\">map</span>&lt;S, ServerInfo&gt; server_map;  <span class=\"comment\">// 每个服务端独立的响应统计情况</span></div><div class=\"line\">&#125;;</div></pre></td></tr></table></figure>\n<p><strong>过程B</strong>每接收到一个响应，递增全局的delta_counter以及特定服务端的my_delta。对rho也一样，详细过程参考ServiceTracker::track_resp函数。<br><strong>过程A</strong>在准备请求阶段计算delta，公式如下：</p>\n<figure class=\"highlight c++\"><table><tr><td class=\"code\"><pre><div class=\"line\">delta = <span class=\"number\">1</span> + delta_counter - it-&gt;second.delta_prev_req - it-&gt;second.my_delta</div></pre></td></tr></table></figure>\n<p>其中，it代表ServiceTracker::server_map的迭代器。计算delta时之所以减去my_delta，是因为在t1~t2这个时间段内除了能够接收到其它服务端的响应外还会接收到来自自己服务端的响应，因此要减去自己服务端的响应个数。</p>\n<h2 id=\"服务端\"><a href=\"#服务端\" class=\"headerlink\" title=\"服务端\"></a>服务端</h2><p>从客户端发送请求到请求到达服务端的优先级队列的函数调用过程：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">SimulatedClient::run_req() --&gt; SimulatedClient::submit_f() --&gt; SimulatedServer::post() --&gt; </div><div class=\"line\">PushPriorityQueue::add_request() --&gt; PushPriorityQueue::do_add_request()</div></pre></td></tr></table></figure>\n<p>在这过程中，请求被添加上了一个伪Tag。之所以假是因为Tag中除当前时间外其它的内容都为0。具体参看PushPriorityQueue::do_add_request函数。</p>\n<p>从请求开始调度到请求送到服务端的内部队列的函数调用过程：<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">PushPriorityQueue::schedule_request() --&gt; PushPriorityQueue::submit_request() --&gt;</div><div class=\"line\">PushPriorityQueue::submit_top_request() --&gt; PriorityQueueBase::pop_process_request() --&gt; </div><div class=\"line\">PushPriorityQueue::handle_f() --&gt; SimulatedServer::inner_post()</div></pre></td></tr></table></figure></p>\n<p>PriorityQueueBase::pop_process_request开始计算请求的tag值，具体计算过程在RequestTag::tag_calc函数完成，计算过程和算法描述基本一致。注意，对第一个请求，RequestTag计算Tag时，依赖的前个请求为默认的prev_tag(0.0, 0.0, 0.0, TimeZero)，这样第一个请求的各个Tag值将为请求到达的时间。</p>\n<h3 id=\"数据结构\"><a href=\"#数据结构\" class=\"headerlink\" title=\"数据结构\"></a>数据结构</h3><h4 id=\"DMC客户\"><a href=\"#DMC客户\" class=\"headerlink\" title=\"DMC客户\"></a>DMC客户</h4><figure class=\"highlight c++\"><table><tr><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\">// C: 客户ID</span></div><div class=\"line\"><span class=\"comment\">// R: 请求类型</span></div><div class=\"line\"><span class=\"comment\">// B: B叉堆，堆的分叉数</span></div><div class=\"line\"><span class=\"keyword\">template</span>&lt;<span class=\"keyword\">typename</span> C, <span class=\"keyword\">typename</span> R, uint B&gt;</div><div class=\"line\"><span class=\"keyword\">class</span> PriorityQueueBase &#123;</div><div class=\"line\">     <span class=\"built_in\">std</span>::<span class=\"built_in\">map</span>&lt;C,ClientRecRef&gt; client_map;  <span class=\"comment\">// 每个客户的记录</span></div><div class=\"line\">&#125;;</div><div class=\"line\"></div><div class=\"line\"><span class=\"keyword\">class</span> ClientRec &#123;</div><div class=\"line\">    <span class=\"keyword\">uint32_t</span> cur_rho;</div><div class=\"line\">    <span class=\"keyword\">uint32_t</span> cur_delta;</div><div class=\"line\">    ClientInfo info;  <span class=\"comment\">// 包含limit、reservation、proportion参数及其倒数</span></div><div class=\"line\"></div><div class=\"line\">    RequestTag  prev_tag;  <span class=\"comment\">// 前一个请求的Tag值</span></div><div class=\"line\">    <span class=\"built_in\">std</span>::<span class=\"built_in\">deque</span>&lt;ClientReq&gt; requests;  <span class=\"comment\">// 请求队列</span></div><div class=\"line\">&#125;;</div></pre></td></tr></table></figure>\n<p>服务端中每个客户即ClientRec类的一个实例，客户的请求保存到ClientRec::requests队列，该队列是FCFS队列。也就说，同个客户端的请求的相对顺序保持不变。dmClock改变的是不同客户端间的处理顺序。</p>\n<h4 id=\"请求堆\"><a href=\"#请求堆\" class=\"headerlink\" title=\"请求堆\"></a>请求堆</h4><figure class=\"highlight c++\"><table><tr><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\">// I: 元素类型</span></div><div class=\"line\"><span class=\"comment\">// T: 元素类型 </span></div><div class=\"line\"><span class=\"comment\">// C: 用于比较两个T类型的参数大小的函数类</span></div><div class=\"line\"><span class=\"comment\">// K: 堆的分叉，K=2代表2叉堆</span></div><div class=\"line\"><span class=\"comment\">// heap_info: T的成员变量</span></div><div class=\"line\"><span class=\"keyword\">template</span>&lt;typenmae I, <span class=\"keyword\">typename</span> T, IndIntruHeapData T::*heap_info, <span class=\"keyword\">typename</span> C, uint K=<span class=\"number\">2</span>&gt;</div><div class=\"line\"><span class=\"keyword\">class</span> IndIntruHeap &#123;</div><div class=\"line\"><span class=\"keyword\">protected</span>:</div><div class=\"line\">    <span class=\"built_in\">std</span>::<span class=\"built_in\">vector</span>&lt;I&gt; data;  <span class=\"comment\">// 存储元素的容器</span></div><div class=\"line\">    <span class=\"keyword\">index_t</span> count;  <span class=\"comment\">// 元素个数</span></div><div class=\"line\">    C comparator;  <span class=\"comment\">// 重建堆时，比较两元素的优先顺序</span></div><div class=\"line\"><span class=\"keyword\">public</span>:</div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">void</span> <span class=\"title\">push</span><span class=\"params\">(<span class=\"keyword\">const</span> I&amp; item)</span></span>;  <span class=\"comment\">// 加入新元素</span></div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">void</span> <span class=\"title\">adjust</span><span class=\"params\">(T&amp; item)</span></span>;</div><div class=\"line\"></div><div class=\"line\"><span class=\"keyword\">protected</span>:</div><div class=\"line\">    <span class=\"comment\">// 从下至上重建堆</span></div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">void</span> <span class=\"title\">shift_up</span><span class=\"params\">(<span class=\"keyword\">index_t</span> i)</span></span>;</div><div class=\"line\">    <span class=\"comment\">// 由上到下重建堆</span></div><div class=\"line\">    <span class=\"comment\">// 如果i为最后1个元素，不调整</span></div><div class=\"line\">    <span class=\"comment\">// </span></div><div class=\"line\">    <span class=\"keyword\">typename</span> <span class=\"built_in\">std</span>::enable_if&lt;(K&gt;<span class=\"number\">2</span>)&amp;&amp;EnableBool,<span class=\"keyword\">void</span>&gt;::<span class=\"function\">type <span class=\"title\">sift_down</span><span class=\"params\">(<span class=\"keyword\">index_t</span> i)</span></span>;</div><div class=\"line\">    <span class=\"comment\">// 给定元素被修改后，调整其在堆中的位置</span></div><div class=\"line\">    <span class=\"comment\">// 如果i为根节点，则向下调整</span></div><div class=\"line\">    <span class=\"comment\">// 如果i大于其parent节点，则向上调整</span></div><div class=\"line\">    <span class=\"comment\">// 如果i小于其parent节点，则向下调整</span></div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">void</span> <span class=\"title\">sift</span><span class=\"params\">(<span class=\"keyword\">index_t</span> i)</span></span>;</div><div class=\"line\">&#125;;</div></pre></td></tr></table></figure>\n<p>参考<a href=\"http://www.cnblogs.com/yangecnu/archive/2014/03/02/3577568.html\" target=\"_blank\" rel=\"external\">浅谈算法和数据结构: 五 优先级队列与堆排序</a> 获取更多关于堆的内容。</p>\n<figure class=\"highlight c++\"><table><tr><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">void</span> IndIntruHeap::sift_up(<span class=\"keyword\">index_t</span> i) &#123;</div><div class=\"line\">    <span class=\"comment\">// 入参i代表元素在IndIntruHeap::data中的位置（或者说，数组下标）</span></div><div class=\"line\">    <span class=\"keyword\">while</span> (i &gt; <span class=\"number\">0</span>) &#123;</div><div class=\"line\">        <span class=\"keyword\">index_t</span> pi = parent(i);  <span class=\"comment\">// i的父节点位置</span></div><div class=\"line\">        <span class=\"keyword\">if</span> (!comparator(*data[i], *data[pi])) &#123;</div><div class=\"line\">            <span class=\"comment\">// data[i] 优先级低于 data[pi]，保持相对结构不变</span></div><div class=\"line\">            <span class=\"keyword\">break</span>;</div><div class=\"line\">        &#125;</div><div class=\"line\"></div><div class=\"line\">        <span class=\"comment\">// data[i] 优先级低于 data[pi]，将data[i]上移</span></div><div class=\"line\">        <span class=\"comment\">// 因为pi的左节点的优先级低于pi，而pi的优先级低于i</span></div><div class=\"line\">        <span class=\"comment\">// 所以pi的左节点的优先级也低于i，i可以作为其父节点，不必再进行比较</span></div><div class=\"line\">        <span class=\"built_in\">std</span>::swap(data[i], data[pi]);</div><div class=\"line\">        <span class=\"comment\">// 修改数据的位置信息</span></div><div class=\"line\">        intru_data_of(data[i]) = i;</div><div class=\"line\">        intru_data_of(data[pi]) = pi;</div><div class=\"line\">        <span class=\"comment\">// data[i] 继续与其新的父节点进行比较</span></div><div class=\"line\">        i = pi;</div><div class=\"line\">   &#125;</div><div class=\"line\">&#125; </div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">// 此处的std::enable_if可以理解为返回值函数重载，实际上是模板偏特化</span></div><div class=\"line\"><span class=\"comment\">// 代码中实现了连个shift_down函数，用以区分K==2和K&gt;2两种情况，这两种情况分别调用两个不同的函数</span></div><div class=\"line\"><span class=\"keyword\">template</span>&lt;<span class=\"keyword\">bool</span> EnableBool=<span class=\"literal\">true</span>&gt;</div><div class=\"line\"><span class=\"keyword\">typename</span> <span class=\"built_in\">std</span>::enable_if&lt;(K&gt;<span class=\"number\">2</span>)&amp;&amp;EnableBool,<span class=\"keyword\">void</span>&gt;::type IndIntruHeap::sift_down(<span class=\"keyword\">index_t</span> i) &#123;</div><div class=\"line\">    <span class=\"comment\">// count为元素数目，i大于它为非法</span></div><div class=\"line\">    <span class=\"keyword\">if</span> (i &gt;= count) <span class=\"keyword\">return</span>;</div><div class=\"line\">    <span class=\"keyword\">while</span> (<span class=\"literal\">true</span>) &#123;</div><div class=\"line\">        <span class=\"comment\">// li为i最左边的子节点</span></div><div class=\"line\">        <span class=\"keyword\">index_t</span> li = lhs(i);</div><div class=\"line\"></div><div class=\"line\">        <span class=\"keyword\">if</span> (li &lt; count) &#123;</div><div class=\"line\">            <span class=\"comment\">// ri为i最右边的子节点</span></div><div class=\"line\">            <span class=\"keyword\">index_t</span> ri = <span class=\"built_in\">std</span>::min(rhs(i), count - <span class=\"number\">1</span>);</div><div class=\"line\"></div><div class=\"line\">            <span class=\"comment\">// 查找优先级最高的子节点</span></div><div class=\"line\">            <span class=\"keyword\">index_t</span> min_i = li;</div><div class=\"line\">            <span class=\"keyword\">for</span> (<span class=\"keyword\">index_t</span> k = li + <span class=\"number\">1</span>; k &lt;= ri; ++k) &#123;</div><div class=\"line\">                <span class=\"keyword\">if</span> (comparator(*data[k], *data[min_i])) &#123;</div><div class=\"line\">                    min_i = k;</div><div class=\"line\">                &#125;</div><div class=\"line\">            &#125;</div><div class=\"line\"></div><div class=\"line\">            <span class=\"keyword\">if</span> (comparator(*data[min_i], *data[i])) &#123;</div><div class=\"line\">                <span class=\"comment\">// i优先级低于子节点min_i，交换data[i]和data[min_i]的位置</span></div><div class=\"line\">                <span class=\"comment\">// 因为min_i已经是i所有子节点中优先级最高的，因此它可以作为其余子节点的父节点</span></div><div class=\"line\">                <span class=\"built_in\">std</span>::swap(data[i], data[min_i]);</div><div class=\"line\">                intru_data_of(data[i]) = i;</div><div class=\"line\">                intru_data_of(data[min_i]) = min_i;</div><div class=\"line\">                <span class=\"comment\">// i继续和新的子节点进行比较</span></div><div class=\"line\">                i = min_i;</div><div class=\"line\">            &#125; <span class=\"keyword\">else</span> &#123;</div><div class=\"line\">                <span class=\"comment\">// i节点比所有的子节点优先级都高，退出</span></div><div class=\"line\">                <span class=\"keyword\">break</span>;</div><div class=\"line\">            &#125;</div><div class=\"line\">        &#125; <span class=\"keyword\">else</span> &#123;</div><div class=\"line\">            <span class=\"comment\">// i节点已是叶子节点</span></div><div class=\"line\">            <span class=\"keyword\">break</span>;</div><div class=\"line\">        &#125;</div><div class=\"line\">    &#125;</div><div class=\"line\">&#125; <span class=\"comment\">// sift_down</span></div></pre></td></tr></table></figure>\n<h3 id=\"优先级比较\"><a href=\"#优先级比较\" class=\"headerlink\" title=\"优先级比较\"></a>优先级比较</h3><figure class=\"highlight c++\"><table><tr><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\">// 请求的Tag，包含预留(reservation)、上限(limit)和权重(proportion)。</span></div><div class=\"line\"><span class=\"keyword\">class</span> RequestTag &#123;</div><div class=\"line\">    <span class=\"keyword\">double</span> reservation;  <span class=\"comment\">// 预留</span></div><div class=\"line\">    <span class=\"keyword\">double</span> proportion;  <span class=\"comment\">// 权重</span></div><div class=\"line\">    <span class=\"keyword\">double</span> limit;  <span class=\"comment\">// 上限</span></div><div class=\"line\">    <span class=\"keyword\">bool</span> ready;  <span class=\"comment\">// 请求是否允许在Weight-based阶段被处理，当Limit低于当前时间（请求调度）</span></div><div class=\"line\">    Time arrival;  <span class=\"comment\">// 接收到请求的时间</span></div><div class=\"line\">&#125;;</div></pre></td></tr></table></figure>\n<p>请求调度的Weight-based阶段，先从Limit堆中筛选出能够在这个阶段被处理的请求，设置请求的ready字段为True。筛选的条件是请求的L Tag低于当前时间。筛选结束后再从Ready堆中选择请求。详情参考 PriorityQueueBase::do_next_request函数。</p>\n<figure class=\"highlight c++\"><table><tr><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\">// tag_field代表RequestTag类的一个成员变量</span></div><div class=\"line\"><span class=\"keyword\">template</span>&lt;<span class=\"keyword\">double</span> RequestTag::*tag_field, ReadyOption ready_opt, <span class=\"keyword\">bool</span> use_prop_delta&gt;</div><div class=\"line\"><span class=\"keyword\">struct</span> ClientCompare &#123;</div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">bool</span> <span class=\"title\">operator</span><span class=\"params\">()</span><span class=\"params\">(<span class=\"keyword\">const</span> ClientRec&amp; n1, <span class=\"keyword\">const</span> ClientRec&amp; n2)</span> <span class=\"keyword\">const</span> </span>&#123;</div><div class=\"line\">        <span class=\"comment\">// n1没有请求，n2有请求，返回false</span></div><div class=\"line\">        <span class=\"comment\">// n1没有请求，n2也没有请求，返回false，保持堆结构不变</span></div><div class=\"line\">        <span class=\"comment\">// n1有请求，n2没有请求，返回true</span></div><div class=\"line\">        <span class=\"comment\">// n1有请求，n1也有请求</span></div><div class=\"line\">    &#125;</div><div class=\"line\">&#125;;</div></pre></td></tr></table></figure>\n<p>比较客户n1和客户n2的Next请求的优先级。根据mClock算法，Tag值越小优先级应该越高。该函数的本意是，判断“n1的Next请求的优先级高于n2？”。下文使用tag1和tag2分别表示n1和n2的Next请求的Tag值。</p>\n<p>对<strong>reservation堆</strong>，依据“谁小谁优先级高”的原则，若tag1小于tag2，则返回True；<br>对<strong>ready堆</strong>，如果t1、t2两者的ready状态相同（都为true或者都为false），那么遵循“谁小谁优先级高”的原则；如果t1、t2的两者的ready状态不同（一个为true一个为false），那么依据“<strong>谁ready谁优先级高</strong>”的原则；<br>对<strong>limit堆</strong>，如果t1、t2两者的ready状态相同，那么遵循“谁小谁优先级高”的原则。如果两者的ready状态不同，按照“<strong>谁没ready谁优先级高</strong>”的原则，对已经Ready的请求将在请求调度中直接处理掉。</p>\n<h3 id=\"Tag校准\"><a href=\"#Tag校准\" class=\"headerlink\" title=\"Tag校准\"></a>Tag校准</h3><p>根据mClock算法，当有<strong>新VM</strong>启动时所有<strong>老VM</strong>请求的P Tag都要向左偏移一个位置（即P Tag减去偏移量），以避免新VM请求偏低影响公平性。偏移量为所有老VM的剩余请求中最小的P Tag减去当前时间（新VM第一个请求的到达时间）。反过来思考，将所有老VM的剩余请求在P Tag时间轴上向左偏移和将新VM的请求向右偏移的结果是相同的，一样能够消除不公平性。但这在代码实现上就简单了，只要在新VM中记录偏移量，并在Tag比较时加上偏移量即可，不必对所有老VM的剩余请求大动干戈。</p>\n<figure class=\"highlight c++\"><table><tr><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">template</span>&lt;<span class=\"keyword\">typename</span> C, <span class=\"keyword\">typename</span> R, uint B&gt;</div><div class=\"line\"><span class=\"keyword\">class</span> PriorityQueueBase &#123;</div><div class=\"line\">    c::IndIntruHeap&lt;ClientRecRef, ClientRec, &amp;ClientRec::reserv_heap_data,</div><div class=\"line\">                    ClientCompare&lt;&amp;RequestTag::reservation, ReadyOption::ignore, <span class=\"literal\">false</span>&gt;,</div><div class=\"line\">                    B&gt; resv_heap;</div><div class=\"line\">    c::IndIntruHeap&lt;ClientRecRef, ClientRec, &amp;ClientRec::lim_heap_data,</div><div class=\"line\">                    ClientCompare&lt;&amp;RequestTag::limit, ReadyOption::lowers, <span class=\"literal\">false</span>&gt;,</div><div class=\"line\">                    B&gt; limit_heap;</div><div class=\"line\">    c::IndIntruHeap&lt;ClientRecRef, ClientRec, &amp;ClientRec::ready_heap_data,</div><div class=\"line\">                    ClientCompare&lt;&amp;RequestTag::proportion, ReadyOption::raises, <span class=\"literal\">true</span>&gt;,</div><div class=\"line\">                    B&gt; ready_heap;</div><div class=\"line\">&#125;;</div></pre></td></tr></table></figure>\n<p>算法实现中分两步完成Tag校准：</p>\n<ol>\n<li>接收请求时，检查Client是否空闲，若空闲则计算偏移量，并将偏移量保存到ClientRec::prop_delta变量。参考PriorityQueueBase::do_add_request函数的<em>if (client.idle)</em>分支；</li>\n<li>比较P Tag时， 为请求的P Tag加上Client的prop_delta后再比较。参考ClientCompare的operator()函数。</li>\n</ol>\n<p>另外，注意只有P Tag需要校准。ClientCompare模板的最后一个参数代表是否使用prop_delta，只有ready_heap变量为True。</p>\n<h3 id=\"判断客户为空闲\"><a href=\"#判断客户为空闲\" class=\"headerlink\" title=\"判断客户为空闲\"></a>判断客户为空闲</h3><p>客户是否为空闲，在Tag校准中非常重要，只有发现空闲的客户时才需要进行校准。以下是算法实现中，判断客户空闲的一些手段：</p>\n<ul>\n<li>服务端第一次接收到来自该客户的请求，就认为该客户为空闲，参考 PriorityQueueBase::do_add_request函数</li>\n<li>定时检查，将超过idle_age(10分钟)时间内没有新增请求的客户标记为idle, 参考 PriorityQueueBase::do_clean函数</li>\n</ul>\n<p>对定时检查策略，除了将客户标记为idle状态外还会清理长久没有发送请求的客户。默认情况下，定时检查的时间间隔为6分钟，超过10分钟没新请求的客户将被标记为idle状态，超过15分钟没有新请求的客户将被从服务端清理。</p>\n<h3 id=\"请求调度\"><a href=\"#请求调度\" class=\"headerlink\" title=\"请求调度\"></a>请求调度</h3><p>请求调度，从堆结构中获取待处理请求的函数调用过程：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">PushPriorityQueue::schedule_request() --&gt; PushPriorityQueue::next_request --&gt; PriorityQueueBase::do_next_request()</div></pre></td></tr></table></figure>\n<p>DmClock算法的请求调度流程在PriorityQueueBase::do_next_request函数中实现。</p>\n<p><strong>递减剩余请求的R Tag</strong><br>根据DmClock算法，在Weight-based阶段每处理一个请求时，为避免影响R tag的排序，需要将该客户的剩余请求的R tag值递减 1/ri。这个过程在函数PushPriorityQueue::submit_request()以及PriorityQueueBase::reduce_reservation_tags函数中实现。</p>\n<hr>\n<p><strong>版权声明</strong></p>\n<p>本作品采用<a href=\"https://creativecommons.org/licenses/by/2.5/cn/\" target=\"_blank\" rel=\"external\">知识共享署名2.5 中国大陆许可协议</a>进行许可，欢迎转载，但转载请注明来自<a href=\"https://wuxiangwei.cn/\"><em>Shanno’s Blog</em></a>，并保持转载后文章内容的完整。本人保留所有版权相关权利，侵权必究。</p>\n","excerpt":"<h1 id=\"Tag计算\"><a href=\"#Tag计算\" class=\"headerlink\" title=\"Tag计算\"></a>Tag计算</h1><p>Tag值的计算依赖于两类参数，一类是用户配置的上限(limit)、下限(reservation)以及权重(weight)，另一类是动态统计出来的delta和rho。根据dmClock算法的描述，delta和rho这两个参数由客户端来统计，并将其夹带到请求中发送给服务端。</p>","more":"<h2 id=\"客户端\"><a href=\"#客户端\" class=\"headerlink\" title=\"客户端\"></a>客户端</h2><p>从客户端发送请求到请求到达服务端的内部队列的函数调用过程，后文记为<strong>过程A</strong>：<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">SimulatedClient::run_req() --&gt; SimulatedClient::submit_f() --&gt; SimulatedServer::post() --&gt; </div><div class=\"line\">PushPriorityQueue::add_request() --&gt; PushPriorityQueue::schedule_request() --&gt; </div><div class=\"line\">PushPriorityQueue::submit_request() --&gt; PushPriorityQueue::submit_top_request() --&gt; </div><div class=\"line\">PriorityQueueBase::pop_process_request() --&gt; PushPriorityQueue::handle_f() --&gt; SimulatedServer::inner_post()</div></pre></td></tr></table></figure></p>\n<p>从服务端发送响应到客户端接收到响应的函数调用过程，后文记为<strong>过程B</strong>：<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">SimulatedServer::run() --&gt; SimulatedServer::client_resp_f() --&gt; </div><div class=\"line\">SimulatedClient::receive_response() --&gt; SimulatedClient::run_resp() --&gt; ServiceTracker::track_resp()</div></pre></td></tr></table></figure></p>\n<p>根据dmClock算法，发送给给定服务端的请求中携带的delta的定义为：</p>\n<blockquote>\n<p>从最近一次发送给该服务端请求的时刻到当前时刻这段时间内，客户发送给其它服务端的请求的数目。</p>\n</blockquote>\n<p>为计算delta，对任意一台服务端发送请求时记录下此时全局已发送的请求数目，计算delta时只要将当前的全局已发送请求数目减去最近一次发送时的全局已发送请求数目即可。算法实现中使用响应来近似请求，具体做法如下：</p>\n<p>对给定的服务端，使用t1代表最近一次客户向其发送请求的时刻，t2代表当前时刻，即本次向其发送请求的时刻。算法实现中用到的关键数据结构：</p>\n<figure class=\"highlight c++\"><table><tr><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\">// 一个服务端的响应统计情况</span></div><div class=\"line\"><span class=\"keyword\">struct</span> ServerInfo &#123;</div><div class=\"line\">    Counter delta_prev_req;  <span class=\"comment\">// t1时刻的全局响应数目</span></div><div class=\"line\">    Counter rho_prev_req;  <span class=\"comment\">// t1时刻的全局reservation响应数目</span></div><div class=\"line\">    <span class=\"keyword\">uint32_t</span> my_delta;  <span class=\"comment\">// &gt;t1时间段内，属于本服务端的响应数目</span></div><div class=\"line\">    <span class=\"keyword\">uint32_t</span> my_rho;  <span class=\"comment\">// &gt;t1的时间段内，属于本服务端的reservation响应数目</span></div><div class=\"line\">&#125;;</div><div class=\"line\"></div><div class=\"line\"><span class=\"keyword\">template</span>&lt;<span class=\"keyword\">typename</span> S&gt;</div><div class=\"line\"><span class=\"keyword\">class</span> ServiceTracker &#123;</div><div class=\"line\">    Counter delta_counter;  <span class=\"comment\">// 所有服务端的响应数目</span></div><div class=\"line\">    Counter rho_counter;  <span class=\"comment\">// 所有服务端的Reservation响应数目</span></div><div class=\"line\">    <span class=\"built_in\">std</span>::<span class=\"built_in\">map</span>&lt;S, ServerInfo&gt; server_map;  <span class=\"comment\">// 每个服务端独立的响应统计情况</span></div><div class=\"line\">&#125;;</div></pre></td></tr></table></figure>\n<p><strong>过程B</strong>每接收到一个响应，递增全局的delta_counter以及特定服务端的my_delta。对rho也一样，详细过程参考ServiceTracker::track_resp函数。<br><strong>过程A</strong>在准备请求阶段计算delta，公式如下：</p>\n<figure class=\"highlight c++\"><table><tr><td class=\"code\"><pre><div class=\"line\">delta = <span class=\"number\">1</span> + delta_counter - it-&gt;second.delta_prev_req - it-&gt;second.my_delta</div></pre></td></tr></table></figure>\n<p>其中，it代表ServiceTracker::server_map的迭代器。计算delta时之所以减去my_delta，是因为在t1~t2这个时间段内除了能够接收到其它服务端的响应外还会接收到来自自己服务端的响应，因此要减去自己服务端的响应个数。</p>\n<h2 id=\"服务端\"><a href=\"#服务端\" class=\"headerlink\" title=\"服务端\"></a>服务端</h2><p>从客户端发送请求到请求到达服务端的优先级队列的函数调用过程：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">SimulatedClient::run_req() --&gt; SimulatedClient::submit_f() --&gt; SimulatedServer::post() --&gt; </div><div class=\"line\">PushPriorityQueue::add_request() --&gt; PushPriorityQueue::do_add_request()</div></pre></td></tr></table></figure>\n<p>在这过程中，请求被添加上了一个伪Tag。之所以假是因为Tag中除当前时间外其它的内容都为0。具体参看PushPriorityQueue::do_add_request函数。</p>\n<p>从请求开始调度到请求送到服务端的内部队列的函数调用过程：<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">PushPriorityQueue::schedule_request() --&gt; PushPriorityQueue::submit_request() --&gt;</div><div class=\"line\">PushPriorityQueue::submit_top_request() --&gt; PriorityQueueBase::pop_process_request() --&gt; </div><div class=\"line\">PushPriorityQueue::handle_f() --&gt; SimulatedServer::inner_post()</div></pre></td></tr></table></figure></p>\n<p>PriorityQueueBase::pop_process_request开始计算请求的tag值，具体计算过程在RequestTag::tag_calc函数完成，计算过程和算法描述基本一致。注意，对第一个请求，RequestTag计算Tag时，依赖的前个请求为默认的prev_tag(0.0, 0.0, 0.0, TimeZero)，这样第一个请求的各个Tag值将为请求到达的时间。</p>\n<h3 id=\"数据结构\"><a href=\"#数据结构\" class=\"headerlink\" title=\"数据结构\"></a>数据结构</h3><h4 id=\"DMC客户\"><a href=\"#DMC客户\" class=\"headerlink\" title=\"DMC客户\"></a>DMC客户</h4><figure class=\"highlight c++\"><table><tr><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\">// C: 客户ID</span></div><div class=\"line\"><span class=\"comment\">// R: 请求类型</span></div><div class=\"line\"><span class=\"comment\">// B: B叉堆，堆的分叉数</span></div><div class=\"line\"><span class=\"keyword\">template</span>&lt;<span class=\"keyword\">typename</span> C, <span class=\"keyword\">typename</span> R, uint B&gt;</div><div class=\"line\"><span class=\"keyword\">class</span> PriorityQueueBase &#123;</div><div class=\"line\">     <span class=\"built_in\">std</span>::<span class=\"built_in\">map</span>&lt;C,ClientRecRef&gt; client_map;  <span class=\"comment\">// 每个客户的记录</span></div><div class=\"line\">&#125;;</div><div class=\"line\"></div><div class=\"line\"><span class=\"keyword\">class</span> ClientRec &#123;</div><div class=\"line\">    <span class=\"keyword\">uint32_t</span> cur_rho;</div><div class=\"line\">    <span class=\"keyword\">uint32_t</span> cur_delta;</div><div class=\"line\">    ClientInfo info;  <span class=\"comment\">// 包含limit、reservation、proportion参数及其倒数</span></div><div class=\"line\"></div><div class=\"line\">    RequestTag  prev_tag;  <span class=\"comment\">// 前一个请求的Tag值</span></div><div class=\"line\">    <span class=\"built_in\">std</span>::<span class=\"built_in\">deque</span>&lt;ClientReq&gt; requests;  <span class=\"comment\">// 请求队列</span></div><div class=\"line\">&#125;;</div></pre></td></tr></table></figure>\n<p>服务端中每个客户即ClientRec类的一个实例，客户的请求保存到ClientRec::requests队列，该队列是FCFS队列。也就说，同个客户端的请求的相对顺序保持不变。dmClock改变的是不同客户端间的处理顺序。</p>\n<h4 id=\"请求堆\"><a href=\"#请求堆\" class=\"headerlink\" title=\"请求堆\"></a>请求堆</h4><figure class=\"highlight c++\"><table><tr><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\">// I: 元素类型</span></div><div class=\"line\"><span class=\"comment\">// T: 元素类型 </span></div><div class=\"line\"><span class=\"comment\">// C: 用于比较两个T类型的参数大小的函数类</span></div><div class=\"line\"><span class=\"comment\">// K: 堆的分叉，K=2代表2叉堆</span></div><div class=\"line\"><span class=\"comment\">// heap_info: T的成员变量</span></div><div class=\"line\"><span class=\"keyword\">template</span>&lt;typenmae I, <span class=\"keyword\">typename</span> T, IndIntruHeapData T::*heap_info, <span class=\"keyword\">typename</span> C, uint K=<span class=\"number\">2</span>&gt;</div><div class=\"line\"><span class=\"keyword\">class</span> IndIntruHeap &#123;</div><div class=\"line\"><span class=\"keyword\">protected</span>:</div><div class=\"line\">    <span class=\"built_in\">std</span>::<span class=\"built_in\">vector</span>&lt;I&gt; data;  <span class=\"comment\">// 存储元素的容器</span></div><div class=\"line\">    <span class=\"keyword\">index_t</span> count;  <span class=\"comment\">// 元素个数</span></div><div class=\"line\">    C comparator;  <span class=\"comment\">// 重建堆时，比较两元素的优先顺序</span></div><div class=\"line\"><span class=\"keyword\">public</span>:</div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">void</span> <span class=\"title\">push</span><span class=\"params\">(<span class=\"keyword\">const</span> I&amp; item)</span></span>;  <span class=\"comment\">// 加入新元素</span></div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">void</span> <span class=\"title\">adjust</span><span class=\"params\">(T&amp; item)</span></span>;</div><div class=\"line\"></div><div class=\"line\"><span class=\"keyword\">protected</span>:</div><div class=\"line\">    <span class=\"comment\">// 从下至上重建堆</span></div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">void</span> <span class=\"title\">shift_up</span><span class=\"params\">(<span class=\"keyword\">index_t</span> i)</span></span>;</div><div class=\"line\">    <span class=\"comment\">// 由上到下重建堆</span></div><div class=\"line\">    <span class=\"comment\">// 如果i为最后1个元素，不调整</span></div><div class=\"line\">    <span class=\"comment\">// </span></div><div class=\"line\">    <span class=\"keyword\">typename</span> <span class=\"built_in\">std</span>::enable_if&lt;(K&gt;<span class=\"number\">2</span>)&amp;&amp;EnableBool,<span class=\"keyword\">void</span>&gt;::<span class=\"function\">type <span class=\"title\">sift_down</span><span class=\"params\">(<span class=\"keyword\">index_t</span> i)</span></span>;</div><div class=\"line\">    <span class=\"comment\">// 给定元素被修改后，调整其在堆中的位置</span></div><div class=\"line\">    <span class=\"comment\">// 如果i为根节点，则向下调整</span></div><div class=\"line\">    <span class=\"comment\">// 如果i大于其parent节点，则向上调整</span></div><div class=\"line\">    <span class=\"comment\">// 如果i小于其parent节点，则向下调整</span></div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">void</span> <span class=\"title\">sift</span><span class=\"params\">(<span class=\"keyword\">index_t</span> i)</span></span>;</div><div class=\"line\">&#125;;</div></pre></td></tr></table></figure>\n<p>参考<a href=\"http://www.cnblogs.com/yangecnu/archive/2014/03/02/3577568.html\">浅谈算法和数据结构: 五 优先级队列与堆排序</a> 获取更多关于堆的内容。</p>\n<figure class=\"highlight c++\"><table><tr><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">void</span> IndIntruHeap::sift_up(<span class=\"keyword\">index_t</span> i) &#123;</div><div class=\"line\">    <span class=\"comment\">// 入参i代表元素在IndIntruHeap::data中的位置（或者说，数组下标）</span></div><div class=\"line\">    <span class=\"keyword\">while</span> (i &gt; <span class=\"number\">0</span>) &#123;</div><div class=\"line\">        <span class=\"keyword\">index_t</span> pi = parent(i);  <span class=\"comment\">// i的父节点位置</span></div><div class=\"line\">        <span class=\"keyword\">if</span> (!comparator(*data[i], *data[pi])) &#123;</div><div class=\"line\">            <span class=\"comment\">// data[i] 优先级低于 data[pi]，保持相对结构不变</span></div><div class=\"line\">            <span class=\"keyword\">break</span>;</div><div class=\"line\">        &#125;</div><div class=\"line\"></div><div class=\"line\">        <span class=\"comment\">// data[i] 优先级低于 data[pi]，将data[i]上移</span></div><div class=\"line\">        <span class=\"comment\">// 因为pi的左节点的优先级低于pi，而pi的优先级低于i</span></div><div class=\"line\">        <span class=\"comment\">// 所以pi的左节点的优先级也低于i，i可以作为其父节点，不必再进行比较</span></div><div class=\"line\">        <span class=\"built_in\">std</span>::swap(data[i], data[pi]);</div><div class=\"line\">        <span class=\"comment\">// 修改数据的位置信息</span></div><div class=\"line\">        intru_data_of(data[i]) = i;</div><div class=\"line\">        intru_data_of(data[pi]) = pi;</div><div class=\"line\">        <span class=\"comment\">// data[i] 继续与其新的父节点进行比较</span></div><div class=\"line\">        i = pi;</div><div class=\"line\">   &#125;</div><div class=\"line\">&#125; </div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">// 此处的std::enable_if可以理解为返回值函数重载，实际上是模板偏特化</span></div><div class=\"line\"><span class=\"comment\">// 代码中实现了连个shift_down函数，用以区分K==2和K&gt;2两种情况，这两种情况分别调用两个不同的函数</span></div><div class=\"line\"><span class=\"keyword\">template</span>&lt;<span class=\"keyword\">bool</span> EnableBool=<span class=\"literal\">true</span>&gt;</div><div class=\"line\"><span class=\"keyword\">typename</span> <span class=\"built_in\">std</span>::enable_if&lt;(K&gt;<span class=\"number\">2</span>)&amp;&amp;EnableBool,<span class=\"keyword\">void</span>&gt;::type IndIntruHeap::sift_down(<span class=\"keyword\">index_t</span> i) &#123;</div><div class=\"line\">    <span class=\"comment\">// count为元素数目，i大于它为非法</span></div><div class=\"line\">    <span class=\"keyword\">if</span> (i &gt;= count) <span class=\"keyword\">return</span>;</div><div class=\"line\">    <span class=\"keyword\">while</span> (<span class=\"literal\">true</span>) &#123;</div><div class=\"line\">        <span class=\"comment\">// li为i最左边的子节点</span></div><div class=\"line\">        <span class=\"keyword\">index_t</span> li = lhs(i);</div><div class=\"line\"></div><div class=\"line\">        <span class=\"keyword\">if</span> (li &lt; count) &#123;</div><div class=\"line\">            <span class=\"comment\">// ri为i最右边的子节点</span></div><div class=\"line\">            <span class=\"keyword\">index_t</span> ri = <span class=\"built_in\">std</span>::min(rhs(i), count - <span class=\"number\">1</span>);</div><div class=\"line\"></div><div class=\"line\">            <span class=\"comment\">// 查找优先级最高的子节点</span></div><div class=\"line\">            <span class=\"keyword\">index_t</span> min_i = li;</div><div class=\"line\">            <span class=\"keyword\">for</span> (<span class=\"keyword\">index_t</span> k = li + <span class=\"number\">1</span>; k &lt;= ri; ++k) &#123;</div><div class=\"line\">                <span class=\"keyword\">if</span> (comparator(*data[k], *data[min_i])) &#123;</div><div class=\"line\">                    min_i = k;</div><div class=\"line\">                &#125;</div><div class=\"line\">            &#125;</div><div class=\"line\"></div><div class=\"line\">            <span class=\"keyword\">if</span> (comparator(*data[min_i], *data[i])) &#123;</div><div class=\"line\">                <span class=\"comment\">// i优先级低于子节点min_i，交换data[i]和data[min_i]的位置</span></div><div class=\"line\">                <span class=\"comment\">// 因为min_i已经是i所有子节点中优先级最高的，因此它可以作为其余子节点的父节点</span></div><div class=\"line\">                <span class=\"built_in\">std</span>::swap(data[i], data[min_i]);</div><div class=\"line\">                intru_data_of(data[i]) = i;</div><div class=\"line\">                intru_data_of(data[min_i]) = min_i;</div><div class=\"line\">                <span class=\"comment\">// i继续和新的子节点进行比较</span></div><div class=\"line\">                i = min_i;</div><div class=\"line\">            &#125; <span class=\"keyword\">else</span> &#123;</div><div class=\"line\">                <span class=\"comment\">// i节点比所有的子节点优先级都高，退出</span></div><div class=\"line\">                <span class=\"keyword\">break</span>;</div><div class=\"line\">            &#125;</div><div class=\"line\">        &#125; <span class=\"keyword\">else</span> &#123;</div><div class=\"line\">            <span class=\"comment\">// i节点已是叶子节点</span></div><div class=\"line\">            <span class=\"keyword\">break</span>;</div><div class=\"line\">        &#125;</div><div class=\"line\">    &#125;</div><div class=\"line\">&#125; <span class=\"comment\">// sift_down</span></div></pre></td></tr></table></figure>\n<h3 id=\"优先级比较\"><a href=\"#优先级比较\" class=\"headerlink\" title=\"优先级比较\"></a>优先级比较</h3><figure class=\"highlight c++\"><table><tr><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\">// 请求的Tag，包含预留(reservation)、上限(limit)和权重(proportion)。</span></div><div class=\"line\"><span class=\"keyword\">class</span> RequestTag &#123;</div><div class=\"line\">    <span class=\"keyword\">double</span> reservation;  <span class=\"comment\">// 预留</span></div><div class=\"line\">    <span class=\"keyword\">double</span> proportion;  <span class=\"comment\">// 权重</span></div><div class=\"line\">    <span class=\"keyword\">double</span> limit;  <span class=\"comment\">// 上限</span></div><div class=\"line\">    <span class=\"keyword\">bool</span> ready;  <span class=\"comment\">// 请求是否允许在Weight-based阶段被处理，当Limit低于当前时间（请求调度）</span></div><div class=\"line\">    Time arrival;  <span class=\"comment\">// 接收到请求的时间</span></div><div class=\"line\">&#125;;</div></pre></td></tr></table></figure>\n<p>请求调度的Weight-based阶段，先从Limit堆中筛选出能够在这个阶段被处理的请求，设置请求的ready字段为True。筛选的条件是请求的L Tag低于当前时间。筛选结束后再从Ready堆中选择请求。详情参考 PriorityQueueBase::do_next_request函数。</p>\n<figure class=\"highlight c++\"><table><tr><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\">// tag_field代表RequestTag类的一个成员变量</span></div><div class=\"line\"><span class=\"keyword\">template</span>&lt;<span class=\"keyword\">double</span> RequestTag::*tag_field, ReadyOption ready_opt, <span class=\"keyword\">bool</span> use_prop_delta&gt;</div><div class=\"line\"><span class=\"keyword\">struct</span> ClientCompare &#123;</div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">bool</span> <span class=\"title\">operator</span><span class=\"params\">()</span><span class=\"params\">(<span class=\"keyword\">const</span> ClientRec&amp; n1, <span class=\"keyword\">const</span> ClientRec&amp; n2)</span> <span class=\"keyword\">const</span> </span>&#123;</div><div class=\"line\">        <span class=\"comment\">// n1没有请求，n2有请求，返回false</span></div><div class=\"line\">        <span class=\"comment\">// n1没有请求，n2也没有请求，返回false，保持堆结构不变</span></div><div class=\"line\">        <span class=\"comment\">// n1有请求，n2没有请求，返回true</span></div><div class=\"line\">        <span class=\"comment\">// n1有请求，n1也有请求</span></div><div class=\"line\">    &#125;</div><div class=\"line\">&#125;;</div></pre></td></tr></table></figure>\n<p>比较客户n1和客户n2的Next请求的优先级。根据mClock算法，Tag值越小优先级应该越高。该函数的本意是，判断“n1的Next请求的优先级高于n2？”。下文使用tag1和tag2分别表示n1和n2的Next请求的Tag值。</p>\n<p>对<strong>reservation堆</strong>，依据“谁小谁优先级高”的原则，若tag1小于tag2，则返回True；<br>对<strong>ready堆</strong>，如果t1、t2两者的ready状态相同（都为true或者都为false），那么遵循“谁小谁优先级高”的原则；如果t1、t2的两者的ready状态不同（一个为true一个为false），那么依据“<strong>谁ready谁优先级高</strong>”的原则；<br>对<strong>limit堆</strong>，如果t1、t2两者的ready状态相同，那么遵循“谁小谁优先级高”的原则。如果两者的ready状态不同，按照“<strong>谁没ready谁优先级高</strong>”的原则，对已经Ready的请求将在请求调度中直接处理掉。</p>\n<h3 id=\"Tag校准\"><a href=\"#Tag校准\" class=\"headerlink\" title=\"Tag校准\"></a>Tag校准</h3><p>根据mClock算法，当有<strong>新VM</strong>启动时所有<strong>老VM</strong>请求的P Tag都要向左偏移一个位置（即P Tag减去偏移量），以避免新VM请求偏低影响公平性。偏移量为所有老VM的剩余请求中最小的P Tag减去当前时间（新VM第一个请求的到达时间）。反过来思考，将所有老VM的剩余请求在P Tag时间轴上向左偏移和将新VM的请求向右偏移的结果是相同的，一样能够消除不公平性。但这在代码实现上就简单了，只要在新VM中记录偏移量，并在Tag比较时加上偏移量即可，不必对所有老VM的剩余请求大动干戈。</p>\n<figure class=\"highlight c++\"><table><tr><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">template</span>&lt;<span class=\"keyword\">typename</span> C, <span class=\"keyword\">typename</span> R, uint B&gt;</div><div class=\"line\"><span class=\"keyword\">class</span> PriorityQueueBase &#123;</div><div class=\"line\">    c::IndIntruHeap&lt;ClientRecRef, ClientRec, &amp;ClientRec::reserv_heap_data,</div><div class=\"line\">                    ClientCompare&lt;&amp;RequestTag::reservation, ReadyOption::ignore, <span class=\"literal\">false</span>&gt;,</div><div class=\"line\">                    B&gt; resv_heap;</div><div class=\"line\">    c::IndIntruHeap&lt;ClientRecRef, ClientRec, &amp;ClientRec::lim_heap_data,</div><div class=\"line\">                    ClientCompare&lt;&amp;RequestTag::limit, ReadyOption::lowers, <span class=\"literal\">false</span>&gt;,</div><div class=\"line\">                    B&gt; limit_heap;</div><div class=\"line\">    c::IndIntruHeap&lt;ClientRecRef, ClientRec, &amp;ClientRec::ready_heap_data,</div><div class=\"line\">                    ClientCompare&lt;&amp;RequestTag::proportion, ReadyOption::raises, <span class=\"literal\">true</span>&gt;,</div><div class=\"line\">                    B&gt; ready_heap;</div><div class=\"line\">&#125;;</div></pre></td></tr></table></figure>\n<p>算法实现中分两步完成Tag校准：</p>\n<ol>\n<li>接收请求时，检查Client是否空闲，若空闲则计算偏移量，并将偏移量保存到ClientRec::prop_delta变量。参考PriorityQueueBase::do_add_request函数的<em>if (client.idle)</em>分支；</li>\n<li>比较P Tag时， 为请求的P Tag加上Client的prop_delta后再比较。参考ClientCompare的operator()函数。</li>\n</ol>\n<p>另外，注意只有P Tag需要校准。ClientCompare模板的最后一个参数代表是否使用prop_delta，只有ready_heap变量为True。</p>\n<h3 id=\"判断客户为空闲\"><a href=\"#判断客户为空闲\" class=\"headerlink\" title=\"判断客户为空闲\"></a>判断客户为空闲</h3><p>客户是否为空闲，在Tag校准中非常重要，只有发现空闲的客户时才需要进行校准。以下是算法实现中，判断客户空闲的一些手段：</p>\n<ul>\n<li>服务端第一次接收到来自该客户的请求，就认为该客户为空闲，参考 PriorityQueueBase::do_add_request函数</li>\n<li>定时检查，将超过idle_age(10分钟)时间内没有新增请求的客户标记为idle, 参考 PriorityQueueBase::do_clean函数</li>\n</ul>\n<p>对定时检查策略，除了将客户标记为idle状态外还会清理长久没有发送请求的客户。默认情况下，定时检查的时间间隔为6分钟，超过10分钟没新请求的客户将被标记为idle状态，超过15分钟没有新请求的客户将被从服务端清理。</p>\n<h3 id=\"请求调度\"><a href=\"#请求调度\" class=\"headerlink\" title=\"请求调度\"></a>请求调度</h3><p>请求调度，从堆结构中获取待处理请求的函数调用过程：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">PushPriorityQueue::schedule_request() --&gt; PushPriorityQueue::next_request --&gt; PriorityQueueBase::do_next_request()</div></pre></td></tr></table></figure>\n<p>DmClock算法的请求调度流程在PriorityQueueBase::do_next_request函数中实现。</p>\n<p><strong>递减剩余请求的R Tag</strong><br>根据DmClock算法，在Weight-based阶段每处理一个请求时，为避免影响R tag的排序，需要将该客户的剩余请求的R tag值递减 1/ri。这个过程在函数PushPriorityQueue::submit_request()以及PriorityQueueBase::reduce_reservation_tags函数中实现。</p>\n<hr>\n<p><strong>版权声明</strong></p>\n<p>本作品采用<a href=\"https://creativecommons.org/licenses/by/2.5/cn/\">知识共享署名2.5 中国大陆许可协议</a>进行许可，欢迎转载，但转载请注明来自<a href=\"https://wuxiangwei.cn/\"><em>Shanno’s Blog</em></a>，并保持转载后文章内容的完整。本人保留所有版权相关权利，侵权必究。</p>"},{"title":"DmClock算法原理分析","date":"2016-11-10T06:01:26.000Z","toc":true,"_content":"\n**术语**\n\n| 英文 | 中文 |\n|:--|:--|\n| share/weight | 权重 |\n| reservation | 预留或下限 | \n| limit | 上限 |\n\n\n存储IO调度和CPU、内存调度相比存在以下几个不同点：\n\n* CPU、内存通常是主机内部的资源调度。但存储IO通常是跨主机的，多台主机使用共享的存储资源，主机内IO的变化会影响相邻主机的IO。\n* Hypervisor中IO调度层位于客户机IO调度的底层。因此会滋生出许多问题，例如同台主机不同VM间的局部性、同时存在多种不同大小的IO、不同请求的优先级以及突发负载等。\n\n<!--more-->\n\n\n| 算法类别 | 按比例 | 延时 | 预留 | 上限 | 突发负载 |\n|:--|:--|:--|:--|:--|:--|\n| 第1类算法 | Y | N | N | N | N |\n| 第2类算法 | Y | Y | N | N | N |\n| 第3类算法 | Y | Y | Y | N | N |\n| mClock    | Y | Y | Y | Y | Y |\n\nIO资源调度一般有3类相关的算法。第1类算法按比例分配IO资源，例如SFQ(D)，IO资源主要包括IOPS和带宽资源。第2类算法也是按比例分配IO资源，但IO资源包括延迟。第3类算法还是按比例分配IO资源，支持对给定的客户机提供最低资源保证。\n\n## 一个例子\n\n| VM | IOPS | 延时 | 带宽 | 权重(S) | 上限(L) | 预留(R) |\n|:--|:--|:--|:--|:--|:--|:--|\n| RD   | 低 | 低 | - | 100 | Inf | 250 |\n| OLTP | 高 | 低 | - | 200 | Inf | 250 |\n| DM   | 高 | - | - |  300 | 1000 | 0 |\n\n上表给出3个VM，分别跑不同的业务。RD运行远程桌面，对IOPS要求不高但对延迟要求高，当IOPS较低时用户体验会很差因此保证预留为250个IOPS。OLTP要求高IOPS和低延迟，同样地预留为250个IOPS。DM运行数据迁移业务，设置上限为1000 IOPS，避免过度消耗系统带宽影响其它业务。mClock的目标是将VM的IO限制在预留和上限之间。\n\n当系统总IOPS为1200时，按**权重比例**分配IO，那么每个VM分配到的IOPS如下：\n\n| VM | IOPS(按比例) | IOPS(mclock) |\n|:--|:--|:--|\n| RD | 200 | 250 |\n| OLTP | 400 | 380 |\n| DM | 600 | 570 |\n\n虽然此时系统拥有能够满足各个VM预留的IOPS（250 + 250 + 0 = 500），但RD只分配到了200IOPS低于其预留值。这种情况下，mclock的做法是：首先分配250 IOPS给RD，然后将剩余的950 IOPS按2:3的比例分配给OLTP和DM。\n当系统总IOPS低于500时，无法满足每个VM的预留，因此mclock的做法是将可用的系统IO按照**预留比例**分配给VM，其中DM无法分配到IO。\n当系统总IOPS在1500到2000时，直接按**权重比例**分配IO。此时不会出现VM的IO低于预留或者高于上限的情况。\n当系统总IOPS高于2000时，直接按**权重比例**分配IO会导致DM的IOPS高于其上限。mclock的做法是：先为DM分配1000个IOPS，然后将剩余的IOPS按1:2的比例分配给RD和OLTP。\n\n总而言之，分配策略会根据总吞吐量和活动VM而动态变化。将VM划分为三类：预留(R)、上限(L)、权重(P)，系统总吞吐量为T，那么IO分配公式如下：\n\n![](http://ohn764ue3.bkt.clouddn.com/DmClock/paper/Eq.png-name)\n\nmclock通过一种创新的tagging分配策略来达成上述公式。\n上述公式存在的问题是，Tp可能小于0，也就是说系统总吞吐量要低于所有VM的总预留值。当出现这种情况时，mclock按**预留比例**来分配IO资源。这在某些情况下可能无法满足用户需求，例如有些VM没有分配预留值或者预留值为0，那么该VM可能永远无法分配到IO。不过这可以通过为每个VM都设置预留来解决。此外，针对这种情况也可考虑通过为预留设置优先级来解决（本文不再涉及）。\n\n\n# mClock算法\n\n适用于单主机的情况。\nmClock是种基于tag的算法，基于tag的算法的**基本思路**: 为每个请求赋予1个tag，scheduler按照tag从小到大的顺序依次处理请求。\n\n| VM | 1th | 2nd | 3rd | 4th | 5th | \n|:--|:--|:--|:--|:--|:--|\n| A | **2** | **4** | **6** | 8 | 10 | \n| B | **3** | **6** | 9 | 12 | 15 | \n| C | **6** | 12 | 18| 24 | 30 | \n\n举个例子，假设A、B、C三台VM的权重分别为1/2,1/3和1/6。\nscheduler为每个请求设置tag的方法：在上个请求权重的基础上递增1/wi。假设主机分别接收到了5个来自各台VM的请求，每个请求的tag如上表所示。假设主机在给定的时间段内只能处理其中的6个请求，那么按照tag的顺序，被处理的请求为：A的前3个、B的前2个、C的前1个请求。A、B、C被处理的请求数目恰好是它们权重的比例3:2:1。\n\n这种tag赋值方法隐含着一个前提条件，那就是每个VM都是同时启动同时处于活动状态的。假如B一开始处于非活动状态，直到A接收到第5个请求时B才启动，这时B的连续若干个请求的tag值都会小于A，这将导致IO分配不符合预先配置的比例。为解决这个问题需要引入一个全局虚拟时钟（global virtual time），每个请求的tag尽量保持与gvt相同。\n\nmClock提出两个主要的概念：多时钟(multiple real-time clocks)和动态时钟选择(dynamic clock selection)。多时钟是指mClock分别为上限(L)、预留(R)和权重(P)提供独立的tag，每个VM的请求都包含3个不同的tag值。动态始终选择指scheduler动态选择1个tag来调度IO资源。\n\nmClock由三大组件组成：Tag赋值、Tag微调、请求调度。\n\n\n## Tag赋值 Tag Assigment\n\n![](http://ohn764ue3.bkt.clouddn.com/DmClock/paper/Symbols.png-name)\n\n目的是为每个请求设置预留(R)、上限(L)和权重(P)三个Tag值。3个Tag的计算公式都相同，此处以预留为例。\n\n![](http://ohn764ue3.bkt.clouddn.com/DmClock/paper/Eq_3.png-name)\n\n\n假设对给定VM1，设置其预留的IOPS为10。也就说，平均每隔1/10秒处理一个请求。将VM1的请求对应到R Tag时间轴上，相邻两个请求的时间间隔为1/10。假设Server从T1时刻开始接收来自VM1的请求，在T2时刻开始处理请求。T2时刻只要完成R Tag时间轴中Tag位于T2时刻之前的请求，就能够满足预留的要求了。这是因为，R Tag时间轴的[T1, T2)区间可以排列10\\*(T2-T1)个请求，完成10\\*(T2-T1)个请求刚好可以满足平均IOPS为10。如果这段时间内Server接收到超过10\\*(T2-T1)个请求，那么超过部分将排列到T2时刻后面，可暂不处理。如果这段时间内Server接收到的请求数目小于或等于10\\*(T2-T1)，那么这些请求都将排列在T2时刻前面，被处理掉。\n\nR Tag时间轴上的时间段[T1, T2)就像一个固定大小的容器，容器的大小便是用户配置的预留。这段时间内，如果有大量的IO请求，那么这些请求将填满容器，同时多出的部分将被放到后面的容器。T2时刻处理请求时，只需完成容器内部的请求就能够满足预留要求。\n\n根据公式，VM1的第一个请求的R Tag将被设置为T1，T1是Server接收到VM1第一个请求的时刻。为什么R Tag的计算公式要跟当前时间进行比较？考虑如下场景：VM1先在T1时刻启动，运行段时间后在T2时刻关闭，后又在T3时刻启动。那么，从T3时刻开始VM1的请求的R Tag将相对较小，因为它是基于T2时刻前接收到的最后一个请求为基准的，而Server是按照R Tag从小到大的顺序处理请求的, 也就说，VM1 T3时刻后的请求具有空闲优先，将被优先处理掉。这对那些一直处于活动状态的VM来说是不公平的。算法实现时，可以通过一个定时器来定时检查VM是否空闲。空闲的判断标准是，如果VM的最后一个请求的达到时间到当前时间的时间差大于给定的阈值时，认为VM空闲。这样，VM重新活动后的第一个请求将被设置为该请求的达到时间，以此来消除空闲优势。\n\nL Tag和R Tag比较类似，都可以理解为IOPS。L Tag的[T1, T2)容器的大小代表的是VM的上限。T2时刻处理请求时，若VM1剩余请求的R Tag都大于T2，那就不能继续处理VM1的请求了，否则，将导致VM1突破IOPS上限。\n\nP Tag不同于L和R Tag，P Tag是个**相对值**(比例值)不能理解为IOPS。它的主要作用是按照给定的比例将Server的IO能力分配给不同的VM。\n\n## Tag校准 Tag Adjustment\n\n上文提到的**空闲优势**是VM处于非活动状态带来的一个问题。除此之外，**新VM**（刚处于活动状态的VM）也会带来一些相对于**老VM**(一直处于活动状态的VM)来说不公平的调度问题。老VM运行一段时间后，尤其是一直处于繁忙状态的VM，将越来越偏离Server的当前时间。\n\n![](http://ohn764ue3.bkt.clouddn.com/DmClock/paper/adjust_01.png-name)\n\n假设，如上图所示，VM_A在T1时刻启动，其首个请求的Tag设置为T1。VM_B在T2时刻启动，此时VM_A剩余请求中P Tag最小的请求的Tag为T3。如此，新启动的VM_B将获得相对于VM_A有(T3-T2)的**后起优势**。\n\n![](http://ohn764ue3.bkt.clouddn.com/DmClock/paper/adjust_02.png-name)\n\n消除新VM带来的后起优势的问题也很简单。将老VM的剩余请求中最早的请求的P Tag调整到当前时间（新VM的启动时间），同时保证老VM的剩余请求的相对位置不变。简而言之，将VM_A的所有剩余请求的P Tag都减少(T3-T1)，如此调整后新VM和老VM将处于相同的起跑线。此例中只有两个VM，一个老VM，如果有多个老VM时，调整时还要保证老VM间的相对位置不变。因此，P Tag的差值应该是**所有老VM**中最小请求的P Tag和当前时间的差值。\n\n## 请求调度 Request Scheduling\n\nmClock的请求调度分两个阶段，这两个阶段交替执行。第一个阶段称为**Constraint-based**阶段，目的是保证每个VM的预留。根据上文的解释，只要在R Tag时间轴上将所有低于当前时间的请求全部发送即可。第二个阶段称为**Weight-based**阶段，顾名思义，就是依据P Tag按比例处理不同VM的请求个数。同时，这个阶段还考虑VM的上限，对L Tag高于当前时间的请求不予处理。\n\n![](http://ohn764ue3.bkt.clouddn.com/DmClock/paper/alg.png-name)\n\nWeight-based阶段的请求调度会影响到下一轮的Constraint-based阶段的请求调度。假设T1时刻开始第一轮请求调度，T2时刻开始第二轮请求调度，T1时刻VM1请求的P Tag已经排到T2后面了。如果第1轮请求调度的Weight-based阶段将VM1中R Tag位于[T1, T2)时间段的请求全部处理掉，那么第2轮请求调度的Constraint-based阶段VM1将无请求可调度，也就是说，在T1~T2时间段内VM1的平均IOPS为0。这不符合用户的预留要求。\n\n破解这个问题的方法是，在Weight-based阶段每处理一个VM1的请求时，就将VM1中所有剩余请求的R Tag递减1/r。如此，原来R Tag排在T2后面的请求在Weigt-based阶段完成后将被重排到到T1、T2间。第2轮请求调度的Constraint-based阶段将不会出现无请求可调度的情况。\n\n## 特定于存储的问题 Storage-specific Issue\n\nmClock算法能够应用于多种资源的调度，例如CPU、内存和存储。在应用于存储资源调度时，有诸多特定于存储相关的问题。\n\n### 突发请求 Burst Handling\n\n目标，为突发请求设置一定的优先处理的好处。何为突发请求？VM空闲了一小段时间，突然又有新请求的情况。给予好处的方法很简单，对这种情况原先是直接将新请求的P Tag设置为当前时间，现在可以将其设置为比当前时间更早点的时间。时间越早，好处越大。注意，此处只修改P Tag，不会影响R Tag。\n\n### 请求类型 Request Type\n\n无差别对待读请求和写请求。不区分读写请求的原因是，在Hypervisor层如果调整了读写请求的顺序将出现一致性问题。但，如果读写顺序在应用层保证，那么在存储层或许可以调整读、写请求的相对顺序。实际上，很多商业存储都允许分别为读或写设置优先级。\n\n### IO大小 IO Size\n\nP Tag是个相对值。默认情况，这个相对值指的是IOPS。因为，它只考虑了请求的个数，没有考虑每个请求的大小、延迟等其它信息。\n\n\n\n若P Tag指的是带宽，那请考虑请求大小。假设VM1、VM2的P值都为10，希望这两个VM的带宽是相同的。同时假设，VM1 请求的大小为1M，VM2请求的大小为2M。按照原来的Tag Assigment方法，每次请求调度VM1和VM2处理的请求数目应该都相同，这将导致VM2的带宽为VM1的两倍。解决方法是，以1M大小的请求为参考，将2M大小的请求间的间隔调整为1M请求的2倍。如此调整，Server原本每处理10个VM1的请求就也要同时处理10个VM2的请求，现在每处理10个VM1的请求的同时只需要处理5个VM2的请求，从而使得VM1和VM2的带宽相同。\n\n### 请求局部性 Request Location\n\nIO请求具有一定的空间局部性，为提高整体的处理性能，mClock允许批量地处理一组来自同个VM的请求。只要这些请求的总大小接近逻辑块(logical block number space)大小，例如4MB。同时，设定一次批量处理中请求的数目，例如最多处理8个。\n\n上述的优化方法会影响IO的时延。举个例子，本来按照排序是第N个进行处理的请求，由于批量处理前面N个请求（属于其它VM）都各自夹带了8个相邻的请求，从而实际上将自己排到了第8N的位置。\n\n# dmClock算法\n\n不同于单主机的情况，给定VM的请求全部经由该主机进行处理，多主机的情况中给定VM的请求将分散给不同的主机进行处理。因此，在Tag赋值阶段，处理请求的主机并不知道VM的前一个请求的Tag值，只知道本主机处理的来自该VM的前个请求的Tag值。为解决这个问题，VM向目标主机发送请求时需要携带相关信息。目标主机根据这些信息来分配Tag的大小。\n\n![](http://ohn764ue3.bkt.clouddn.com/DmClock/paper/Eq_5.png-name)\n\n对给定的VM，delta代表处理当前请求的存储主机从接收到前个请求到当前请求这段时间内，VM发送给其它主机的请求数目。同样地，rou代表对给定的VM，处理当前请求的目标主机从接收到前个请求到当前请求这段时间内，VM发送给其它主机并且在constraint-based阶段被处理的请求的个数。\n\n![](http://ohn764ue3.bkt.clouddn.com/DmClock/paper/dmclock_01.png-name)\n\n举个例子，如上图所示，对Server3而言，只在T1、T2两个时间点接收到请求。在T2时刻时，delta值为Server1和Server2在T1、T2时间段中接收的请求数目，为8。rou为在T1、T2时间段内Server1和Server2中在constraint-based阶段处理掉的请求个数，为3。\n","source":"_posts/DmClock/MCLOCK_paper.md","raw":"\n---\ntitle: \"DmClock算法原理分析\"\ndate: 2016-11-10 14:01:26\ncategories: [mClock]\ntags: [mClock]\ntoc: true\n---\n\n**术语**\n\n| 英文 | 中文 |\n|:--|:--|\n| share/weight | 权重 |\n| reservation | 预留或下限 | \n| limit | 上限 |\n\n\n存储IO调度和CPU、内存调度相比存在以下几个不同点：\n\n* CPU、内存通常是主机内部的资源调度。但存储IO通常是跨主机的，多台主机使用共享的存储资源，主机内IO的变化会影响相邻主机的IO。\n* Hypervisor中IO调度层位于客户机IO调度的底层。因此会滋生出许多问题，例如同台主机不同VM间的局部性、同时存在多种不同大小的IO、不同请求的优先级以及突发负载等。\n\n<!--more-->\n\n\n| 算法类别 | 按比例 | 延时 | 预留 | 上限 | 突发负载 |\n|:--|:--|:--|:--|:--|:--|\n| 第1类算法 | Y | N | N | N | N |\n| 第2类算法 | Y | Y | N | N | N |\n| 第3类算法 | Y | Y | Y | N | N |\n| mClock    | Y | Y | Y | Y | Y |\n\nIO资源调度一般有3类相关的算法。第1类算法按比例分配IO资源，例如SFQ(D)，IO资源主要包括IOPS和带宽资源。第2类算法也是按比例分配IO资源，但IO资源包括延迟。第3类算法还是按比例分配IO资源，支持对给定的客户机提供最低资源保证。\n\n## 一个例子\n\n| VM | IOPS | 延时 | 带宽 | 权重(S) | 上限(L) | 预留(R) |\n|:--|:--|:--|:--|:--|:--|:--|\n| RD   | 低 | 低 | - | 100 | Inf | 250 |\n| OLTP | 高 | 低 | - | 200 | Inf | 250 |\n| DM   | 高 | - | - |  300 | 1000 | 0 |\n\n上表给出3个VM，分别跑不同的业务。RD运行远程桌面，对IOPS要求不高但对延迟要求高，当IOPS较低时用户体验会很差因此保证预留为250个IOPS。OLTP要求高IOPS和低延迟，同样地预留为250个IOPS。DM运行数据迁移业务，设置上限为1000 IOPS，避免过度消耗系统带宽影响其它业务。mClock的目标是将VM的IO限制在预留和上限之间。\n\n当系统总IOPS为1200时，按**权重比例**分配IO，那么每个VM分配到的IOPS如下：\n\n| VM | IOPS(按比例) | IOPS(mclock) |\n|:--|:--|:--|\n| RD | 200 | 250 |\n| OLTP | 400 | 380 |\n| DM | 600 | 570 |\n\n虽然此时系统拥有能够满足各个VM预留的IOPS（250 + 250 + 0 = 500），但RD只分配到了200IOPS低于其预留值。这种情况下，mclock的做法是：首先分配250 IOPS给RD，然后将剩余的950 IOPS按2:3的比例分配给OLTP和DM。\n当系统总IOPS低于500时，无法满足每个VM的预留，因此mclock的做法是将可用的系统IO按照**预留比例**分配给VM，其中DM无法分配到IO。\n当系统总IOPS在1500到2000时，直接按**权重比例**分配IO。此时不会出现VM的IO低于预留或者高于上限的情况。\n当系统总IOPS高于2000时，直接按**权重比例**分配IO会导致DM的IOPS高于其上限。mclock的做法是：先为DM分配1000个IOPS，然后将剩余的IOPS按1:2的比例分配给RD和OLTP。\n\n总而言之，分配策略会根据总吞吐量和活动VM而动态变化。将VM划分为三类：预留(R)、上限(L)、权重(P)，系统总吞吐量为T，那么IO分配公式如下：\n\n![](http://ohn764ue3.bkt.clouddn.com/DmClock/paper/Eq.png-name)\n\nmclock通过一种创新的tagging分配策略来达成上述公式。\n上述公式存在的问题是，Tp可能小于0，也就是说系统总吞吐量要低于所有VM的总预留值。当出现这种情况时，mclock按**预留比例**来分配IO资源。这在某些情况下可能无法满足用户需求，例如有些VM没有分配预留值或者预留值为0，那么该VM可能永远无法分配到IO。不过这可以通过为每个VM都设置预留来解决。此外，针对这种情况也可考虑通过为预留设置优先级来解决（本文不再涉及）。\n\n\n# mClock算法\n\n适用于单主机的情况。\nmClock是种基于tag的算法，基于tag的算法的**基本思路**: 为每个请求赋予1个tag，scheduler按照tag从小到大的顺序依次处理请求。\n\n| VM | 1th | 2nd | 3rd | 4th | 5th | \n|:--|:--|:--|:--|:--|:--|\n| A | **2** | **4** | **6** | 8 | 10 | \n| B | **3** | **6** | 9 | 12 | 15 | \n| C | **6** | 12 | 18| 24 | 30 | \n\n举个例子，假设A、B、C三台VM的权重分别为1/2,1/3和1/6。\nscheduler为每个请求设置tag的方法：在上个请求权重的基础上递增1/wi。假设主机分别接收到了5个来自各台VM的请求，每个请求的tag如上表所示。假设主机在给定的时间段内只能处理其中的6个请求，那么按照tag的顺序，被处理的请求为：A的前3个、B的前2个、C的前1个请求。A、B、C被处理的请求数目恰好是它们权重的比例3:2:1。\n\n这种tag赋值方法隐含着一个前提条件，那就是每个VM都是同时启动同时处于活动状态的。假如B一开始处于非活动状态，直到A接收到第5个请求时B才启动，这时B的连续若干个请求的tag值都会小于A，这将导致IO分配不符合预先配置的比例。为解决这个问题需要引入一个全局虚拟时钟（global virtual time），每个请求的tag尽量保持与gvt相同。\n\nmClock提出两个主要的概念：多时钟(multiple real-time clocks)和动态时钟选择(dynamic clock selection)。多时钟是指mClock分别为上限(L)、预留(R)和权重(P)提供独立的tag，每个VM的请求都包含3个不同的tag值。动态始终选择指scheduler动态选择1个tag来调度IO资源。\n\nmClock由三大组件组成：Tag赋值、Tag微调、请求调度。\n\n\n## Tag赋值 Tag Assigment\n\n![](http://ohn764ue3.bkt.clouddn.com/DmClock/paper/Symbols.png-name)\n\n目的是为每个请求设置预留(R)、上限(L)和权重(P)三个Tag值。3个Tag的计算公式都相同，此处以预留为例。\n\n![](http://ohn764ue3.bkt.clouddn.com/DmClock/paper/Eq_3.png-name)\n\n\n假设对给定VM1，设置其预留的IOPS为10。也就说，平均每隔1/10秒处理一个请求。将VM1的请求对应到R Tag时间轴上，相邻两个请求的时间间隔为1/10。假设Server从T1时刻开始接收来自VM1的请求，在T2时刻开始处理请求。T2时刻只要完成R Tag时间轴中Tag位于T2时刻之前的请求，就能够满足预留的要求了。这是因为，R Tag时间轴的[T1, T2)区间可以排列10\\*(T2-T1)个请求，完成10\\*(T2-T1)个请求刚好可以满足平均IOPS为10。如果这段时间内Server接收到超过10\\*(T2-T1)个请求，那么超过部分将排列到T2时刻后面，可暂不处理。如果这段时间内Server接收到的请求数目小于或等于10\\*(T2-T1)，那么这些请求都将排列在T2时刻前面，被处理掉。\n\nR Tag时间轴上的时间段[T1, T2)就像一个固定大小的容器，容器的大小便是用户配置的预留。这段时间内，如果有大量的IO请求，那么这些请求将填满容器，同时多出的部分将被放到后面的容器。T2时刻处理请求时，只需完成容器内部的请求就能够满足预留要求。\n\n根据公式，VM1的第一个请求的R Tag将被设置为T1，T1是Server接收到VM1第一个请求的时刻。为什么R Tag的计算公式要跟当前时间进行比较？考虑如下场景：VM1先在T1时刻启动，运行段时间后在T2时刻关闭，后又在T3时刻启动。那么，从T3时刻开始VM1的请求的R Tag将相对较小，因为它是基于T2时刻前接收到的最后一个请求为基准的，而Server是按照R Tag从小到大的顺序处理请求的, 也就说，VM1 T3时刻后的请求具有空闲优先，将被优先处理掉。这对那些一直处于活动状态的VM来说是不公平的。算法实现时，可以通过一个定时器来定时检查VM是否空闲。空闲的判断标准是，如果VM的最后一个请求的达到时间到当前时间的时间差大于给定的阈值时，认为VM空闲。这样，VM重新活动后的第一个请求将被设置为该请求的达到时间，以此来消除空闲优势。\n\nL Tag和R Tag比较类似，都可以理解为IOPS。L Tag的[T1, T2)容器的大小代表的是VM的上限。T2时刻处理请求时，若VM1剩余请求的R Tag都大于T2，那就不能继续处理VM1的请求了，否则，将导致VM1突破IOPS上限。\n\nP Tag不同于L和R Tag，P Tag是个**相对值**(比例值)不能理解为IOPS。它的主要作用是按照给定的比例将Server的IO能力分配给不同的VM。\n\n## Tag校准 Tag Adjustment\n\n上文提到的**空闲优势**是VM处于非活动状态带来的一个问题。除此之外，**新VM**（刚处于活动状态的VM）也会带来一些相对于**老VM**(一直处于活动状态的VM)来说不公平的调度问题。老VM运行一段时间后，尤其是一直处于繁忙状态的VM，将越来越偏离Server的当前时间。\n\n![](http://ohn764ue3.bkt.clouddn.com/DmClock/paper/adjust_01.png-name)\n\n假设，如上图所示，VM_A在T1时刻启动，其首个请求的Tag设置为T1。VM_B在T2时刻启动，此时VM_A剩余请求中P Tag最小的请求的Tag为T3。如此，新启动的VM_B将获得相对于VM_A有(T3-T2)的**后起优势**。\n\n![](http://ohn764ue3.bkt.clouddn.com/DmClock/paper/adjust_02.png-name)\n\n消除新VM带来的后起优势的问题也很简单。将老VM的剩余请求中最早的请求的P Tag调整到当前时间（新VM的启动时间），同时保证老VM的剩余请求的相对位置不变。简而言之，将VM_A的所有剩余请求的P Tag都减少(T3-T1)，如此调整后新VM和老VM将处于相同的起跑线。此例中只有两个VM，一个老VM，如果有多个老VM时，调整时还要保证老VM间的相对位置不变。因此，P Tag的差值应该是**所有老VM**中最小请求的P Tag和当前时间的差值。\n\n## 请求调度 Request Scheduling\n\nmClock的请求调度分两个阶段，这两个阶段交替执行。第一个阶段称为**Constraint-based**阶段，目的是保证每个VM的预留。根据上文的解释，只要在R Tag时间轴上将所有低于当前时间的请求全部发送即可。第二个阶段称为**Weight-based**阶段，顾名思义，就是依据P Tag按比例处理不同VM的请求个数。同时，这个阶段还考虑VM的上限，对L Tag高于当前时间的请求不予处理。\n\n![](http://ohn764ue3.bkt.clouddn.com/DmClock/paper/alg.png-name)\n\nWeight-based阶段的请求调度会影响到下一轮的Constraint-based阶段的请求调度。假设T1时刻开始第一轮请求调度，T2时刻开始第二轮请求调度，T1时刻VM1请求的P Tag已经排到T2后面了。如果第1轮请求调度的Weight-based阶段将VM1中R Tag位于[T1, T2)时间段的请求全部处理掉，那么第2轮请求调度的Constraint-based阶段VM1将无请求可调度，也就是说，在T1~T2时间段内VM1的平均IOPS为0。这不符合用户的预留要求。\n\n破解这个问题的方法是，在Weight-based阶段每处理一个VM1的请求时，就将VM1中所有剩余请求的R Tag递减1/r。如此，原来R Tag排在T2后面的请求在Weigt-based阶段完成后将被重排到到T1、T2间。第2轮请求调度的Constraint-based阶段将不会出现无请求可调度的情况。\n\n## 特定于存储的问题 Storage-specific Issue\n\nmClock算法能够应用于多种资源的调度，例如CPU、内存和存储。在应用于存储资源调度时，有诸多特定于存储相关的问题。\n\n### 突发请求 Burst Handling\n\n目标，为突发请求设置一定的优先处理的好处。何为突发请求？VM空闲了一小段时间，突然又有新请求的情况。给予好处的方法很简单，对这种情况原先是直接将新请求的P Tag设置为当前时间，现在可以将其设置为比当前时间更早点的时间。时间越早，好处越大。注意，此处只修改P Tag，不会影响R Tag。\n\n### 请求类型 Request Type\n\n无差别对待读请求和写请求。不区分读写请求的原因是，在Hypervisor层如果调整了读写请求的顺序将出现一致性问题。但，如果读写顺序在应用层保证，那么在存储层或许可以调整读、写请求的相对顺序。实际上，很多商业存储都允许分别为读或写设置优先级。\n\n### IO大小 IO Size\n\nP Tag是个相对值。默认情况，这个相对值指的是IOPS。因为，它只考虑了请求的个数，没有考虑每个请求的大小、延迟等其它信息。\n\n\n\n若P Tag指的是带宽，那请考虑请求大小。假设VM1、VM2的P值都为10，希望这两个VM的带宽是相同的。同时假设，VM1 请求的大小为1M，VM2请求的大小为2M。按照原来的Tag Assigment方法，每次请求调度VM1和VM2处理的请求数目应该都相同，这将导致VM2的带宽为VM1的两倍。解决方法是，以1M大小的请求为参考，将2M大小的请求间的间隔调整为1M请求的2倍。如此调整，Server原本每处理10个VM1的请求就也要同时处理10个VM2的请求，现在每处理10个VM1的请求的同时只需要处理5个VM2的请求，从而使得VM1和VM2的带宽相同。\n\n### 请求局部性 Request Location\n\nIO请求具有一定的空间局部性，为提高整体的处理性能，mClock允许批量地处理一组来自同个VM的请求。只要这些请求的总大小接近逻辑块(logical block number space)大小，例如4MB。同时，设定一次批量处理中请求的数目，例如最多处理8个。\n\n上述的优化方法会影响IO的时延。举个例子，本来按照排序是第N个进行处理的请求，由于批量处理前面N个请求（属于其它VM）都各自夹带了8个相邻的请求，从而实际上将自己排到了第8N的位置。\n\n# dmClock算法\n\n不同于单主机的情况，给定VM的请求全部经由该主机进行处理，多主机的情况中给定VM的请求将分散给不同的主机进行处理。因此，在Tag赋值阶段，处理请求的主机并不知道VM的前一个请求的Tag值，只知道本主机处理的来自该VM的前个请求的Tag值。为解决这个问题，VM向目标主机发送请求时需要携带相关信息。目标主机根据这些信息来分配Tag的大小。\n\n![](http://ohn764ue3.bkt.clouddn.com/DmClock/paper/Eq_5.png-name)\n\n对给定的VM，delta代表处理当前请求的存储主机从接收到前个请求到当前请求这段时间内，VM发送给其它主机的请求数目。同样地，rou代表对给定的VM，处理当前请求的目标主机从接收到前个请求到当前请求这段时间内，VM发送给其它主机并且在constraint-based阶段被处理的请求的个数。\n\n![](http://ohn764ue3.bkt.clouddn.com/DmClock/paper/dmclock_01.png-name)\n\n举个例子，如上图所示，对Server3而言，只在T1、T2两个时间点接收到请求。在T2时刻时，delta值为Server1和Server2在T1、T2时间段中接收的请求数目，为8。rou为在T1、T2时间段内Server1和Server2中在constraint-based阶段处理掉的请求个数，为3。\n","slug":"DmClock/MCLOCK_paper","published":1,"updated":"2016-12-23T07:55:41.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cix1kgaub000zdaeieifaadhm","content":"<p><strong>术语</strong></p>\n<table>\n<thead>\n<tr>\n<th style=\"text-align:left\">英文</th>\n<th style=\"text-align:left\">中文</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:left\">share/weight</td>\n<td style=\"text-align:left\">权重</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">reservation</td>\n<td style=\"text-align:left\">预留或下限</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">limit</td>\n<td style=\"text-align:left\">上限</td>\n</tr>\n</tbody>\n</table>\n<p>存储IO调度和CPU、内存调度相比存在以下几个不同点：</p>\n<ul>\n<li>CPU、内存通常是主机内部的资源调度。但存储IO通常是跨主机的，多台主机使用共享的存储资源，主机内IO的变化会影响相邻主机的IO。</li>\n<li>Hypervisor中IO调度层位于客户机IO调度的底层。因此会滋生出许多问题，例如同台主机不同VM间的局部性、同时存在多种不同大小的IO、不同请求的优先级以及突发负载等。</li>\n</ul>\n<a id=\"more\"></a>\n<table>\n<thead>\n<tr>\n<th style=\"text-align:left\">算法类别</th>\n<th style=\"text-align:left\">按比例</th>\n<th style=\"text-align:left\">延时</th>\n<th style=\"text-align:left\">预留</th>\n<th style=\"text-align:left\">上限</th>\n<th style=\"text-align:left\">突发负载</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:left\">第1类算法</td>\n<td style=\"text-align:left\">Y</td>\n<td style=\"text-align:left\">N</td>\n<td style=\"text-align:left\">N</td>\n<td style=\"text-align:left\">N</td>\n<td style=\"text-align:left\">N</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">第2类算法</td>\n<td style=\"text-align:left\">Y</td>\n<td style=\"text-align:left\">Y</td>\n<td style=\"text-align:left\">N</td>\n<td style=\"text-align:left\">N</td>\n<td style=\"text-align:left\">N</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">第3类算法</td>\n<td style=\"text-align:left\">Y</td>\n<td style=\"text-align:left\">Y</td>\n<td style=\"text-align:left\">Y</td>\n<td style=\"text-align:left\">N</td>\n<td style=\"text-align:left\">N</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">mClock</td>\n<td style=\"text-align:left\">Y</td>\n<td style=\"text-align:left\">Y</td>\n<td style=\"text-align:left\">Y</td>\n<td style=\"text-align:left\">Y</td>\n<td style=\"text-align:left\">Y</td>\n</tr>\n</tbody>\n</table>\n<p>IO资源调度一般有3类相关的算法。第1类算法按比例分配IO资源，例如SFQ(D)，IO资源主要包括IOPS和带宽资源。第2类算法也是按比例分配IO资源，但IO资源包括延迟。第3类算法还是按比例分配IO资源，支持对给定的客户机提供最低资源保证。</p>\n<h2 id=\"一个例子\"><a href=\"#一个例子\" class=\"headerlink\" title=\"一个例子\"></a>一个例子</h2><table>\n<thead>\n<tr>\n<th style=\"text-align:left\">VM</th>\n<th style=\"text-align:left\">IOPS</th>\n<th style=\"text-align:left\">延时</th>\n<th style=\"text-align:left\">带宽</th>\n<th style=\"text-align:left\">权重(S)</th>\n<th style=\"text-align:left\">上限(L)</th>\n<th style=\"text-align:left\">预留(R)</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:left\">RD</td>\n<td style=\"text-align:left\">低</td>\n<td style=\"text-align:left\">低</td>\n<td style=\"text-align:left\">-</td>\n<td style=\"text-align:left\">100</td>\n<td style=\"text-align:left\">Inf</td>\n<td style=\"text-align:left\">250</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">OLTP</td>\n<td style=\"text-align:left\">高</td>\n<td style=\"text-align:left\">低</td>\n<td style=\"text-align:left\">-</td>\n<td style=\"text-align:left\">200</td>\n<td style=\"text-align:left\">Inf</td>\n<td style=\"text-align:left\">250</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">DM</td>\n<td style=\"text-align:left\">高</td>\n<td style=\"text-align:left\">-</td>\n<td style=\"text-align:left\">-</td>\n<td style=\"text-align:left\">300</td>\n<td style=\"text-align:left\">1000</td>\n<td style=\"text-align:left\">0</td>\n</tr>\n</tbody>\n</table>\n<p>上表给出3个VM，分别跑不同的业务。RD运行远程桌面，对IOPS要求不高但对延迟要求高，当IOPS较低时用户体验会很差因此保证预留为250个IOPS。OLTP要求高IOPS和低延迟，同样地预留为250个IOPS。DM运行数据迁移业务，设置上限为1000 IOPS，避免过度消耗系统带宽影响其它业务。mClock的目标是将VM的IO限制在预留和上限之间。</p>\n<p>当系统总IOPS为1200时，按<strong>权重比例</strong>分配IO，那么每个VM分配到的IOPS如下：</p>\n<table>\n<thead>\n<tr>\n<th style=\"text-align:left\">VM</th>\n<th style=\"text-align:left\">IOPS(按比例)</th>\n<th style=\"text-align:left\">IOPS(mclock)</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:left\">RD</td>\n<td style=\"text-align:left\">200</td>\n<td style=\"text-align:left\">250</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">OLTP</td>\n<td style=\"text-align:left\">400</td>\n<td style=\"text-align:left\">380</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">DM</td>\n<td style=\"text-align:left\">600</td>\n<td style=\"text-align:left\">570</td>\n</tr>\n</tbody>\n</table>\n<p>虽然此时系统拥有能够满足各个VM预留的IOPS（250 + 250 + 0 = 500），但RD只分配到了200IOPS低于其预留值。这种情况下，mclock的做法是：首先分配250 IOPS给RD，然后将剩余的950 IOPS按2:3的比例分配给OLTP和DM。<br>当系统总IOPS低于500时，无法满足每个VM的预留，因此mclock的做法是将可用的系统IO按照<strong>预留比例</strong>分配给VM，其中DM无法分配到IO。<br>当系统总IOPS在1500到2000时，直接按<strong>权重比例</strong>分配IO。此时不会出现VM的IO低于预留或者高于上限的情况。<br>当系统总IOPS高于2000时，直接按<strong>权重比例</strong>分配IO会导致DM的IOPS高于其上限。mclock的做法是：先为DM分配1000个IOPS，然后将剩余的IOPS按1:2的比例分配给RD和OLTP。</p>\n<p>总而言之，分配策略会根据总吞吐量和活动VM而动态变化。将VM划分为三类：预留(R)、上限(L)、权重(P)，系统总吞吐量为T，那么IO分配公式如下：</p>\n<p><img src=\"http://ohn764ue3.bkt.clouddn.com/DmClock/paper/Eq.png-name\" alt=\"\"></p>\n<p>mclock通过一种创新的tagging分配策略来达成上述公式。<br>上述公式存在的问题是，Tp可能小于0，也就是说系统总吞吐量要低于所有VM的总预留值。当出现这种情况时，mclock按<strong>预留比例</strong>来分配IO资源。这在某些情况下可能无法满足用户需求，例如有些VM没有分配预留值或者预留值为0，那么该VM可能永远无法分配到IO。不过这可以通过为每个VM都设置预留来解决。此外，针对这种情况也可考虑通过为预留设置优先级来解决（本文不再涉及）。</p>\n<h1 id=\"mClock算法\"><a href=\"#mClock算法\" class=\"headerlink\" title=\"mClock算法\"></a>mClock算法</h1><p>适用于单主机的情况。<br>mClock是种基于tag的算法，基于tag的算法的<strong>基本思路</strong>: 为每个请求赋予1个tag，scheduler按照tag从小到大的顺序依次处理请求。</p>\n<table>\n<thead>\n<tr>\n<th style=\"text-align:left\">VM</th>\n<th style=\"text-align:left\">1th</th>\n<th style=\"text-align:left\">2nd</th>\n<th style=\"text-align:left\">3rd</th>\n<th style=\"text-align:left\">4th</th>\n<th style=\"text-align:left\">5th</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:left\">A</td>\n<td style=\"text-align:left\"><strong>2</strong></td>\n<td style=\"text-align:left\"><strong>4</strong></td>\n<td style=\"text-align:left\"><strong>6</strong></td>\n<td style=\"text-align:left\">8</td>\n<td style=\"text-align:left\">10</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">B</td>\n<td style=\"text-align:left\"><strong>3</strong></td>\n<td style=\"text-align:left\"><strong>6</strong></td>\n<td style=\"text-align:left\">9</td>\n<td style=\"text-align:left\">12</td>\n<td style=\"text-align:left\">15</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">C</td>\n<td style=\"text-align:left\"><strong>6</strong></td>\n<td style=\"text-align:left\">12</td>\n<td style=\"text-align:left\">18</td>\n<td style=\"text-align:left\">24</td>\n<td style=\"text-align:left\">30</td>\n</tr>\n</tbody>\n</table>\n<p>举个例子，假设A、B、C三台VM的权重分别为1/2,1/3和1/6。<br>scheduler为每个请求设置tag的方法：在上个请求权重的基础上递增1/wi。假设主机分别接收到了5个来自各台VM的请求，每个请求的tag如上表所示。假设主机在给定的时间段内只能处理其中的6个请求，那么按照tag的顺序，被处理的请求为：A的前3个、B的前2个、C的前1个请求。A、B、C被处理的请求数目恰好是它们权重的比例3:2:1。</p>\n<p>这种tag赋值方法隐含着一个前提条件，那就是每个VM都是同时启动同时处于活动状态的。假如B一开始处于非活动状态，直到A接收到第5个请求时B才启动，这时B的连续若干个请求的tag值都会小于A，这将导致IO分配不符合预先配置的比例。为解决这个问题需要引入一个全局虚拟时钟（global virtual time），每个请求的tag尽量保持与gvt相同。</p>\n<p>mClock提出两个主要的概念：多时钟(multiple real-time clocks)和动态时钟选择(dynamic clock selection)。多时钟是指mClock分别为上限(L)、预留(R)和权重(P)提供独立的tag，每个VM的请求都包含3个不同的tag值。动态始终选择指scheduler动态选择1个tag来调度IO资源。</p>\n<p>mClock由三大组件组成：Tag赋值、Tag微调、请求调度。</p>\n<h2 id=\"Tag赋值-Tag-Assigment\"><a href=\"#Tag赋值-Tag-Assigment\" class=\"headerlink\" title=\"Tag赋值 Tag Assigment\"></a>Tag赋值 Tag Assigment</h2><p><img src=\"http://ohn764ue3.bkt.clouddn.com/DmClock/paper/Symbols.png-name\" alt=\"\"></p>\n<p>目的是为每个请求设置预留(R)、上限(L)和权重(P)三个Tag值。3个Tag的计算公式都相同，此处以预留为例。</p>\n<p><img src=\"http://ohn764ue3.bkt.clouddn.com/DmClock/paper/Eq_3.png-name\" alt=\"\"></p>\n<p>假设对给定VM1，设置其预留的IOPS为10。也就说，平均每隔1/10秒处理一个请求。将VM1的请求对应到R Tag时间轴上，相邻两个请求的时间间隔为1/10。假设Server从T1时刻开始接收来自VM1的请求，在T2时刻开始处理请求。T2时刻只要完成R Tag时间轴中Tag位于T2时刻之前的请求，就能够满足预留的要求了。这是因为，R Tag时间轴的[T1, T2)区间可以排列10*(T2-T1)个请求，完成10*(T2-T1)个请求刚好可以满足平均IOPS为10。如果这段时间内Server接收到超过10*(T2-T1)个请求，那么超过部分将排列到T2时刻后面，可暂不处理。如果这段时间内Server接收到的请求数目小于或等于10*(T2-T1)，那么这些请求都将排列在T2时刻前面，被处理掉。</p>\n<p>R Tag时间轴上的时间段[T1, T2)就像一个固定大小的容器，容器的大小便是用户配置的预留。这段时间内，如果有大量的IO请求，那么这些请求将填满容器，同时多出的部分将被放到后面的容器。T2时刻处理请求时，只需完成容器内部的请求就能够满足预留要求。</p>\n<p>根据公式，VM1的第一个请求的R Tag将被设置为T1，T1是Server接收到VM1第一个请求的时刻。为什么R Tag的计算公式要跟当前时间进行比较？考虑如下场景：VM1先在T1时刻启动，运行段时间后在T2时刻关闭，后又在T3时刻启动。那么，从T3时刻开始VM1的请求的R Tag将相对较小，因为它是基于T2时刻前接收到的最后一个请求为基准的，而Server是按照R Tag从小到大的顺序处理请求的, 也就说，VM1 T3时刻后的请求具有空闲优先，将被优先处理掉。这对那些一直处于活动状态的VM来说是不公平的。算法实现时，可以通过一个定时器来定时检查VM是否空闲。空闲的判断标准是，如果VM的最后一个请求的达到时间到当前时间的时间差大于给定的阈值时，认为VM空闲。这样，VM重新活动后的第一个请求将被设置为该请求的达到时间，以此来消除空闲优势。</p>\n<p>L Tag和R Tag比较类似，都可以理解为IOPS。L Tag的[T1, T2)容器的大小代表的是VM的上限。T2时刻处理请求时，若VM1剩余请求的R Tag都大于T2，那就不能继续处理VM1的请求了，否则，将导致VM1突破IOPS上限。</p>\n<p>P Tag不同于L和R Tag，P Tag是个<strong>相对值</strong>(比例值)不能理解为IOPS。它的主要作用是按照给定的比例将Server的IO能力分配给不同的VM。</p>\n<h2 id=\"Tag校准-Tag-Adjustment\"><a href=\"#Tag校准-Tag-Adjustment\" class=\"headerlink\" title=\"Tag校准 Tag Adjustment\"></a>Tag校准 Tag Adjustment</h2><p>上文提到的<strong>空闲优势</strong>是VM处于非活动状态带来的一个问题。除此之外，<strong>新VM</strong>（刚处于活动状态的VM）也会带来一些相对于<strong>老VM</strong>(一直处于活动状态的VM)来说不公平的调度问题。老VM运行一段时间后，尤其是一直处于繁忙状态的VM，将越来越偏离Server的当前时间。</p>\n<p><img src=\"http://ohn764ue3.bkt.clouddn.com/DmClock/paper/adjust_01.png-name\" alt=\"\"></p>\n<p>假设，如上图所示，VM_A在T1时刻启动，其首个请求的Tag设置为T1。VM_B在T2时刻启动，此时VM_A剩余请求中P Tag最小的请求的Tag为T3。如此，新启动的VM_B将获得相对于VM_A有(T3-T2)的<strong>后起优势</strong>。</p>\n<p><img src=\"http://ohn764ue3.bkt.clouddn.com/DmClock/paper/adjust_02.png-name\" alt=\"\"></p>\n<p>消除新VM带来的后起优势的问题也很简单。将老VM的剩余请求中最早的请求的P Tag调整到当前时间（新VM的启动时间），同时保证老VM的剩余请求的相对位置不变。简而言之，将VM_A的所有剩余请求的P Tag都减少(T3-T1)，如此调整后新VM和老VM将处于相同的起跑线。此例中只有两个VM，一个老VM，如果有多个老VM时，调整时还要保证老VM间的相对位置不变。因此，P Tag的差值应该是<strong>所有老VM</strong>中最小请求的P Tag和当前时间的差值。</p>\n<h2 id=\"请求调度-Request-Scheduling\"><a href=\"#请求调度-Request-Scheduling\" class=\"headerlink\" title=\"请求调度 Request Scheduling\"></a>请求调度 Request Scheduling</h2><p>mClock的请求调度分两个阶段，这两个阶段交替执行。第一个阶段称为<strong>Constraint-based</strong>阶段，目的是保证每个VM的预留。根据上文的解释，只要在R Tag时间轴上将所有低于当前时间的请求全部发送即可。第二个阶段称为<strong>Weight-based</strong>阶段，顾名思义，就是依据P Tag按比例处理不同VM的请求个数。同时，这个阶段还考虑VM的上限，对L Tag高于当前时间的请求不予处理。</p>\n<p><img src=\"http://ohn764ue3.bkt.clouddn.com/DmClock/paper/alg.png-name\" alt=\"\"></p>\n<p>Weight-based阶段的请求调度会影响到下一轮的Constraint-based阶段的请求调度。假设T1时刻开始第一轮请求调度，T2时刻开始第二轮请求调度，T1时刻VM1请求的P Tag已经排到T2后面了。如果第1轮请求调度的Weight-based阶段将VM1中R Tag位于[T1, T2)时间段的请求全部处理掉，那么第2轮请求调度的Constraint-based阶段VM1将无请求可调度，也就是说，在T1~T2时间段内VM1的平均IOPS为0。这不符合用户的预留要求。</p>\n<p>破解这个问题的方法是，在Weight-based阶段每处理一个VM1的请求时，就将VM1中所有剩余请求的R Tag递减1/r。如此，原来R Tag排在T2后面的请求在Weigt-based阶段完成后将被重排到到T1、T2间。第2轮请求调度的Constraint-based阶段将不会出现无请求可调度的情况。</p>\n<h2 id=\"特定于存储的问题-Storage-specific-Issue\"><a href=\"#特定于存储的问题-Storage-specific-Issue\" class=\"headerlink\" title=\"特定于存储的问题 Storage-specific Issue\"></a>特定于存储的问题 Storage-specific Issue</h2><p>mClock算法能够应用于多种资源的调度，例如CPU、内存和存储。在应用于存储资源调度时，有诸多特定于存储相关的问题。</p>\n<h3 id=\"突发请求-Burst-Handling\"><a href=\"#突发请求-Burst-Handling\" class=\"headerlink\" title=\"突发请求 Burst Handling\"></a>突发请求 Burst Handling</h3><p>目标，为突发请求设置一定的优先处理的好处。何为突发请求？VM空闲了一小段时间，突然又有新请求的情况。给予好处的方法很简单，对这种情况原先是直接将新请求的P Tag设置为当前时间，现在可以将其设置为比当前时间更早点的时间。时间越早，好处越大。注意，此处只修改P Tag，不会影响R Tag。</p>\n<h3 id=\"请求类型-Request-Type\"><a href=\"#请求类型-Request-Type\" class=\"headerlink\" title=\"请求类型 Request Type\"></a>请求类型 Request Type</h3><p>无差别对待读请求和写请求。不区分读写请求的原因是，在Hypervisor层如果调整了读写请求的顺序将出现一致性问题。但，如果读写顺序在应用层保证，那么在存储层或许可以调整读、写请求的相对顺序。实际上，很多商业存储都允许分别为读或写设置优先级。</p>\n<h3 id=\"IO大小-IO-Size\"><a href=\"#IO大小-IO-Size\" class=\"headerlink\" title=\"IO大小 IO Size\"></a>IO大小 IO Size</h3><p>P Tag是个相对值。默认情况，这个相对值指的是IOPS。因为，它只考虑了请求的个数，没有考虑每个请求的大小、延迟等其它信息。</p>\n<p>若P Tag指的是带宽，那请考虑请求大小。假设VM1、VM2的P值都为10，希望这两个VM的带宽是相同的。同时假设，VM1 请求的大小为1M，VM2请求的大小为2M。按照原来的Tag Assigment方法，每次请求调度VM1和VM2处理的请求数目应该都相同，这将导致VM2的带宽为VM1的两倍。解决方法是，以1M大小的请求为参考，将2M大小的请求间的间隔调整为1M请求的2倍。如此调整，Server原本每处理10个VM1的请求就也要同时处理10个VM2的请求，现在每处理10个VM1的请求的同时只需要处理5个VM2的请求，从而使得VM1和VM2的带宽相同。</p>\n<h3 id=\"请求局部性-Request-Location\"><a href=\"#请求局部性-Request-Location\" class=\"headerlink\" title=\"请求局部性 Request Location\"></a>请求局部性 Request Location</h3><p>IO请求具有一定的空间局部性，为提高整体的处理性能，mClock允许批量地处理一组来自同个VM的请求。只要这些请求的总大小接近逻辑块(logical block number space)大小，例如4MB。同时，设定一次批量处理中请求的数目，例如最多处理8个。</p>\n<p>上述的优化方法会影响IO的时延。举个例子，本来按照排序是第N个进行处理的请求，由于批量处理前面N个请求（属于其它VM）都各自夹带了8个相邻的请求，从而实际上将自己排到了第8N的位置。</p>\n<h1 id=\"dmClock算法\"><a href=\"#dmClock算法\" class=\"headerlink\" title=\"dmClock算法\"></a>dmClock算法</h1><p>不同于单主机的情况，给定VM的请求全部经由该主机进行处理，多主机的情况中给定VM的请求将分散给不同的主机进行处理。因此，在Tag赋值阶段，处理请求的主机并不知道VM的前一个请求的Tag值，只知道本主机处理的来自该VM的前个请求的Tag值。为解决这个问题，VM向目标主机发送请求时需要携带相关信息。目标主机根据这些信息来分配Tag的大小。</p>\n<p><img src=\"http://ohn764ue3.bkt.clouddn.com/DmClock/paper/Eq_5.png-name\" alt=\"\"></p>\n<p>对给定的VM，delta代表处理当前请求的存储主机从接收到前个请求到当前请求这段时间内，VM发送给其它主机的请求数目。同样地，rou代表对给定的VM，处理当前请求的目标主机从接收到前个请求到当前请求这段时间内，VM发送给其它主机并且在constraint-based阶段被处理的请求的个数。</p>\n<p><img src=\"http://ohn764ue3.bkt.clouddn.com/DmClock/paper/dmclock_01.png-name\" alt=\"\"></p>\n<p>举个例子，如上图所示，对Server3而言，只在T1、T2两个时间点接收到请求。在T2时刻时，delta值为Server1和Server2在T1、T2时间段中接收的请求数目，为8。rou为在T1、T2时间段内Server1和Server2中在constraint-based阶段处理掉的请求个数，为3。</p>\n<hr>\n<p><strong>版权声明</strong></p>\n<p>本作品采用<a href=\"https://creativecommons.org/licenses/by/2.5/cn/\" target=\"_blank\" rel=\"external\">知识共享署名2.5 中国大陆许可协议</a>进行许可，欢迎转载，但转载请注明来自<a href=\"https://wuxiangwei.cn/\"><em>Shanno’s Blog</em></a>，并保持转载后文章内容的完整。本人保留所有版权相关权利，侵权必究。</p>\n","excerpt":"<p><strong>术语</strong></p>\n<table>\n<thead>\n<tr>\n<th style=\"text-align:left\">英文</th>\n<th style=\"text-align:left\">中文</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:left\">share/weight</td>\n<td style=\"text-align:left\">权重</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">reservation</td>\n<td style=\"text-align:left\">预留或下限</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">limit</td>\n<td style=\"text-align:left\">上限</td>\n</tr>\n</tbody>\n</table>\n<p>存储IO调度和CPU、内存调度相比存在以下几个不同点：</p>\n<ul>\n<li>CPU、内存通常是主机内部的资源调度。但存储IO通常是跨主机的，多台主机使用共享的存储资源，主机内IO的变化会影响相邻主机的IO。</li>\n<li>Hypervisor中IO调度层位于客户机IO调度的底层。因此会滋生出许多问题，例如同台主机不同VM间的局部性、同时存在多种不同大小的IO、不同请求的优先级以及突发负载等。</li>\n</ul>","more":"<table>\n<thead>\n<tr>\n<th style=\"text-align:left\">算法类别</th>\n<th style=\"text-align:left\">按比例</th>\n<th style=\"text-align:left\">延时</th>\n<th style=\"text-align:left\">预留</th>\n<th style=\"text-align:left\">上限</th>\n<th style=\"text-align:left\">突发负载</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:left\">第1类算法</td>\n<td style=\"text-align:left\">Y</td>\n<td style=\"text-align:left\">N</td>\n<td style=\"text-align:left\">N</td>\n<td style=\"text-align:left\">N</td>\n<td style=\"text-align:left\">N</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">第2类算法</td>\n<td style=\"text-align:left\">Y</td>\n<td style=\"text-align:left\">Y</td>\n<td style=\"text-align:left\">N</td>\n<td style=\"text-align:left\">N</td>\n<td style=\"text-align:left\">N</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">第3类算法</td>\n<td style=\"text-align:left\">Y</td>\n<td style=\"text-align:left\">Y</td>\n<td style=\"text-align:left\">Y</td>\n<td style=\"text-align:left\">N</td>\n<td style=\"text-align:left\">N</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">mClock</td>\n<td style=\"text-align:left\">Y</td>\n<td style=\"text-align:left\">Y</td>\n<td style=\"text-align:left\">Y</td>\n<td style=\"text-align:left\">Y</td>\n<td style=\"text-align:left\">Y</td>\n</tr>\n</tbody>\n</table>\n<p>IO资源调度一般有3类相关的算法。第1类算法按比例分配IO资源，例如SFQ(D)，IO资源主要包括IOPS和带宽资源。第2类算法也是按比例分配IO资源，但IO资源包括延迟。第3类算法还是按比例分配IO资源，支持对给定的客户机提供最低资源保证。</p>\n<h2 id=\"一个例子\"><a href=\"#一个例子\" class=\"headerlink\" title=\"一个例子\"></a>一个例子</h2><table>\n<thead>\n<tr>\n<th style=\"text-align:left\">VM</th>\n<th style=\"text-align:left\">IOPS</th>\n<th style=\"text-align:left\">延时</th>\n<th style=\"text-align:left\">带宽</th>\n<th style=\"text-align:left\">权重(S)</th>\n<th style=\"text-align:left\">上限(L)</th>\n<th style=\"text-align:left\">预留(R)</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:left\">RD</td>\n<td style=\"text-align:left\">低</td>\n<td style=\"text-align:left\">低</td>\n<td style=\"text-align:left\">-</td>\n<td style=\"text-align:left\">100</td>\n<td style=\"text-align:left\">Inf</td>\n<td style=\"text-align:left\">250</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">OLTP</td>\n<td style=\"text-align:left\">高</td>\n<td style=\"text-align:left\">低</td>\n<td style=\"text-align:left\">-</td>\n<td style=\"text-align:left\">200</td>\n<td style=\"text-align:left\">Inf</td>\n<td style=\"text-align:left\">250</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">DM</td>\n<td style=\"text-align:left\">高</td>\n<td style=\"text-align:left\">-</td>\n<td style=\"text-align:left\">-</td>\n<td style=\"text-align:left\">300</td>\n<td style=\"text-align:left\">1000</td>\n<td style=\"text-align:left\">0</td>\n</tr>\n</tbody>\n</table>\n<p>上表给出3个VM，分别跑不同的业务。RD运行远程桌面，对IOPS要求不高但对延迟要求高，当IOPS较低时用户体验会很差因此保证预留为250个IOPS。OLTP要求高IOPS和低延迟，同样地预留为250个IOPS。DM运行数据迁移业务，设置上限为1000 IOPS，避免过度消耗系统带宽影响其它业务。mClock的目标是将VM的IO限制在预留和上限之间。</p>\n<p>当系统总IOPS为1200时，按<strong>权重比例</strong>分配IO，那么每个VM分配到的IOPS如下：</p>\n<table>\n<thead>\n<tr>\n<th style=\"text-align:left\">VM</th>\n<th style=\"text-align:left\">IOPS(按比例)</th>\n<th style=\"text-align:left\">IOPS(mclock)</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:left\">RD</td>\n<td style=\"text-align:left\">200</td>\n<td style=\"text-align:left\">250</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">OLTP</td>\n<td style=\"text-align:left\">400</td>\n<td style=\"text-align:left\">380</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">DM</td>\n<td style=\"text-align:left\">600</td>\n<td style=\"text-align:left\">570</td>\n</tr>\n</tbody>\n</table>\n<p>虽然此时系统拥有能够满足各个VM预留的IOPS（250 + 250 + 0 = 500），但RD只分配到了200IOPS低于其预留值。这种情况下，mclock的做法是：首先分配250 IOPS给RD，然后将剩余的950 IOPS按2:3的比例分配给OLTP和DM。<br>当系统总IOPS低于500时，无法满足每个VM的预留，因此mclock的做法是将可用的系统IO按照<strong>预留比例</strong>分配给VM，其中DM无法分配到IO。<br>当系统总IOPS在1500到2000时，直接按<strong>权重比例</strong>分配IO。此时不会出现VM的IO低于预留或者高于上限的情况。<br>当系统总IOPS高于2000时，直接按<strong>权重比例</strong>分配IO会导致DM的IOPS高于其上限。mclock的做法是：先为DM分配1000个IOPS，然后将剩余的IOPS按1:2的比例分配给RD和OLTP。</p>\n<p>总而言之，分配策略会根据总吞吐量和活动VM而动态变化。将VM划分为三类：预留(R)、上限(L)、权重(P)，系统总吞吐量为T，那么IO分配公式如下：</p>\n<p><img src=\"http://ohn764ue3.bkt.clouddn.com/DmClock/paper/Eq.png-name\" alt=\"\"></p>\n<p>mclock通过一种创新的tagging分配策略来达成上述公式。<br>上述公式存在的问题是，Tp可能小于0，也就是说系统总吞吐量要低于所有VM的总预留值。当出现这种情况时，mclock按<strong>预留比例</strong>来分配IO资源。这在某些情况下可能无法满足用户需求，例如有些VM没有分配预留值或者预留值为0，那么该VM可能永远无法分配到IO。不过这可以通过为每个VM都设置预留来解决。此外，针对这种情况也可考虑通过为预留设置优先级来解决（本文不再涉及）。</p>\n<h1 id=\"mClock算法\"><a href=\"#mClock算法\" class=\"headerlink\" title=\"mClock算法\"></a>mClock算法</h1><p>适用于单主机的情况。<br>mClock是种基于tag的算法，基于tag的算法的<strong>基本思路</strong>: 为每个请求赋予1个tag，scheduler按照tag从小到大的顺序依次处理请求。</p>\n<table>\n<thead>\n<tr>\n<th style=\"text-align:left\">VM</th>\n<th style=\"text-align:left\">1th</th>\n<th style=\"text-align:left\">2nd</th>\n<th style=\"text-align:left\">3rd</th>\n<th style=\"text-align:left\">4th</th>\n<th style=\"text-align:left\">5th</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:left\">A</td>\n<td style=\"text-align:left\"><strong>2</strong></td>\n<td style=\"text-align:left\"><strong>4</strong></td>\n<td style=\"text-align:left\"><strong>6</strong></td>\n<td style=\"text-align:left\">8</td>\n<td style=\"text-align:left\">10</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">B</td>\n<td style=\"text-align:left\"><strong>3</strong></td>\n<td style=\"text-align:left\"><strong>6</strong></td>\n<td style=\"text-align:left\">9</td>\n<td style=\"text-align:left\">12</td>\n<td style=\"text-align:left\">15</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">C</td>\n<td style=\"text-align:left\"><strong>6</strong></td>\n<td style=\"text-align:left\">12</td>\n<td style=\"text-align:left\">18</td>\n<td style=\"text-align:left\">24</td>\n<td style=\"text-align:left\">30</td>\n</tr>\n</tbody>\n</table>\n<p>举个例子，假设A、B、C三台VM的权重分别为1/2,1/3和1/6。<br>scheduler为每个请求设置tag的方法：在上个请求权重的基础上递增1/wi。假设主机分别接收到了5个来自各台VM的请求，每个请求的tag如上表所示。假设主机在给定的时间段内只能处理其中的6个请求，那么按照tag的顺序，被处理的请求为：A的前3个、B的前2个、C的前1个请求。A、B、C被处理的请求数目恰好是它们权重的比例3:2:1。</p>\n<p>这种tag赋值方法隐含着一个前提条件，那就是每个VM都是同时启动同时处于活动状态的。假如B一开始处于非活动状态，直到A接收到第5个请求时B才启动，这时B的连续若干个请求的tag值都会小于A，这将导致IO分配不符合预先配置的比例。为解决这个问题需要引入一个全局虚拟时钟（global virtual time），每个请求的tag尽量保持与gvt相同。</p>\n<p>mClock提出两个主要的概念：多时钟(multiple real-time clocks)和动态时钟选择(dynamic clock selection)。多时钟是指mClock分别为上限(L)、预留(R)和权重(P)提供独立的tag，每个VM的请求都包含3个不同的tag值。动态始终选择指scheduler动态选择1个tag来调度IO资源。</p>\n<p>mClock由三大组件组成：Tag赋值、Tag微调、请求调度。</p>\n<h2 id=\"Tag赋值-Tag-Assigment\"><a href=\"#Tag赋值-Tag-Assigment\" class=\"headerlink\" title=\"Tag赋值 Tag Assigment\"></a>Tag赋值 Tag Assigment</h2><p><img src=\"http://ohn764ue3.bkt.clouddn.com/DmClock/paper/Symbols.png-name\" alt=\"\"></p>\n<p>目的是为每个请求设置预留(R)、上限(L)和权重(P)三个Tag值。3个Tag的计算公式都相同，此处以预留为例。</p>\n<p><img src=\"http://ohn764ue3.bkt.clouddn.com/DmClock/paper/Eq_3.png-name\" alt=\"\"></p>\n<p>假设对给定VM1，设置其预留的IOPS为10。也就说，平均每隔1/10秒处理一个请求。将VM1的请求对应到R Tag时间轴上，相邻两个请求的时间间隔为1/10。假设Server从T1时刻开始接收来自VM1的请求，在T2时刻开始处理请求。T2时刻只要完成R Tag时间轴中Tag位于T2时刻之前的请求，就能够满足预留的要求了。这是因为，R Tag时间轴的[T1, T2)区间可以排列10*(T2-T1)个请求，完成10*(T2-T1)个请求刚好可以满足平均IOPS为10。如果这段时间内Server接收到超过10*(T2-T1)个请求，那么超过部分将排列到T2时刻后面，可暂不处理。如果这段时间内Server接收到的请求数目小于或等于10*(T2-T1)，那么这些请求都将排列在T2时刻前面，被处理掉。</p>\n<p>R Tag时间轴上的时间段[T1, T2)就像一个固定大小的容器，容器的大小便是用户配置的预留。这段时间内，如果有大量的IO请求，那么这些请求将填满容器，同时多出的部分将被放到后面的容器。T2时刻处理请求时，只需完成容器内部的请求就能够满足预留要求。</p>\n<p>根据公式，VM1的第一个请求的R Tag将被设置为T1，T1是Server接收到VM1第一个请求的时刻。为什么R Tag的计算公式要跟当前时间进行比较？考虑如下场景：VM1先在T1时刻启动，运行段时间后在T2时刻关闭，后又在T3时刻启动。那么，从T3时刻开始VM1的请求的R Tag将相对较小，因为它是基于T2时刻前接收到的最后一个请求为基准的，而Server是按照R Tag从小到大的顺序处理请求的, 也就说，VM1 T3时刻后的请求具有空闲优先，将被优先处理掉。这对那些一直处于活动状态的VM来说是不公平的。算法实现时，可以通过一个定时器来定时检查VM是否空闲。空闲的判断标准是，如果VM的最后一个请求的达到时间到当前时间的时间差大于给定的阈值时，认为VM空闲。这样，VM重新活动后的第一个请求将被设置为该请求的达到时间，以此来消除空闲优势。</p>\n<p>L Tag和R Tag比较类似，都可以理解为IOPS。L Tag的[T1, T2)容器的大小代表的是VM的上限。T2时刻处理请求时，若VM1剩余请求的R Tag都大于T2，那就不能继续处理VM1的请求了，否则，将导致VM1突破IOPS上限。</p>\n<p>P Tag不同于L和R Tag，P Tag是个<strong>相对值</strong>(比例值)不能理解为IOPS。它的主要作用是按照给定的比例将Server的IO能力分配给不同的VM。</p>\n<h2 id=\"Tag校准-Tag-Adjustment\"><a href=\"#Tag校准-Tag-Adjustment\" class=\"headerlink\" title=\"Tag校准 Tag Adjustment\"></a>Tag校准 Tag Adjustment</h2><p>上文提到的<strong>空闲优势</strong>是VM处于非活动状态带来的一个问题。除此之外，<strong>新VM</strong>（刚处于活动状态的VM）也会带来一些相对于<strong>老VM</strong>(一直处于活动状态的VM)来说不公平的调度问题。老VM运行一段时间后，尤其是一直处于繁忙状态的VM，将越来越偏离Server的当前时间。</p>\n<p><img src=\"http://ohn764ue3.bkt.clouddn.com/DmClock/paper/adjust_01.png-name\" alt=\"\"></p>\n<p>假设，如上图所示，VM_A在T1时刻启动，其首个请求的Tag设置为T1。VM_B在T2时刻启动，此时VM_A剩余请求中P Tag最小的请求的Tag为T3。如此，新启动的VM_B将获得相对于VM_A有(T3-T2)的<strong>后起优势</strong>。</p>\n<p><img src=\"http://ohn764ue3.bkt.clouddn.com/DmClock/paper/adjust_02.png-name\" alt=\"\"></p>\n<p>消除新VM带来的后起优势的问题也很简单。将老VM的剩余请求中最早的请求的P Tag调整到当前时间（新VM的启动时间），同时保证老VM的剩余请求的相对位置不变。简而言之，将VM_A的所有剩余请求的P Tag都减少(T3-T1)，如此调整后新VM和老VM将处于相同的起跑线。此例中只有两个VM，一个老VM，如果有多个老VM时，调整时还要保证老VM间的相对位置不变。因此，P Tag的差值应该是<strong>所有老VM</strong>中最小请求的P Tag和当前时间的差值。</p>\n<h2 id=\"请求调度-Request-Scheduling\"><a href=\"#请求调度-Request-Scheduling\" class=\"headerlink\" title=\"请求调度 Request Scheduling\"></a>请求调度 Request Scheduling</h2><p>mClock的请求调度分两个阶段，这两个阶段交替执行。第一个阶段称为<strong>Constraint-based</strong>阶段，目的是保证每个VM的预留。根据上文的解释，只要在R Tag时间轴上将所有低于当前时间的请求全部发送即可。第二个阶段称为<strong>Weight-based</strong>阶段，顾名思义，就是依据P Tag按比例处理不同VM的请求个数。同时，这个阶段还考虑VM的上限，对L Tag高于当前时间的请求不予处理。</p>\n<p><img src=\"http://ohn764ue3.bkt.clouddn.com/DmClock/paper/alg.png-name\" alt=\"\"></p>\n<p>Weight-based阶段的请求调度会影响到下一轮的Constraint-based阶段的请求调度。假设T1时刻开始第一轮请求调度，T2时刻开始第二轮请求调度，T1时刻VM1请求的P Tag已经排到T2后面了。如果第1轮请求调度的Weight-based阶段将VM1中R Tag位于[T1, T2)时间段的请求全部处理掉，那么第2轮请求调度的Constraint-based阶段VM1将无请求可调度，也就是说，在T1~T2时间段内VM1的平均IOPS为0。这不符合用户的预留要求。</p>\n<p>破解这个问题的方法是，在Weight-based阶段每处理一个VM1的请求时，就将VM1中所有剩余请求的R Tag递减1/r。如此，原来R Tag排在T2后面的请求在Weigt-based阶段完成后将被重排到到T1、T2间。第2轮请求调度的Constraint-based阶段将不会出现无请求可调度的情况。</p>\n<h2 id=\"特定于存储的问题-Storage-specific-Issue\"><a href=\"#特定于存储的问题-Storage-specific-Issue\" class=\"headerlink\" title=\"特定于存储的问题 Storage-specific Issue\"></a>特定于存储的问题 Storage-specific Issue</h2><p>mClock算法能够应用于多种资源的调度，例如CPU、内存和存储。在应用于存储资源调度时，有诸多特定于存储相关的问题。</p>\n<h3 id=\"突发请求-Burst-Handling\"><a href=\"#突发请求-Burst-Handling\" class=\"headerlink\" title=\"突发请求 Burst Handling\"></a>突发请求 Burst Handling</h3><p>目标，为突发请求设置一定的优先处理的好处。何为突发请求？VM空闲了一小段时间，突然又有新请求的情况。给予好处的方法很简单，对这种情况原先是直接将新请求的P Tag设置为当前时间，现在可以将其设置为比当前时间更早点的时间。时间越早，好处越大。注意，此处只修改P Tag，不会影响R Tag。</p>\n<h3 id=\"请求类型-Request-Type\"><a href=\"#请求类型-Request-Type\" class=\"headerlink\" title=\"请求类型 Request Type\"></a>请求类型 Request Type</h3><p>无差别对待读请求和写请求。不区分读写请求的原因是，在Hypervisor层如果调整了读写请求的顺序将出现一致性问题。但，如果读写顺序在应用层保证，那么在存储层或许可以调整读、写请求的相对顺序。实际上，很多商业存储都允许分别为读或写设置优先级。</p>\n<h3 id=\"IO大小-IO-Size\"><a href=\"#IO大小-IO-Size\" class=\"headerlink\" title=\"IO大小 IO Size\"></a>IO大小 IO Size</h3><p>P Tag是个相对值。默认情况，这个相对值指的是IOPS。因为，它只考虑了请求的个数，没有考虑每个请求的大小、延迟等其它信息。</p>\n<p>若P Tag指的是带宽，那请考虑请求大小。假设VM1、VM2的P值都为10，希望这两个VM的带宽是相同的。同时假设，VM1 请求的大小为1M，VM2请求的大小为2M。按照原来的Tag Assigment方法，每次请求调度VM1和VM2处理的请求数目应该都相同，这将导致VM2的带宽为VM1的两倍。解决方法是，以1M大小的请求为参考，将2M大小的请求间的间隔调整为1M请求的2倍。如此调整，Server原本每处理10个VM1的请求就也要同时处理10个VM2的请求，现在每处理10个VM1的请求的同时只需要处理5个VM2的请求，从而使得VM1和VM2的带宽相同。</p>\n<h3 id=\"请求局部性-Request-Location\"><a href=\"#请求局部性-Request-Location\" class=\"headerlink\" title=\"请求局部性 Request Location\"></a>请求局部性 Request Location</h3><p>IO请求具有一定的空间局部性，为提高整体的处理性能，mClock允许批量地处理一组来自同个VM的请求。只要这些请求的总大小接近逻辑块(logical block number space)大小，例如4MB。同时，设定一次批量处理中请求的数目，例如最多处理8个。</p>\n<p>上述的优化方法会影响IO的时延。举个例子，本来按照排序是第N个进行处理的请求，由于批量处理前面N个请求（属于其它VM）都各自夹带了8个相邻的请求，从而实际上将自己排到了第8N的位置。</p>\n<h1 id=\"dmClock算法\"><a href=\"#dmClock算法\" class=\"headerlink\" title=\"dmClock算法\"></a>dmClock算法</h1><p>不同于单主机的情况，给定VM的请求全部经由该主机进行处理，多主机的情况中给定VM的请求将分散给不同的主机进行处理。因此，在Tag赋值阶段，处理请求的主机并不知道VM的前一个请求的Tag值，只知道本主机处理的来自该VM的前个请求的Tag值。为解决这个问题，VM向目标主机发送请求时需要携带相关信息。目标主机根据这些信息来分配Tag的大小。</p>\n<p><img src=\"http://ohn764ue3.bkt.clouddn.com/DmClock/paper/Eq_5.png-name\" alt=\"\"></p>\n<p>对给定的VM，delta代表处理当前请求的存储主机从接收到前个请求到当前请求这段时间内，VM发送给其它主机的请求数目。同样地，rou代表对给定的VM，处理当前请求的目标主机从接收到前个请求到当前请求这段时间内，VM发送给其它主机并且在constraint-based阶段被处理的请求的个数。</p>\n<p><img src=\"http://ohn764ue3.bkt.clouddn.com/DmClock/paper/dmclock_01.png-name\" alt=\"\"></p>\n<p>举个例子，如上图所示，对Server3而言，只在T1、T2两个时间点接收到请求。在T2时刻时，delta值为Server1和Server2在T1、T2时间段中接收的请求数目，为8。rou为在T1、T2时间段内Server1和Server2中在constraint-based阶段处理掉的请求个数，为3。</p>\n<hr>\n<p><strong>版权声明</strong></p>\n<p>本作品采用<a href=\"https://creativecommons.org/licenses/by/2.5/cn/\">知识共享署名2.5 中国大陆许可协议</a>进行许可，欢迎转载，但转载请注明来自<a href=\"https://wuxiangwei.cn/\"><em>Shanno’s Blog</em></a>，并保持转载后文章内容的完整。本人保留所有版权相关权利，侵权必究。</p>"},{"title":"dmClock源码分析之Sim模块","date":"2016-11-11T07:01:26.000Z","toc":true,"_content":"\nSim模块构建一个模拟器来模拟一组客户机向一组服务主机发送请求，同时服务主机根据指定的QoS算法调度请求的处理顺序的过程。Sim提供了两种调度算法，一种是FCFS的简单调度算法，另一种就是dmClock调度算法。\n\n<!--more-->\n\n``` C++\n// TS代表服务，例如 SimulatedServer\n// TC代表客户，例如 SimulatedClient\ntemplate<typename ServerId, typename ClientId, typename TS, typename TC>\nclass Simulation {\npublic:\n    ServerMap servers; // 服务列表\n    ClientMap clients; // 客户列表\n\n    TimePoint early_time;  // 实例化时的时间\n    TimePoint late_time;  // 客户结束的时间\n\n    void display_stats();  // 打印统计信息\n};\n```\n\n打印客户报告：\n\n依赖于每个client的op_times属性，该属性记录每个请求完成时的时间。\n打印报告时，对每个client每隔measure_unit(2s)秒打印一次IOPS数据。此外，调整report_unit参数能够等比例放大或者缩小IOPS的值便于查看分析结果。\n\n打印服务报告：\n\n打印每个服务处理的请求数目。\n\n## 客户端 \n\n``` C++\nclass SimulatedClient {\nprotected:\n    std::vector<CliInst> instructions;\n\n    std::thread thd_req;\n    std::thread thd_resp;\n\n    // 性能数据收集\n    std::vector<TimePoint> op_times;  // 请求完成的时间，从小到大排序\n\n    void run_req();  // 发送请求的线程入口\n    void run_resp();  // 接收响应的线程入口\n    \n    // 接收来自服务的响应\n    void receive_response(\n        const TestResponse& resp,\n\t    const ServerId& server_id,\n\t    const RespPm& resp_params\n    );\n};\n```\n\nCliInst类描述生成请求的指令（instruction），两种生成请求的方式：一种是生成count(1000)个请求，同时设定最多允许max_outstanding(100)个等待请求以及每个请求的时延time_bw_reqs(单位：微秒)；另一种只是等待。何为等待请求？已发送但未接收到响应的请求。\n\n发送请求的流程：\n\n1. 检查等待请求数目是否大于上限值max_outstanding，如果大于，则一直等待不发送请求。\n2. 开始发送请求：\n    a) 为给定的请求o选择一个处理请求的Server；\n    b) 发送请求给a)中选定的Server；\n    c) 递增等待的请求数目 outstanding_ops；\n    d) 等待time_bw_reqs时间后，继续发生下个请求，即继续a)步骤。\n3. 所有请求发送结束，设置标志位requests_complete为True。\n\n\n处理响应的流程：\n\n1. 检查响应队列是否为空，若为空，则等待1s后继续检查，若不为空，则继续步骤2；\n2. 从响应队列中pop出一个响应，递减等待请求个数 outstanding_ops；\n3. 如果还有没发送的请求，则通知发送流程继续发送请求；\n4. 如果所有的请求还没发送结束，则跳转到步骤1继续执行；\n4. 如果所有的请求已经发送结束，但还存在等待请求，则跳转到步骤2)继续执行；\n\n## 服务端\n\n``` C++\n// Q代表优先级队列的类型\ntemplate<typename Q, typename ReqPm, typename RespPm, typename Accum>\nclass SimulatedServer {\n\tQ* priority_queue;  // 请求队列\n    // 内部请求队列\n    // 数据从priority_queue队列导入，导入策略由priority_queue队列定义\n    std::deque<QueueItem> inner_queue; \n    // priority_queue向inner_queue投递请求\n    void inner_post(\n        const ClientId& client,\n        std::unique_ptr<TestRequest> request,\n        const RespPm& additional\n    );\n\n\tClientRespFunc\tclient_resp_f;  // 返回响应给客户\n\tint iops;  // Server的IOPS能力\n    size_t thread_pool_size;  // 线程池的大小\n    std::chrono::microseconds op_time;  // 处理1个IO花费的时间，用于模拟IO处理\n    size_t thread_pool_size;\n    ServerAccumFunc accum_f;\n\n    std::chrono::microseconds op_time;  // 完成1个请求花费的时间\n\n    // 接收客户请求\n    void post(\n        const TestRequest& request,  // 请求内容\n        const ClientId& client_id,  // 客户id\n        const ReqPm& req_params  // 请求参数\n    );\n};\n```\n\nServer属于自己的属性：IOPS(默认值 40)、线程数。IOPS代表Server每秒处理IO数目的能力。根据这个属性可以推算出处理一个IO花费的时间，即op_time属性。在IOPS能力给定的情况下，如果使用多线程，那么op_time的值为单线程的thread_pool_size倍。thread_pool_size为线程池的大小。\n\n请求处理流程：\n\n1. 检查内部队列是否为空。若为空，则等待1s后继续检查；\n2. 从内部队列中pop一个请求，开始处理请求，即等待op_time时间；\n3. 发送响应给客户。\n\nSimulatedServer内部维护了两个队列：内部队列inner_queue和优先级队列priority_queue。内部队列是FCFS队列，Server处理请求时直接从该队列读取请求。priority_queue队列用于定义QoS调度算法，客户的请求将先入priority_queue队列，然后由priority_queue队列根据自己的算法将请求放入内部队列。\n\n## 简单调度 ssched\n\n``` shell\n./ssched_sim\n```\n启动模拟器。\n\n``` C++\n// 服务端参数\nconst uint server_count = 100;  // 服务数目\nconst uint server_iops = 40;  // 每个服务的IOPS能力\nconst uint server_threads = 1;  // 每个服务的线程池数目\n\n// 客户端参数\nconst uint client_total_ops = 1000;  // 每个客户发送的请求数目\nconst uint client_count = 100;  // 客户数目\nconst uint client_server_select_range = 10;\nconst uint client_wait_count = 1;  // 延迟启动的客户的数目\nconst uint client_iops_goal = 50;  // 每个客户期望的IOPS\nconst uint client_outstanding_ops = 100;  // 每个客户允许的等待请求\nconst std::chrono::seconds client_wait(10);  // 延迟启动的客户的延迟时间\n```\n\nSimpleServer的优先级队列是SimpleQueue类，SimpleQueue类调度请求的顺序是先进先出。具体实现参考该类的schedule_request()方法。\n\n## mClock调度 dmc\n\n``` shell\n./dmc_sim --conf dmc_sim_100th.conf\n```\n启动模拟器，dmc_sim_100th.conf为配置文件。\n\n```\n[global]                                                                                                                                 \nserver_groups = 1   # 1组服务\nclient_groups = 2   # 2组客户，分别定义在后面的client.0和client.1两个section内\nserver_random_selection = true\nserver_soft_limit = true\n\n[client.0]                                                                                          \nclient_count = 99  # 客户数目\nclient_wait = 0  # 客户延迟启动的时间                                                                       \nclient_total_ops = 2000  # 每个客户发送的请求数目\nclient_server_select_range = 10\nclient_iops_goal = 50  # 每个客户发送请求的IOPS能力，也就是说，如果服务能力充足，那么客户最大的IOPS为50\nclient_outstanding_ops = 100  # 最大等待请求的数目\nclient_reservation = 5.0  # dmClock中的下限\nclient_limit = 60.0  # dmClock中的上限                                                         \nclient_weight = 1.0  # dmClock中的权重                                                              \n\n[client.1]\nclient_count = 1\nclient_wait = 10  # 延迟10s后启动客户机\nclient_total_ops = 2000\nclient_server_select_range = 10                                                                     \nclient_iops_goal = 300\nclient_outstanding_ops = 100                                                                        \nclient_reservation = 20.0                                                                           \nclient_limit = 200.0                                                                                \nclient_weight = 10.0                                                                                \n\n[server.0]\nserver_count = 100  # 服务的数目\nserver_iops = 40  # 每个服务处理IO的能力，最大为40，总共为 100 * 40 = 4000\nserver_threads = 1  # 每个服务的线程池大小\n```\n配置文件中各个字段的说明，另外重点说明下client.1客户机的配置：\n- 延迟启动(client_wait配置项)。延迟启动的目的是为了比较client.0客户机在有新客户机启动时IOPS的变化，一般来说都会开始下降；\n- 权重大(client_weight配置项) 。client.1只有一个客户机，客户机启动后能够比较出该客户机同其它客户机在IOPS上的差异，一般来说会大点；\n- 客户机能力强(client_iops_goal配置项)。在设置较大权重，但客户机的IOPS还不够高的情况下，可以调整client_iops_goal的值，增加客户机发送请求的能力。\n\n\n``` C++\nstruct ClientInfo {\n    const double reservation;  // 下限，默认20.0\n    const double limit;  // 上限，默认 60.0\n    const double weight;  // 权重，默认 1.0\n    const double reservation_inv; // 下限的倒数\n    const double limit_inv; // 上限的倒数\n    const double weight_inv; //  权重的倒数\n};\n```\n","source":"_posts/DmClock/MCLOCK_sim.md","raw":"---\ntitle: \"dmClock源码分析之Sim模块\"\ndate: 2016-11-11 15:01:26\ncategories: [mClock]\ntags: [mClock]\ntoc: true\n---\n\nSim模块构建一个模拟器来模拟一组客户机向一组服务主机发送请求，同时服务主机根据指定的QoS算法调度请求的处理顺序的过程。Sim提供了两种调度算法，一种是FCFS的简单调度算法，另一种就是dmClock调度算法。\n\n<!--more-->\n\n``` C++\n// TS代表服务，例如 SimulatedServer\n// TC代表客户，例如 SimulatedClient\ntemplate<typename ServerId, typename ClientId, typename TS, typename TC>\nclass Simulation {\npublic:\n    ServerMap servers; // 服务列表\n    ClientMap clients; // 客户列表\n\n    TimePoint early_time;  // 实例化时的时间\n    TimePoint late_time;  // 客户结束的时间\n\n    void display_stats();  // 打印统计信息\n};\n```\n\n打印客户报告：\n\n依赖于每个client的op_times属性，该属性记录每个请求完成时的时间。\n打印报告时，对每个client每隔measure_unit(2s)秒打印一次IOPS数据。此外，调整report_unit参数能够等比例放大或者缩小IOPS的值便于查看分析结果。\n\n打印服务报告：\n\n打印每个服务处理的请求数目。\n\n## 客户端 \n\n``` C++\nclass SimulatedClient {\nprotected:\n    std::vector<CliInst> instructions;\n\n    std::thread thd_req;\n    std::thread thd_resp;\n\n    // 性能数据收集\n    std::vector<TimePoint> op_times;  // 请求完成的时间，从小到大排序\n\n    void run_req();  // 发送请求的线程入口\n    void run_resp();  // 接收响应的线程入口\n    \n    // 接收来自服务的响应\n    void receive_response(\n        const TestResponse& resp,\n\t    const ServerId& server_id,\n\t    const RespPm& resp_params\n    );\n};\n```\n\nCliInst类描述生成请求的指令（instruction），两种生成请求的方式：一种是生成count(1000)个请求，同时设定最多允许max_outstanding(100)个等待请求以及每个请求的时延time_bw_reqs(单位：微秒)；另一种只是等待。何为等待请求？已发送但未接收到响应的请求。\n\n发送请求的流程：\n\n1. 检查等待请求数目是否大于上限值max_outstanding，如果大于，则一直等待不发送请求。\n2. 开始发送请求：\n    a) 为给定的请求o选择一个处理请求的Server；\n    b) 发送请求给a)中选定的Server；\n    c) 递增等待的请求数目 outstanding_ops；\n    d) 等待time_bw_reqs时间后，继续发生下个请求，即继续a)步骤。\n3. 所有请求发送结束，设置标志位requests_complete为True。\n\n\n处理响应的流程：\n\n1. 检查响应队列是否为空，若为空，则等待1s后继续检查，若不为空，则继续步骤2；\n2. 从响应队列中pop出一个响应，递减等待请求个数 outstanding_ops；\n3. 如果还有没发送的请求，则通知发送流程继续发送请求；\n4. 如果所有的请求还没发送结束，则跳转到步骤1继续执行；\n4. 如果所有的请求已经发送结束，但还存在等待请求，则跳转到步骤2)继续执行；\n\n## 服务端\n\n``` C++\n// Q代表优先级队列的类型\ntemplate<typename Q, typename ReqPm, typename RespPm, typename Accum>\nclass SimulatedServer {\n\tQ* priority_queue;  // 请求队列\n    // 内部请求队列\n    // 数据从priority_queue队列导入，导入策略由priority_queue队列定义\n    std::deque<QueueItem> inner_queue; \n    // priority_queue向inner_queue投递请求\n    void inner_post(\n        const ClientId& client,\n        std::unique_ptr<TestRequest> request,\n        const RespPm& additional\n    );\n\n\tClientRespFunc\tclient_resp_f;  // 返回响应给客户\n\tint iops;  // Server的IOPS能力\n    size_t thread_pool_size;  // 线程池的大小\n    std::chrono::microseconds op_time;  // 处理1个IO花费的时间，用于模拟IO处理\n    size_t thread_pool_size;\n    ServerAccumFunc accum_f;\n\n    std::chrono::microseconds op_time;  // 完成1个请求花费的时间\n\n    // 接收客户请求\n    void post(\n        const TestRequest& request,  // 请求内容\n        const ClientId& client_id,  // 客户id\n        const ReqPm& req_params  // 请求参数\n    );\n};\n```\n\nServer属于自己的属性：IOPS(默认值 40)、线程数。IOPS代表Server每秒处理IO数目的能力。根据这个属性可以推算出处理一个IO花费的时间，即op_time属性。在IOPS能力给定的情况下，如果使用多线程，那么op_time的值为单线程的thread_pool_size倍。thread_pool_size为线程池的大小。\n\n请求处理流程：\n\n1. 检查内部队列是否为空。若为空，则等待1s后继续检查；\n2. 从内部队列中pop一个请求，开始处理请求，即等待op_time时间；\n3. 发送响应给客户。\n\nSimulatedServer内部维护了两个队列：内部队列inner_queue和优先级队列priority_queue。内部队列是FCFS队列，Server处理请求时直接从该队列读取请求。priority_queue队列用于定义QoS调度算法，客户的请求将先入priority_queue队列，然后由priority_queue队列根据自己的算法将请求放入内部队列。\n\n## 简单调度 ssched\n\n``` shell\n./ssched_sim\n```\n启动模拟器。\n\n``` C++\n// 服务端参数\nconst uint server_count = 100;  // 服务数目\nconst uint server_iops = 40;  // 每个服务的IOPS能力\nconst uint server_threads = 1;  // 每个服务的线程池数目\n\n// 客户端参数\nconst uint client_total_ops = 1000;  // 每个客户发送的请求数目\nconst uint client_count = 100;  // 客户数目\nconst uint client_server_select_range = 10;\nconst uint client_wait_count = 1;  // 延迟启动的客户的数目\nconst uint client_iops_goal = 50;  // 每个客户期望的IOPS\nconst uint client_outstanding_ops = 100;  // 每个客户允许的等待请求\nconst std::chrono::seconds client_wait(10);  // 延迟启动的客户的延迟时间\n```\n\nSimpleServer的优先级队列是SimpleQueue类，SimpleQueue类调度请求的顺序是先进先出。具体实现参考该类的schedule_request()方法。\n\n## mClock调度 dmc\n\n``` shell\n./dmc_sim --conf dmc_sim_100th.conf\n```\n启动模拟器，dmc_sim_100th.conf为配置文件。\n\n```\n[global]                                                                                                                                 \nserver_groups = 1   # 1组服务\nclient_groups = 2   # 2组客户，分别定义在后面的client.0和client.1两个section内\nserver_random_selection = true\nserver_soft_limit = true\n\n[client.0]                                                                                          \nclient_count = 99  # 客户数目\nclient_wait = 0  # 客户延迟启动的时间                                                                       \nclient_total_ops = 2000  # 每个客户发送的请求数目\nclient_server_select_range = 10\nclient_iops_goal = 50  # 每个客户发送请求的IOPS能力，也就是说，如果服务能力充足，那么客户最大的IOPS为50\nclient_outstanding_ops = 100  # 最大等待请求的数目\nclient_reservation = 5.0  # dmClock中的下限\nclient_limit = 60.0  # dmClock中的上限                                                         \nclient_weight = 1.0  # dmClock中的权重                                                              \n\n[client.1]\nclient_count = 1\nclient_wait = 10  # 延迟10s后启动客户机\nclient_total_ops = 2000\nclient_server_select_range = 10                                                                     \nclient_iops_goal = 300\nclient_outstanding_ops = 100                                                                        \nclient_reservation = 20.0                                                                           \nclient_limit = 200.0                                                                                \nclient_weight = 10.0                                                                                \n\n[server.0]\nserver_count = 100  # 服务的数目\nserver_iops = 40  # 每个服务处理IO的能力，最大为40，总共为 100 * 40 = 4000\nserver_threads = 1  # 每个服务的线程池大小\n```\n配置文件中各个字段的说明，另外重点说明下client.1客户机的配置：\n- 延迟启动(client_wait配置项)。延迟启动的目的是为了比较client.0客户机在有新客户机启动时IOPS的变化，一般来说都会开始下降；\n- 权重大(client_weight配置项) 。client.1只有一个客户机，客户机启动后能够比较出该客户机同其它客户机在IOPS上的差异，一般来说会大点；\n- 客户机能力强(client_iops_goal配置项)。在设置较大权重，但客户机的IOPS还不够高的情况下，可以调整client_iops_goal的值，增加客户机发送请求的能力。\n\n\n``` C++\nstruct ClientInfo {\n    const double reservation;  // 下限，默认20.0\n    const double limit;  // 上限，默认 60.0\n    const double weight;  // 权重，默认 1.0\n    const double reservation_inv; // 下限的倒数\n    const double limit_inv; // 上限的倒数\n    const double weight_inv; //  权重的倒数\n};\n```\n","slug":"DmClock/MCLOCK_sim","published":1,"updated":"2016-12-15T07:18:40.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cix1kgaue0012daeimkjmfmbr","content":"<p>Sim模块构建一个模拟器来模拟一组客户机向一组服务主机发送请求，同时服务主机根据指定的QoS算法调度请求的处理顺序的过程。Sim提供了两种调度算法，一种是FCFS的简单调度算法，另一种就是dmClock调度算法。</p>\n<a id=\"more\"></a>\n<figure class=\"highlight c++\"><table><tr><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\">// TS代表服务，例如 SimulatedServer</span></div><div class=\"line\"><span class=\"comment\">// TC代表客户，例如 SimulatedClient</span></div><div class=\"line\"><span class=\"keyword\">template</span>&lt;<span class=\"keyword\">typename</span> ServerId, <span class=\"keyword\">typename</span> ClientId, <span class=\"keyword\">typename</span> TS, <span class=\"keyword\">typename</span> TC&gt;</div><div class=\"line\"><span class=\"keyword\">class</span> Simulation &#123;</div><div class=\"line\"><span class=\"keyword\">public</span>:</div><div class=\"line\">    ServerMap servers; <span class=\"comment\">// 服务列表</span></div><div class=\"line\">    ClientMap clients; <span class=\"comment\">// 客户列表</span></div><div class=\"line\"></div><div class=\"line\">    TimePoint early_time;  <span class=\"comment\">// 实例化时的时间</span></div><div class=\"line\">    TimePoint late_time;  <span class=\"comment\">// 客户结束的时间</span></div><div class=\"line\"></div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">void</span> <span class=\"title\">display_stats</span><span class=\"params\">()</span></span>;  <span class=\"comment\">// 打印统计信息</span></div><div class=\"line\">&#125;;</div></pre></td></tr></table></figure>\n<p>打印客户报告：</p>\n<p>依赖于每个client的op_times属性，该属性记录每个请求完成时的时间。<br>打印报告时，对每个client每隔measure_unit(2s)秒打印一次IOPS数据。此外，调整report_unit参数能够等比例放大或者缩小IOPS的值便于查看分析结果。</p>\n<p>打印服务报告：</p>\n<p>打印每个服务处理的请求数目。</p>\n<h2 id=\"客户端\"><a href=\"#客户端\" class=\"headerlink\" title=\"客户端\"></a>客户端</h2><figure class=\"highlight c++\"><table><tr><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">class</span> SimulatedClient &#123;</div><div class=\"line\"><span class=\"keyword\">protected</span>:</div><div class=\"line\">    <span class=\"built_in\">std</span>::<span class=\"built_in\">vector</span>&lt;CliInst&gt; instructions;</div><div class=\"line\"></div><div class=\"line\">    <span class=\"built_in\">std</span>::thread thd_req;</div><div class=\"line\">    <span class=\"built_in\">std</span>::thread thd_resp;</div><div class=\"line\"></div><div class=\"line\">    <span class=\"comment\">// 性能数据收集</span></div><div class=\"line\">    <span class=\"built_in\">std</span>::<span class=\"built_in\">vector</span>&lt;TimePoint&gt; op_times;  <span class=\"comment\">// 请求完成的时间，从小到大排序</span></div><div class=\"line\"></div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">void</span> <span class=\"title\">run_req</span><span class=\"params\">()</span></span>;  <span class=\"comment\">// 发送请求的线程入口</span></div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">void</span> <span class=\"title\">run_resp</span><span class=\"params\">()</span></span>;  <span class=\"comment\">// 接收响应的线程入口</span></div><div class=\"line\">    </div><div class=\"line\">    <span class=\"comment\">// 接收来自服务的响应</span></div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">void</span> <span class=\"title\">receive_response</span><span class=\"params\">(</span></span></div><div class=\"line\">        <span class=\"keyword\">const</span> TestResponse&amp; resp,</div><div class=\"line\">\t    <span class=\"keyword\">const</span> ServerId&amp; server_id,</div><div class=\"line\">\t    <span class=\"keyword\">const</span> RespPm&amp; resp_params</div><div class=\"line\">    );</div><div class=\"line\">&#125;;</div></pre></td></tr></table></figure>\n<p>CliInst类描述生成请求的指令（instruction），两种生成请求的方式：一种是生成count(1000)个请求，同时设定最多允许max_outstanding(100)个等待请求以及每个请求的时延time_bw_reqs(单位：微秒)；另一种只是等待。何为等待请求？已发送但未接收到响应的请求。</p>\n<p>发送请求的流程：</p>\n<ol>\n<li>检查等待请求数目是否大于上限值max_outstanding，如果大于，则一直等待不发送请求。</li>\n<li>开始发送请求：<br> a) 为给定的请求o选择一个处理请求的Server；<br> b) 发送请求给a)中选定的Server；<br> c) 递增等待的请求数目 outstanding_ops；<br> d) 等待time_bw_reqs时间后，继续发生下个请求，即继续a)步骤。</li>\n<li>所有请求发送结束，设置标志位requests_complete为True。</li>\n</ol>\n<p>处理响应的流程：</p>\n<ol>\n<li>检查响应队列是否为空，若为空，则等待1s后继续检查，若不为空，则继续步骤2；</li>\n<li>从响应队列中pop出一个响应，递减等待请求个数 outstanding_ops；</li>\n<li>如果还有没发送的请求，则通知发送流程继续发送请求；</li>\n<li>如果所有的请求还没发送结束，则跳转到步骤1继续执行；</li>\n<li>如果所有的请求已经发送结束，但还存在等待请求，则跳转到步骤2)继续执行；</li>\n</ol>\n<h2 id=\"服务端\"><a href=\"#服务端\" class=\"headerlink\" title=\"服务端\"></a>服务端</h2><figure class=\"highlight c++\"><table><tr><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\">// Q代表优先级队列的类型</span></div><div class=\"line\"><span class=\"keyword\">template</span>&lt;<span class=\"keyword\">typename</span> Q, <span class=\"keyword\">typename</span> ReqPm, <span class=\"keyword\">typename</span> RespPm, <span class=\"keyword\">typename</span> Accum&gt;</div><div class=\"line\"><span class=\"keyword\">class</span> SimulatedServer &#123;</div><div class=\"line\">\tQ* priority_queue;  <span class=\"comment\">// 请求队列</span></div><div class=\"line\">    <span class=\"comment\">// 内部请求队列</span></div><div class=\"line\">    <span class=\"comment\">// 数据从priority_queue队列导入，导入策略由priority_queue队列定义</span></div><div class=\"line\">    <span class=\"built_in\">std</span>::<span class=\"built_in\">deque</span>&lt;QueueItem&gt; inner_queue; </div><div class=\"line\">    <span class=\"comment\">// priority_queue向inner_queue投递请求</span></div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">void</span> <span class=\"title\">inner_post</span><span class=\"params\">(</span></span></div><div class=\"line\">        <span class=\"keyword\">const</span> ClientId&amp; client,</div><div class=\"line\">        <span class=\"built_in\">std</span>::<span class=\"built_in\">unique_ptr</span>&lt;TestRequest&gt; request,</div><div class=\"line\">        <span class=\"keyword\">const</span> RespPm&amp; additional</div><div class=\"line\">    );</div><div class=\"line\"></div><div class=\"line\">\tClientRespFunc\tclient_resp_f;  <span class=\"comment\">// 返回响应给客户</span></div><div class=\"line\">\t<span class=\"keyword\">int</span> iops;  <span class=\"comment\">// Server的IOPS能力</span></div><div class=\"line\">    <span class=\"keyword\">size_t</span> thread_pool_size;  <span class=\"comment\">// 线程池的大小</span></div><div class=\"line\">    <span class=\"built_in\">std</span>::chrono::microseconds op_time;  <span class=\"comment\">// 处理1个IO花费的时间，用于模拟IO处理</span></div><div class=\"line\">    <span class=\"keyword\">size_t</span> thread_pool_size;</div><div class=\"line\">    ServerAccumFunc accum_f;</div><div class=\"line\"></div><div class=\"line\">    <span class=\"built_in\">std</span>::chrono::microseconds op_time;  <span class=\"comment\">// 完成1个请求花费的时间</span></div><div class=\"line\"></div><div class=\"line\">    <span class=\"comment\">// 接收客户请求</span></div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">void</span> <span class=\"title\">post</span><span class=\"params\">(</span></span></div><div class=\"line\">        <span class=\"keyword\">const</span> TestRequest&amp; request,  <span class=\"comment\">// 请求内容</span></div><div class=\"line\">        <span class=\"keyword\">const</span> ClientId&amp; client_id,  <span class=\"comment\">// 客户id</span></div><div class=\"line\">        <span class=\"keyword\">const</span> ReqPm&amp; req_params  <span class=\"comment\">// 请求参数</span></div><div class=\"line\">    );</div><div class=\"line\">&#125;;</div></pre></td></tr></table></figure>\n<p>Server属于自己的属性：IOPS(默认值 40)、线程数。IOPS代表Server每秒处理IO数目的能力。根据这个属性可以推算出处理一个IO花费的时间，即op_time属性。在IOPS能力给定的情况下，如果使用多线程，那么op_time的值为单线程的thread_pool_size倍。thread_pool_size为线程池的大小。</p>\n<p>请求处理流程：</p>\n<ol>\n<li>检查内部队列是否为空。若为空，则等待1s后继续检查；</li>\n<li>从内部队列中pop一个请求，开始处理请求，即等待op_time时间；</li>\n<li>发送响应给客户。</li>\n</ol>\n<p>SimulatedServer内部维护了两个队列：内部队列inner_queue和优先级队列priority_queue。内部队列是FCFS队列，Server处理请求时直接从该队列读取请求。priority_queue队列用于定义QoS调度算法，客户的请求将先入priority_queue队列，然后由priority_queue队列根据自己的算法将请求放入内部队列。</p>\n<h2 id=\"简单调度-ssched\"><a href=\"#简单调度-ssched\" class=\"headerlink\" title=\"简单调度 ssched\"></a>简单调度 ssched</h2><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">./ssched_sim</div></pre></td></tr></table></figure>\n<p>启动模拟器。</p>\n<figure class=\"highlight c++\"><table><tr><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\">// 服务端参数</span></div><div class=\"line\"><span class=\"keyword\">const</span> uint server_count = <span class=\"number\">100</span>;  <span class=\"comment\">// 服务数目</span></div><div class=\"line\"><span class=\"keyword\">const</span> uint server_iops = <span class=\"number\">40</span>;  <span class=\"comment\">// 每个服务的IOPS能力</span></div><div class=\"line\"><span class=\"keyword\">const</span> uint server_threads = <span class=\"number\">1</span>;  <span class=\"comment\">// 每个服务的线程池数目</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">// 客户端参数</span></div><div class=\"line\"><span class=\"keyword\">const</span> uint client_total_ops = <span class=\"number\">1000</span>;  <span class=\"comment\">// 每个客户发送的请求数目</span></div><div class=\"line\"><span class=\"keyword\">const</span> uint client_count = <span class=\"number\">100</span>;  <span class=\"comment\">// 客户数目</span></div><div class=\"line\"><span class=\"keyword\">const</span> uint client_server_select_range = <span class=\"number\">10</span>;</div><div class=\"line\"><span class=\"keyword\">const</span> uint client_wait_count = <span class=\"number\">1</span>;  <span class=\"comment\">// 延迟启动的客户的数目</span></div><div class=\"line\"><span class=\"keyword\">const</span> uint client_iops_goal = <span class=\"number\">50</span>;  <span class=\"comment\">// 每个客户期望的IOPS</span></div><div class=\"line\"><span class=\"keyword\">const</span> uint client_outstanding_ops = <span class=\"number\">100</span>;  <span class=\"comment\">// 每个客户允许的等待请求</span></div><div class=\"line\"><span class=\"keyword\">const</span> <span class=\"built_in\">std</span>::chrono::<span class=\"function\">seconds <span class=\"title\">client_wait</span><span class=\"params\">(<span class=\"number\">10</span>)</span></span>;  <span class=\"comment\">// 延迟启动的客户的延迟时间</span></div></pre></td></tr></table></figure>\n<p>SimpleServer的优先级队列是SimpleQueue类，SimpleQueue类调度请求的顺序是先进先出。具体实现参考该类的schedule_request()方法。</p>\n<h2 id=\"mClock调度-dmc\"><a href=\"#mClock调度-dmc\" class=\"headerlink\" title=\"mClock调度 dmc\"></a>mClock调度 dmc</h2><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">./dmc_sim --conf dmc_sim_100th.conf</div></pre></td></tr></table></figure>\n<p>启动模拟器，dmc_sim_100th.conf为配置文件。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">[global]                                                                                                                                 </div><div class=\"line\">server_groups = 1   # 1组服务</div><div class=\"line\">client_groups = 2   # 2组客户，分别定义在后面的client.0和client.1两个section内</div><div class=\"line\">server_random_selection = true</div><div class=\"line\">server_soft_limit = true</div><div class=\"line\"></div><div class=\"line\">[client.0]                                                                                          </div><div class=\"line\">client_count = 99  # 客户数目</div><div class=\"line\">client_wait = 0  # 客户延迟启动的时间                                                                       </div><div class=\"line\">client_total_ops = 2000  # 每个客户发送的请求数目</div><div class=\"line\">client_server_select_range = 10</div><div class=\"line\">client_iops_goal = 50  # 每个客户发送请求的IOPS能力，也就是说，如果服务能力充足，那么客户最大的IOPS为50</div><div class=\"line\">client_outstanding_ops = 100  # 最大等待请求的数目</div><div class=\"line\">client_reservation = 5.0  # dmClock中的下限</div><div class=\"line\">client_limit = 60.0  # dmClock中的上限                                                         </div><div class=\"line\">client_weight = 1.0  # dmClock中的权重                                                              </div><div class=\"line\"></div><div class=\"line\">[client.1]</div><div class=\"line\">client_count = 1</div><div class=\"line\">client_wait = 10  # 延迟10s后启动客户机</div><div class=\"line\">client_total_ops = 2000</div><div class=\"line\">client_server_select_range = 10                                                                     </div><div class=\"line\">client_iops_goal = 300</div><div class=\"line\">client_outstanding_ops = 100                                                                        </div><div class=\"line\">client_reservation = 20.0                                                                           </div><div class=\"line\">client_limit = 200.0                                                                                </div><div class=\"line\">client_weight = 10.0                                                                                </div><div class=\"line\"></div><div class=\"line\">[server.0]</div><div class=\"line\">server_count = 100  # 服务的数目</div><div class=\"line\">server_iops = 40  # 每个服务处理IO的能力，最大为40，总共为 100 * 40 = 4000</div><div class=\"line\">server_threads = 1  # 每个服务的线程池大小</div></pre></td></tr></table></figure>\n<p>配置文件中各个字段的说明，另外重点说明下client.1客户机的配置：</p>\n<ul>\n<li>延迟启动(client_wait配置项)。延迟启动的目的是为了比较client.0客户机在有新客户机启动时IOPS的变化，一般来说都会开始下降；</li>\n<li>权重大(client_weight配置项) 。client.1只有一个客户机，客户机启动后能够比较出该客户机同其它客户机在IOPS上的差异，一般来说会大点；</li>\n<li>客户机能力强(client_iops_goal配置项)。在设置较大权重，但客户机的IOPS还不够高的情况下，可以调整client_iops_goal的值，增加客户机发送请求的能力。</li>\n</ul>\n<figure class=\"highlight c++\"><table><tr><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">struct</span> ClientInfo &#123;</div><div class=\"line\">    <span class=\"keyword\">const</span> <span class=\"keyword\">double</span> reservation;  <span class=\"comment\">// 下限，默认20.0</span></div><div class=\"line\">    <span class=\"keyword\">const</span> <span class=\"keyword\">double</span> limit;  <span class=\"comment\">// 上限，默认 60.0</span></div><div class=\"line\">    <span class=\"keyword\">const</span> <span class=\"keyword\">double</span> weight;  <span class=\"comment\">// 权重，默认 1.0</span></div><div class=\"line\">    <span class=\"keyword\">const</span> <span class=\"keyword\">double</span> reservation_inv; <span class=\"comment\">// 下限的倒数</span></div><div class=\"line\">    <span class=\"keyword\">const</span> <span class=\"keyword\">double</span> limit_inv; <span class=\"comment\">// 上限的倒数</span></div><div class=\"line\">    <span class=\"keyword\">const</span> <span class=\"keyword\">double</span> weight_inv; <span class=\"comment\">//  权重的倒数</span></div><div class=\"line\">&#125;;</div></pre></td></tr></table></figure>\n<hr>\n<p><strong>版权声明</strong></p>\n<p>本作品采用<a href=\"https://creativecommons.org/licenses/by/2.5/cn/\" target=\"_blank\" rel=\"external\">知识共享署名2.5 中国大陆许可协议</a>进行许可，欢迎转载，但转载请注明来自<a href=\"https://wuxiangwei.cn/\"><em>Shanno’s Blog</em></a>，并保持转载后文章内容的完整。本人保留所有版权相关权利，侵权必究。</p>\n","excerpt":"<p>Sim模块构建一个模拟器来模拟一组客户机向一组服务主机发送请求，同时服务主机根据指定的QoS算法调度请求的处理顺序的过程。Sim提供了两种调度算法，一种是FCFS的简单调度算法，另一种就是dmClock调度算法。</p>","more":"<figure class=\"highlight c++\"><table><tr><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\">// TS代表服务，例如 SimulatedServer</span></div><div class=\"line\"><span class=\"comment\">// TC代表客户，例如 SimulatedClient</span></div><div class=\"line\"><span class=\"keyword\">template</span>&lt;<span class=\"keyword\">typename</span> ServerId, <span class=\"keyword\">typename</span> ClientId, <span class=\"keyword\">typename</span> TS, <span class=\"keyword\">typename</span> TC&gt;</div><div class=\"line\"><span class=\"keyword\">class</span> Simulation &#123;</div><div class=\"line\"><span class=\"keyword\">public</span>:</div><div class=\"line\">    ServerMap servers; <span class=\"comment\">// 服务列表</span></div><div class=\"line\">    ClientMap clients; <span class=\"comment\">// 客户列表</span></div><div class=\"line\"></div><div class=\"line\">    TimePoint early_time;  <span class=\"comment\">// 实例化时的时间</span></div><div class=\"line\">    TimePoint late_time;  <span class=\"comment\">// 客户结束的时间</span></div><div class=\"line\"></div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">void</span> <span class=\"title\">display_stats</span><span class=\"params\">()</span></span>;  <span class=\"comment\">// 打印统计信息</span></div><div class=\"line\">&#125;;</div></pre></td></tr></table></figure>\n<p>打印客户报告：</p>\n<p>依赖于每个client的op_times属性，该属性记录每个请求完成时的时间。<br>打印报告时，对每个client每隔measure_unit(2s)秒打印一次IOPS数据。此外，调整report_unit参数能够等比例放大或者缩小IOPS的值便于查看分析结果。</p>\n<p>打印服务报告：</p>\n<p>打印每个服务处理的请求数目。</p>\n<h2 id=\"客户端\"><a href=\"#客户端\" class=\"headerlink\" title=\"客户端\"></a>客户端</h2><figure class=\"highlight c++\"><table><tr><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">class</span> SimulatedClient &#123;</div><div class=\"line\"><span class=\"keyword\">protected</span>:</div><div class=\"line\">    <span class=\"built_in\">std</span>::<span class=\"built_in\">vector</span>&lt;CliInst&gt; instructions;</div><div class=\"line\"></div><div class=\"line\">    <span class=\"built_in\">std</span>::thread thd_req;</div><div class=\"line\">    <span class=\"built_in\">std</span>::thread thd_resp;</div><div class=\"line\"></div><div class=\"line\">    <span class=\"comment\">// 性能数据收集</span></div><div class=\"line\">    <span class=\"built_in\">std</span>::<span class=\"built_in\">vector</span>&lt;TimePoint&gt; op_times;  <span class=\"comment\">// 请求完成的时间，从小到大排序</span></div><div class=\"line\"></div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">void</span> <span class=\"title\">run_req</span><span class=\"params\">()</span></span>;  <span class=\"comment\">// 发送请求的线程入口</span></div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">void</span> <span class=\"title\">run_resp</span><span class=\"params\">()</span></span>;  <span class=\"comment\">// 接收响应的线程入口</span></div><div class=\"line\">    </div><div class=\"line\">    <span class=\"comment\">// 接收来自服务的响应</span></div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">void</span> <span class=\"title\">receive_response</span><span class=\"params\">(</div><div class=\"line\">        <span class=\"keyword\">const</span> TestResponse&amp; resp,</div><div class=\"line\">\t    <span class=\"keyword\">const</span> ServerId&amp; server_id,</div><div class=\"line\">\t    <span class=\"keyword\">const</span> RespPm&amp; resp_params</div><div class=\"line\">    )</span></span>;</div><div class=\"line\">&#125;;</div></pre></td></tr></table></figure>\n<p>CliInst类描述生成请求的指令（instruction），两种生成请求的方式：一种是生成count(1000)个请求，同时设定最多允许max_outstanding(100)个等待请求以及每个请求的时延time_bw_reqs(单位：微秒)；另一种只是等待。何为等待请求？已发送但未接收到响应的请求。</p>\n<p>发送请求的流程：</p>\n<ol>\n<li>检查等待请求数目是否大于上限值max_outstanding，如果大于，则一直等待不发送请求。</li>\n<li>开始发送请求：<br> a) 为给定的请求o选择一个处理请求的Server；<br> b) 发送请求给a)中选定的Server；<br> c) 递增等待的请求数目 outstanding_ops；<br> d) 等待time_bw_reqs时间后，继续发生下个请求，即继续a)步骤。</li>\n<li>所有请求发送结束，设置标志位requests_complete为True。</li>\n</ol>\n<p>处理响应的流程：</p>\n<ol>\n<li>检查响应队列是否为空，若为空，则等待1s后继续检查，若不为空，则继续步骤2；</li>\n<li>从响应队列中pop出一个响应，递减等待请求个数 outstanding_ops；</li>\n<li>如果还有没发送的请求，则通知发送流程继续发送请求；</li>\n<li>如果所有的请求还没发送结束，则跳转到步骤1继续执行；</li>\n<li>如果所有的请求已经发送结束，但还存在等待请求，则跳转到步骤2)继续执行；</li>\n</ol>\n<h2 id=\"服务端\"><a href=\"#服务端\" class=\"headerlink\" title=\"服务端\"></a>服务端</h2><figure class=\"highlight c++\"><table><tr><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\">// Q代表优先级队列的类型</span></div><div class=\"line\"><span class=\"keyword\">template</span>&lt;<span class=\"keyword\">typename</span> Q, <span class=\"keyword\">typename</span> ReqPm, <span class=\"keyword\">typename</span> RespPm, <span class=\"keyword\">typename</span> Accum&gt;</div><div class=\"line\"><span class=\"keyword\">class</span> SimulatedServer &#123;</div><div class=\"line\">\tQ* priority_queue;  <span class=\"comment\">// 请求队列</span></div><div class=\"line\">    <span class=\"comment\">// 内部请求队列</span></div><div class=\"line\">    <span class=\"comment\">// 数据从priority_queue队列导入，导入策略由priority_queue队列定义</span></div><div class=\"line\">    <span class=\"built_in\">std</span>::<span class=\"built_in\">deque</span>&lt;QueueItem&gt; inner_queue; </div><div class=\"line\">    <span class=\"comment\">// priority_queue向inner_queue投递请求</span></div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">void</span> <span class=\"title\">inner_post</span><span class=\"params\">(</div><div class=\"line\">        <span class=\"keyword\">const</span> ClientId&amp; client,</div><div class=\"line\">        <span class=\"built_in\">std</span>::<span class=\"built_in\">unique_ptr</span>&lt;TestRequest&gt; request,</div><div class=\"line\">        <span class=\"keyword\">const</span> RespPm&amp; additional</div><div class=\"line\">    )</span></span>;</div><div class=\"line\"></div><div class=\"line\">\tClientRespFunc\tclient_resp_f;  <span class=\"comment\">// 返回响应给客户</span></div><div class=\"line\">\t<span class=\"keyword\">int</span> iops;  <span class=\"comment\">// Server的IOPS能力</span></div><div class=\"line\">    <span class=\"keyword\">size_t</span> thread_pool_size;  <span class=\"comment\">// 线程池的大小</span></div><div class=\"line\">    <span class=\"built_in\">std</span>::chrono::microseconds op_time;  <span class=\"comment\">// 处理1个IO花费的时间，用于模拟IO处理</span></div><div class=\"line\">    <span class=\"keyword\">size_t</span> thread_pool_size;</div><div class=\"line\">    ServerAccumFunc accum_f;</div><div class=\"line\"></div><div class=\"line\">    <span class=\"built_in\">std</span>::chrono::microseconds op_time;  <span class=\"comment\">// 完成1个请求花费的时间</span></div><div class=\"line\"></div><div class=\"line\">    <span class=\"comment\">// 接收客户请求</span></div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">void</span> <span class=\"title\">post</span><span class=\"params\">(</div><div class=\"line\">        <span class=\"keyword\">const</span> TestRequest&amp; request,  <span class=\"comment\">// 请求内容</span></div><div class=\"line\">        <span class=\"keyword\">const</span> ClientId&amp; client_id,  <span class=\"comment\">// 客户id</span></div><div class=\"line\">        <span class=\"keyword\">const</span> ReqPm&amp; req_params  <span class=\"comment\">// 请求参数</span></div><div class=\"line\">    )</span></span>;</div><div class=\"line\">&#125;;</div></pre></td></tr></table></figure>\n<p>Server属于自己的属性：IOPS(默认值 40)、线程数。IOPS代表Server每秒处理IO数目的能力。根据这个属性可以推算出处理一个IO花费的时间，即op_time属性。在IOPS能力给定的情况下，如果使用多线程，那么op_time的值为单线程的thread_pool_size倍。thread_pool_size为线程池的大小。</p>\n<p>请求处理流程：</p>\n<ol>\n<li>检查内部队列是否为空。若为空，则等待1s后继续检查；</li>\n<li>从内部队列中pop一个请求，开始处理请求，即等待op_time时间；</li>\n<li>发送响应给客户。</li>\n</ol>\n<p>SimulatedServer内部维护了两个队列：内部队列inner_queue和优先级队列priority_queue。内部队列是FCFS队列，Server处理请求时直接从该队列读取请求。priority_queue队列用于定义QoS调度算法，客户的请求将先入priority_queue队列，然后由priority_queue队列根据自己的算法将请求放入内部队列。</p>\n<h2 id=\"简单调度-ssched\"><a href=\"#简单调度-ssched\" class=\"headerlink\" title=\"简单调度 ssched\"></a>简单调度 ssched</h2><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">./ssched_sim</div></pre></td></tr></table></figure>\n<p>启动模拟器。</p>\n<figure class=\"highlight c++\"><table><tr><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\">// 服务端参数</span></div><div class=\"line\"><span class=\"keyword\">const</span> uint server_count = <span class=\"number\">100</span>;  <span class=\"comment\">// 服务数目</span></div><div class=\"line\"><span class=\"keyword\">const</span> uint server_iops = <span class=\"number\">40</span>;  <span class=\"comment\">// 每个服务的IOPS能力</span></div><div class=\"line\"><span class=\"keyword\">const</span> uint server_threads = <span class=\"number\">1</span>;  <span class=\"comment\">// 每个服务的线程池数目</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">// 客户端参数</span></div><div class=\"line\"><span class=\"keyword\">const</span> uint client_total_ops = <span class=\"number\">1000</span>;  <span class=\"comment\">// 每个客户发送的请求数目</span></div><div class=\"line\"><span class=\"keyword\">const</span> uint client_count = <span class=\"number\">100</span>;  <span class=\"comment\">// 客户数目</span></div><div class=\"line\"><span class=\"keyword\">const</span> uint client_server_select_range = <span class=\"number\">10</span>;</div><div class=\"line\"><span class=\"keyword\">const</span> uint client_wait_count = <span class=\"number\">1</span>;  <span class=\"comment\">// 延迟启动的客户的数目</span></div><div class=\"line\"><span class=\"keyword\">const</span> uint client_iops_goal = <span class=\"number\">50</span>;  <span class=\"comment\">// 每个客户期望的IOPS</span></div><div class=\"line\"><span class=\"keyword\">const</span> uint client_outstanding_ops = <span class=\"number\">100</span>;  <span class=\"comment\">// 每个客户允许的等待请求</span></div><div class=\"line\"><span class=\"keyword\">const</span> <span class=\"built_in\">std</span>::chrono::<span class=\"function\">seconds <span class=\"title\">client_wait</span><span class=\"params\">(<span class=\"number\">10</span>)</span></span>;  <span class=\"comment\">// 延迟启动的客户的延迟时间</span></div></pre></td></tr></table></figure>\n<p>SimpleServer的优先级队列是SimpleQueue类，SimpleQueue类调度请求的顺序是先进先出。具体实现参考该类的schedule_request()方法。</p>\n<h2 id=\"mClock调度-dmc\"><a href=\"#mClock调度-dmc\" class=\"headerlink\" title=\"mClock调度 dmc\"></a>mClock调度 dmc</h2><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">./dmc_sim --conf dmc_sim_100th.conf</div></pre></td></tr></table></figure>\n<p>启动模拟器，dmc_sim_100th.conf为配置文件。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">[global]                                                                                                                                 </div><div class=\"line\">server_groups = 1   # 1组服务</div><div class=\"line\">client_groups = 2   # 2组客户，分别定义在后面的client.0和client.1两个section内</div><div class=\"line\">server_random_selection = true</div><div class=\"line\">server_soft_limit = true</div><div class=\"line\"></div><div class=\"line\">[client.0]                                                                                          </div><div class=\"line\">client_count = 99  # 客户数目</div><div class=\"line\">client_wait = 0  # 客户延迟启动的时间                                                                       </div><div class=\"line\">client_total_ops = 2000  # 每个客户发送的请求数目</div><div class=\"line\">client_server_select_range = 10</div><div class=\"line\">client_iops_goal = 50  # 每个客户发送请求的IOPS能力，也就是说，如果服务能力充足，那么客户最大的IOPS为50</div><div class=\"line\">client_outstanding_ops = 100  # 最大等待请求的数目</div><div class=\"line\">client_reservation = 5.0  # dmClock中的下限</div><div class=\"line\">client_limit = 60.0  # dmClock中的上限                                                         </div><div class=\"line\">client_weight = 1.0  # dmClock中的权重                                                              </div><div class=\"line\"></div><div class=\"line\">[client.1]</div><div class=\"line\">client_count = 1</div><div class=\"line\">client_wait = 10  # 延迟10s后启动客户机</div><div class=\"line\">client_total_ops = 2000</div><div class=\"line\">client_server_select_range = 10                                                                     </div><div class=\"line\">client_iops_goal = 300</div><div class=\"line\">client_outstanding_ops = 100                                                                        </div><div class=\"line\">client_reservation = 20.0                                                                           </div><div class=\"line\">client_limit = 200.0                                                                                </div><div class=\"line\">client_weight = 10.0                                                                                </div><div class=\"line\"></div><div class=\"line\">[server.0]</div><div class=\"line\">server_count = 100  # 服务的数目</div><div class=\"line\">server_iops = 40  # 每个服务处理IO的能力，最大为40，总共为 100 * 40 = 4000</div><div class=\"line\">server_threads = 1  # 每个服务的线程池大小</div></pre></td></tr></table></figure>\n<p>配置文件中各个字段的说明，另外重点说明下client.1客户机的配置：</p>\n<ul>\n<li>延迟启动(client_wait配置项)。延迟启动的目的是为了比较client.0客户机在有新客户机启动时IOPS的变化，一般来说都会开始下降；</li>\n<li>权重大(client_weight配置项) 。client.1只有一个客户机，客户机启动后能够比较出该客户机同其它客户机在IOPS上的差异，一般来说会大点；</li>\n<li>客户机能力强(client_iops_goal配置项)。在设置较大权重，但客户机的IOPS还不够高的情况下，可以调整client_iops_goal的值，增加客户机发送请求的能力。</li>\n</ul>\n<figure class=\"highlight c++\"><table><tr><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">struct</span> ClientInfo &#123;</div><div class=\"line\">    <span class=\"keyword\">const</span> <span class=\"keyword\">double</span> reservation;  <span class=\"comment\">// 下限，默认20.0</span></div><div class=\"line\">    <span class=\"keyword\">const</span> <span class=\"keyword\">double</span> limit;  <span class=\"comment\">// 上限，默认 60.0</span></div><div class=\"line\">    <span class=\"keyword\">const</span> <span class=\"keyword\">double</span> weight;  <span class=\"comment\">// 权重，默认 1.0</span></div><div class=\"line\">    <span class=\"keyword\">const</span> <span class=\"keyword\">double</span> reservation_inv; <span class=\"comment\">// 下限的倒数</span></div><div class=\"line\">    <span class=\"keyword\">const</span> <span class=\"keyword\">double</span> limit_inv; <span class=\"comment\">// 上限的倒数</span></div><div class=\"line\">    <span class=\"keyword\">const</span> <span class=\"keyword\">double</span> weight_inv; <span class=\"comment\">//  权重的倒数</span></div><div class=\"line\">&#125;;</div></pre></td></tr></table></figure>\n<hr>\n<p><strong>版权声明</strong></p>\n<p>本作品采用<a href=\"https://creativecommons.org/licenses/by/2.5/cn/\">知识共享署名2.5 中国大陆许可协议</a>进行许可，欢迎转载，但转载请注明来自<a href=\"https://wuxiangwei.cn/\"><em>Shanno’s Blog</em></a>，并保持转载后文章内容的完整。本人保留所有版权相关权利，侵权必究。</p>"},{"title":"dmClock算法测试","date":"2016-11-18T07:01:26.000Z","toc":true,"_content":"\n### 实验配置\n\n\n本实验，采用3个客户端，每个客户端的配置如下：\n\n| 客户端 | 下限 | 权重 | 上限 |\n|:--|:--|:--|:--|\n| client.0 | 30 | 1 | Inf |\n| client.1 |  0 | 1 | 60  |\n| client.2 |  0 | 2 | 100 |\n\n另外，在服务端请求处理能力充裕的条件下，每个客户端均可以达到200 IOPS的能力。不过本实验限制服务端的处理能力为 100 IOPS，也就说客户端最多只能提供100 IOPS。\n\n<!--more-->\n\n### 结果分析\n\n![](http://ohn764ue3.bkt.clouddn.com/DmClock/test/DMClock_test.png)\n\n3台客户端的启动时间不同，如上图所示，client.0最先启动，client.1在T1时刻启动，client.2在T2时刻启动。下面分析每个时间段内各个客户端的IOPS分配情况：\n\n0~T1时间段，只有client.0处于运行状态。虽然client.0自身有200IOPS的能力，但由于服务端只能提供100IOPS，因此client.0只能享受100IOPS的服务。\n\nT1时刻，client.1开始启动，从而导致client.0的IOPS开始下降。下降过程中短暂的波动后，client.0和client.1的IOPS达到相同的50 IOPS，恰好平分服务端的100 IOPS。client.0和client.1之所以恰好平分的原因是，两者的**权重**均为1，并且client.0没有低于其下限，client.1没有高于其上限。\n\nT2时刻，client.2开始启动，从而导致client.0和client.1的IOPS开始下降。值得注意的是，虽然client.0和client.1都开始下降，但两者最终稳定的IOPS是不同的，client.0稳定在30 IOPS，client.1稳定在23 IOPS，而client.2的IOPS却一直保持在较高的 46 IOPS。首先，服务端只能提供100 IOPS，三个客户端的权重比例为1:1:2，按照权重比例分配IO的结果应该是25，25，50。由于25 IOPS已低于client.0的下限值30IOPS。因此，为保证client.0的下限，dmClock优先满足client.0的下限。所以才有client.0最终稳定在30 IOPS的结果。其次，对剩余的70个IOPS，dmClock将其按照client.1和client.2的权重比例分配。\n\nT3时刻，client.0开始关机，从而导致client.1和client.2的IOPS开始上升。最终，client.1的IOPS稳定在33 IOPS，client.2稳定在66 IOPS。此时，这两者恰好按照权重比例划分服务端总的IOPS。\n\nT4时刻，client.2开始关机，从而导致client.1的IOPS开始上升。正常来说，此时只剩client.1一个客户应该独占服务端的100 IOPS。但实验结果显示client.1只占用了60个IOPS，这是因为client.1的上限值设置为了60 IOPS。\n\n综上分析，dmClock确实已经实现了保证下限、限制上限以及按比例分配IO的能力。\n","source":"_posts/DmClock/MCLOCK_test.md","raw":"\n---\ntitle: \"dmClock算法测试\"\ndate: 2016-11-18 15:01:26\ncategories: [mClock]\ntags: [mClock]\ntoc: true\n---\n\n### 实验配置\n\n\n本实验，采用3个客户端，每个客户端的配置如下：\n\n| 客户端 | 下限 | 权重 | 上限 |\n|:--|:--|:--|:--|\n| client.0 | 30 | 1 | Inf |\n| client.1 |  0 | 1 | 60  |\n| client.2 |  0 | 2 | 100 |\n\n另外，在服务端请求处理能力充裕的条件下，每个客户端均可以达到200 IOPS的能力。不过本实验限制服务端的处理能力为 100 IOPS，也就说客户端最多只能提供100 IOPS。\n\n<!--more-->\n\n### 结果分析\n\n![](http://ohn764ue3.bkt.clouddn.com/DmClock/test/DMClock_test.png)\n\n3台客户端的启动时间不同，如上图所示，client.0最先启动，client.1在T1时刻启动，client.2在T2时刻启动。下面分析每个时间段内各个客户端的IOPS分配情况：\n\n0~T1时间段，只有client.0处于运行状态。虽然client.0自身有200IOPS的能力，但由于服务端只能提供100IOPS，因此client.0只能享受100IOPS的服务。\n\nT1时刻，client.1开始启动，从而导致client.0的IOPS开始下降。下降过程中短暂的波动后，client.0和client.1的IOPS达到相同的50 IOPS，恰好平分服务端的100 IOPS。client.0和client.1之所以恰好平分的原因是，两者的**权重**均为1，并且client.0没有低于其下限，client.1没有高于其上限。\n\nT2时刻，client.2开始启动，从而导致client.0和client.1的IOPS开始下降。值得注意的是，虽然client.0和client.1都开始下降，但两者最终稳定的IOPS是不同的，client.0稳定在30 IOPS，client.1稳定在23 IOPS，而client.2的IOPS却一直保持在较高的 46 IOPS。首先，服务端只能提供100 IOPS，三个客户端的权重比例为1:1:2，按照权重比例分配IO的结果应该是25，25，50。由于25 IOPS已低于client.0的下限值30IOPS。因此，为保证client.0的下限，dmClock优先满足client.0的下限。所以才有client.0最终稳定在30 IOPS的结果。其次，对剩余的70个IOPS，dmClock将其按照client.1和client.2的权重比例分配。\n\nT3时刻，client.0开始关机，从而导致client.1和client.2的IOPS开始上升。最终，client.1的IOPS稳定在33 IOPS，client.2稳定在66 IOPS。此时，这两者恰好按照权重比例划分服务端总的IOPS。\n\nT4时刻，client.2开始关机，从而导致client.1的IOPS开始上升。正常来说，此时只剩client.1一个客户应该独占服务端的100 IOPS。但实验结果显示client.1只占用了60个IOPS，这是因为client.1的上限值设置为了60 IOPS。\n\n综上分析，dmClock确实已经实现了保证下限、限制上限以及按比例分配IO的能力。\n","slug":"DmClock/MCLOCK_test","published":1,"updated":"2016-12-15T07:18:40.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cix1kgaug0015daei19zrvje5","content":"<h3 id=\"实验配置\"><a href=\"#实验配置\" class=\"headerlink\" title=\"实验配置\"></a>实验配置</h3><p>本实验，采用3个客户端，每个客户端的配置如下：</p>\n<table>\n<thead>\n<tr>\n<th style=\"text-align:left\">客户端</th>\n<th style=\"text-align:left\">下限</th>\n<th style=\"text-align:left\">权重</th>\n<th style=\"text-align:left\">上限</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:left\">client.0</td>\n<td style=\"text-align:left\">30</td>\n<td style=\"text-align:left\">1</td>\n<td style=\"text-align:left\">Inf</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">client.1</td>\n<td style=\"text-align:left\">0</td>\n<td style=\"text-align:left\">1</td>\n<td style=\"text-align:left\">60</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">client.2</td>\n<td style=\"text-align:left\">0</td>\n<td style=\"text-align:left\">2</td>\n<td style=\"text-align:left\">100</td>\n</tr>\n</tbody>\n</table>\n<p>另外，在服务端请求处理能力充裕的条件下，每个客户端均可以达到200 IOPS的能力。不过本实验限制服务端的处理能力为 100 IOPS，也就说客户端最多只能提供100 IOPS。</p>\n<a id=\"more\"></a>\n<h3 id=\"结果分析\"><a href=\"#结果分析\" class=\"headerlink\" title=\"结果分析\"></a>结果分析</h3><p><img src=\"http://ohn764ue3.bkt.clouddn.com/DmClock/test/DMClock_test.png\" alt=\"\"></p>\n<p>3台客户端的启动时间不同，如上图所示，client.0最先启动，client.1在T1时刻启动，client.2在T2时刻启动。下面分析每个时间段内各个客户端的IOPS分配情况：</p>\n<p>0~T1时间段，只有client.0处于运行状态。虽然client.0自身有200IOPS的能力，但由于服务端只能提供100IOPS，因此client.0只能享受100IOPS的服务。</p>\n<p>T1时刻，client.1开始启动，从而导致client.0的IOPS开始下降。下降过程中短暂的波动后，client.0和client.1的IOPS达到相同的50 IOPS，恰好平分服务端的100 IOPS。client.0和client.1之所以恰好平分的原因是，两者的<strong>权重</strong>均为1，并且client.0没有低于其下限，client.1没有高于其上限。</p>\n<p>T2时刻，client.2开始启动，从而导致client.0和client.1的IOPS开始下降。值得注意的是，虽然client.0和client.1都开始下降，但两者最终稳定的IOPS是不同的，client.0稳定在30 IOPS，client.1稳定在23 IOPS，而client.2的IOPS却一直保持在较高的 46 IOPS。首先，服务端只能提供100 IOPS，三个客户端的权重比例为1:1:2，按照权重比例分配IO的结果应该是25，25，50。由于25 IOPS已低于client.0的下限值30IOPS。因此，为保证client.0的下限，dmClock优先满足client.0的下限。所以才有client.0最终稳定在30 IOPS的结果。其次，对剩余的70个IOPS，dmClock将其按照client.1和client.2的权重比例分配。</p>\n<p>T3时刻，client.0开始关机，从而导致client.1和client.2的IOPS开始上升。最终，client.1的IOPS稳定在33 IOPS，client.2稳定在66 IOPS。此时，这两者恰好按照权重比例划分服务端总的IOPS。</p>\n<p>T4时刻，client.2开始关机，从而导致client.1的IOPS开始上升。正常来说，此时只剩client.1一个客户应该独占服务端的100 IOPS。但实验结果显示client.1只占用了60个IOPS，这是因为client.1的上限值设置为了60 IOPS。</p>\n<p>综上分析，dmClock确实已经实现了保证下限、限制上限以及按比例分配IO的能力。</p>\n<hr>\n<p><strong>版权声明</strong></p>\n<p>本作品采用<a href=\"https://creativecommons.org/licenses/by/2.5/cn/\" target=\"_blank\" rel=\"external\">知识共享署名2.5 中国大陆许可协议</a>进行许可，欢迎转载，但转载请注明来自<a href=\"https://wuxiangwei.cn/\"><em>Shanno’s Blog</em></a>，并保持转载后文章内容的完整。本人保留所有版权相关权利，侵权必究。</p>\n","excerpt":"<h3 id=\"实验配置\"><a href=\"#实验配置\" class=\"headerlink\" title=\"实验配置\"></a>实验配置</h3><p>本实验，采用3个客户端，每个客户端的配置如下：</p>\n<table>\n<thead>\n<tr>\n<th style=\"text-align:left\">客户端</th>\n<th style=\"text-align:left\">下限</th>\n<th style=\"text-align:left\">权重</th>\n<th style=\"text-align:left\">上限</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:left\">client.0</td>\n<td style=\"text-align:left\">30</td>\n<td style=\"text-align:left\">1</td>\n<td style=\"text-align:left\">Inf</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">client.1</td>\n<td style=\"text-align:left\">0</td>\n<td style=\"text-align:left\">1</td>\n<td style=\"text-align:left\">60</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">client.2</td>\n<td style=\"text-align:left\">0</td>\n<td style=\"text-align:left\">2</td>\n<td style=\"text-align:left\">100</td>\n</tr>\n</tbody>\n</table>\n<p>另外，在服务端请求处理能力充裕的条件下，每个客户端均可以达到200 IOPS的能力。不过本实验限制服务端的处理能力为 100 IOPS，也就说客户端最多只能提供100 IOPS。</p>","more":"<h3 id=\"结果分析\"><a href=\"#结果分析\" class=\"headerlink\" title=\"结果分析\"></a>结果分析</h3><p><img src=\"http://ohn764ue3.bkt.clouddn.com/DmClock/test/DMClock_test.png\" alt=\"\"></p>\n<p>3台客户端的启动时间不同，如上图所示，client.0最先启动，client.1在T1时刻启动，client.2在T2时刻启动。下面分析每个时间段内各个客户端的IOPS分配情况：</p>\n<p>0~T1时间段，只有client.0处于运行状态。虽然client.0自身有200IOPS的能力，但由于服务端只能提供100IOPS，因此client.0只能享受100IOPS的服务。</p>\n<p>T1时刻，client.1开始启动，从而导致client.0的IOPS开始下降。下降过程中短暂的波动后，client.0和client.1的IOPS达到相同的50 IOPS，恰好平分服务端的100 IOPS。client.0和client.1之所以恰好平分的原因是，两者的<strong>权重</strong>均为1，并且client.0没有低于其下限，client.1没有高于其上限。</p>\n<p>T2时刻，client.2开始启动，从而导致client.0和client.1的IOPS开始下降。值得注意的是，虽然client.0和client.1都开始下降，但两者最终稳定的IOPS是不同的，client.0稳定在30 IOPS，client.1稳定在23 IOPS，而client.2的IOPS却一直保持在较高的 46 IOPS。首先，服务端只能提供100 IOPS，三个客户端的权重比例为1:1:2，按照权重比例分配IO的结果应该是25，25，50。由于25 IOPS已低于client.0的下限值30IOPS。因此，为保证client.0的下限，dmClock优先满足client.0的下限。所以才有client.0最终稳定在30 IOPS的结果。其次，对剩余的70个IOPS，dmClock将其按照client.1和client.2的权重比例分配。</p>\n<p>T3时刻，client.0开始关机，从而导致client.1和client.2的IOPS开始上升。最终，client.1的IOPS稳定在33 IOPS，client.2稳定在66 IOPS。此时，这两者恰好按照权重比例划分服务端总的IOPS。</p>\n<p>T4时刻，client.2开始关机，从而导致client.1的IOPS开始上升。正常来说，此时只剩client.1一个客户应该独占服务端的100 IOPS。但实验结果显示client.1只占用了60个IOPS，这是因为client.1的上限值设置为了60 IOPS。</p>\n<p>综上分析，dmClock确实已经实现了保证下限、限制上限以及按比例分配IO的能力。</p>\n<hr>\n<p><strong>版权声明</strong></p>\n<p>本作品采用<a href=\"https://creativecommons.org/licenses/by/2.5/cn/\">知识共享署名2.5 中国大陆许可协议</a>进行许可，欢迎转载，但转载请注明来自<a href=\"https://wuxiangwei.cn/\"><em>Shanno’s Blog</em></a>，并保持转载后文章内容的完整。本人保留所有版权相关权利，侵权必究。</p>"},{"title":"Linux几个流量监控命令","date":"2014-08-29T09:35:27.000Z","_content":"\n总结几个监控IO流量的命令。\n\n<!--more-->\n\n## crontab\ncron可以设定在指定的时间运行任务。\n\n**1、查看定时任务**\n\n```\n[root@client1 ~]# crontab -l -u root\n*/1 * * * * date >> /root/1.txt\n[root@client1 ~]# cat /var/spool/cron/root \n*/1 * * * * date >> /root/1.txt\n```\n\n查看root用户的定时任务。上面的例子中只有一个定时任务，它每隔1分钟时间将当时的时间追加到*/root/1.txt*文件。实际上，定时任务被写入到*/var/spool/cron/*目录中。\n\n**2、编辑定时任务**\n```\ncrontab -e \n```\n编辑定时任务。\n\n```\n[root@client1 ~]# cat /etc/crontab \n# .---------------- minute (0 - 59)\n# |  .------------- hour (0 - 23)\n# |  |  .---------- day of month (1 - 31)\n# |  |  |  .------- month (1 - 12) OR jan,feb,mar,apr ...\n# |  |  |  |  .---- day of week (0 - 6) (Sunday=0 or 7) OR sun,mon,tue,wed,thu,fri,sat\n# |  |  |  |  |\n# *  *  *  *  * user-name command to be executed\n```\n定时任务的设置格式如上所示，它包含执行命令的时间以及命令的内容两部分内容。命令执行时间分成分、时、天、月以及星期5个部分。实际上，cron进程除了从*var/spool/cron*目录中读取定时任务外，还从*/etc/crontab*中读取。后者是针对系统的定时任务，因此命令执行时间和命令内容外还多了项用户。\n\n**3、cron进程**\n```\n[root@client1 ~]# /etc/init.d/crond restart\nStopping crond: [  OK  ]\nStarting crond: [  OK  ]\n```\n修改定时任务后，可以不重启cron进程。\n\n## iostat\n用于打印块设备、分区以及NFS的IO的统计信息。此外，还会打印CPU的统计信息。\n\n**1、用法**\n```\n[root@client1 ~]# iostat -d /dev/sda1\nDevice:            tps   Blk_read/s   Blk_wrtn/s   Blk_read   Blk_wrtn\nsda1              0.02         0.69         0.00      21232         32\n[root@client1 ~]# iostat -d /dev/sda1 -k\nDevice:            tps    kB_read/s    kB_wrtn/s    kB_read    kB_wrtn\nsda1              0.02         0.34         0.00      10616         16\n[root@client1 ~]# iostat -d -x /dev/sda1 10 2\nDevice:         rrqm/s   wrqm/s     r/s     w/s   rsec/s   wsec/s avgrq-sz avgqu-sz   await  svctm  %util\nsda1              0.01     0.00    0.02    0.00     0.77     0.00    32.41     0.00    0.68   0.52   0.00\nDevice:         rrqm/s   wrqm/s     r/s     w/s   rsec/s   wsec/s avgrq-sz avgqu-sz   await  svctm  %util\nsda1              0.00     0.00    0.00    0.00     0.00     0.00     0.00     0.00    0.00   0.00   0.00\n```\n例子打印了2次*/dev/sda1*块设备的IO统计信息，每次间隔为*10*秒钟。命令参数*-d*代表值输出设备的IO信息不打印CPU的统计信息；*-x*代表打印扩展信息; *-k*代表以kB_read的格式输出。\n\n**2、输出说明**\n\n*一般信息(读写数据量)*\n\ntps: 每秒发送到设备的transfer数目，transfer由多个逻辑的请求合并而成，它的大小是不确定的。\n\nBlk_read/s:  每秒从设备中读取的数据的大小，以块为单位。块大小和文件系统有关，一般为4K字节。        \nBlk_write/s: 每秒写入到设备的数据量，以块为单位。\n\nBlk_read: 从设备读取的Block数目；注意这不是每秒的平均值，而是整个测试过程。\nBlk_wrtn: 写入到设备的Block数目；\n\n*扩展信息(IO请求)*\n\nrrqm/s: r(read)rq(request)m(merge)  每秒合并的读请求数目;     \nwrqm/s: w(write)rq(request)m(merge) 每秒合并的写请求数目;\n\nr/s:  r(read)  每秒发送到设备的读请求数目;      \nw/s:  w(write) 每秒发送到设备的写请求数目；\n\nrsec/s:  r(read)sec(sector)  每秒读扇区的数目；       \nwsec/s:  w(write)sec(sector) 每秒写扇区的数目；\n\navgrq_sz: avg(average)rq(request)-sz(size) 发送到扇区的请求的平均大小；       \navgqu_sz: avg(average)qu(queue)-sz(size)   请求队列的平均大小；\n\nawait:  发送到设备的IO的平均时间，包括请求在队列中的等待时间和请求处理时间两部分。时间单位是毫秒(millisecond)      \nutil:  IO请求发送到设备占用的CPU时间。当这个值接近100%时，说明设备接近饱和。\n\n## netstat\n\n打印网络链接、路由表、接口统计等信息。\n\n**1、连接的协议类型**\n\n-t：TCP连接        \n-u: UDP连接\n\n**2、连接状态**\n\n-l: 只打印监听状态的连接；    \n-a: 显示所有状态的连接，默认不打印监听状态的连接。\n\n```\nshanno@taurus-p245 ~ $ netstat -tnp        ## 默认不打印监听端口\n    Proto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program name\n    tcp        0      0 192.168.1.104:44531     203.208.41.153:80       ESTABLISHED 13765/firefox   \nshanno@taurus-p245 ~ $ netstat -tnpl       ## 只打印监听端口 \n    Proto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program name\n    tcp        0      0 127.0.1.1:53            0.0.0.0:*               LISTEN      -               \n    tcp        0      0 127.0.0.1:3306          0.0.0.0:*               LISTEN      -               \nshanno@taurus-p245 ~ $ netstat -tnpa       ## 全打印，包括监听端口\n    Proto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program name\n    tcp        0      0 127.0.0.1:3306          0.0.0.0:*               LISTEN      -               \n    tcp        0      0 0.0.0.0:139             0.0.0.0:*               LISTEN      -               \n    tcp        0      0 192.168.1.104:44531     203.208.41.153:80       ESTABLISHED 13765/firefox   \n```\n\n**3、其它**\n\n-p: 进程\n-c: 每隔1秒钟持续输出连接状态\n-n: 以点分四段（例如192.168.0.1）的格式打印IP地址\n\n**4、用法**\n\na. 通过端口号查询进程号；\nb. 查询进程的端口号。\n\n## sar\n\n打印系统的活动信息，用于监视网络流量、磁盘IO等。\n\n**1、安装**\n\nsudo apt-get install sysstat \n\n**2、命令格式**\n\nsar [options] [interval [count]]\n\ninterval代表采样间隔，count代表采样次数。\n\n**3、监视网络流量**\n\n```\nshanno@taurus-p245 ~ $ sar -n DEV 3\n00时08分52秒     IFACE   rxpck/s   txpck/s    rxkB/s    txkB/s   rxcmp/s   txcmp/s  rxmcst/s   %ifutil\n00时08分55秒      eth0      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00\n00时08分55秒        lo      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00\n00时08分55秒     wlan0      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00\n00时08分55秒     IFACE   rxpck/s   txpck/s    rxkB/s    txkB/s   rxcmp/s   txcmp/s  rxmcst/s   %ifutil\n00时08分58秒      eth0      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00\n00时08分58秒        lo      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00\n00时08分58秒     wlan0      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00\nAverage:        IFACE   rxpck/s   txpck/s    rxkB/s    txkB/s   rxcmp/s   txcmp/s  rxmcst/s   %ifutil\nAverage:         eth0      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00\nAverage:           lo      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00\nAverage:        wlan0      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00\n```\n例子中-n代表network，它后面可以跟DEV，代表网络设备。\n\n**错误信息**       \n```\nroot@cvknode20221:~# date\nTue Sep  2 11:38:26 CST 2014\nroot@cvknode20221:~# sar -n DEV\nInvalid system activity file: /var/log/sysstat/sa02\n```\n\n**解决方法**     \n```\nroot@cvknode20221:~# sar -o 02\nroot@cvknode20221:~# ls /var/log/sysstat/\nsa02\nroot@cvknode20221:~# sar -n DEV\n11:38:46 AM     IFACE   rxpck/s   txpck/s    rxkB/s    txkB/s   rxcmp/s   txcmp/s  rxmcst/s\n11:38:48 AM     vnet4      0.00     45.50      0.00      8.82      0.00      0.00      0.00\n11:38:48 AM     vnet1      0.00     45.50      0.00      8.82      0.00      0.00      0.00\n11:38:48 AM      eth0     52.50      3.00      9.68      0.34      0.00      0.00      8.50\n11:38:48 AM      eth1      0.00      0.00      0.00      0.00      0.00      0.00      0.00\n```\n","source":"_posts/Linux/crontab-netstat-iostat-sar.md","raw":"title: \"Linux几个流量监控命令\"\ndate: 2014-08-29 17:35:27\ncategories: [Linux]\ntags:\n---\n\n总结几个监控IO流量的命令。\n\n<!--more-->\n\n## crontab\ncron可以设定在指定的时间运行任务。\n\n**1、查看定时任务**\n\n```\n[root@client1 ~]# crontab -l -u root\n*/1 * * * * date >> /root/1.txt\n[root@client1 ~]# cat /var/spool/cron/root \n*/1 * * * * date >> /root/1.txt\n```\n\n查看root用户的定时任务。上面的例子中只有一个定时任务，它每隔1分钟时间将当时的时间追加到*/root/1.txt*文件。实际上，定时任务被写入到*/var/spool/cron/*目录中。\n\n**2、编辑定时任务**\n```\ncrontab -e \n```\n编辑定时任务。\n\n```\n[root@client1 ~]# cat /etc/crontab \n# .---------------- minute (0 - 59)\n# |  .------------- hour (0 - 23)\n# |  |  .---------- day of month (1 - 31)\n# |  |  |  .------- month (1 - 12) OR jan,feb,mar,apr ...\n# |  |  |  |  .---- day of week (0 - 6) (Sunday=0 or 7) OR sun,mon,tue,wed,thu,fri,sat\n# |  |  |  |  |\n# *  *  *  *  * user-name command to be executed\n```\n定时任务的设置格式如上所示，它包含执行命令的时间以及命令的内容两部分内容。命令执行时间分成分、时、天、月以及星期5个部分。实际上，cron进程除了从*var/spool/cron*目录中读取定时任务外，还从*/etc/crontab*中读取。后者是针对系统的定时任务，因此命令执行时间和命令内容外还多了项用户。\n\n**3、cron进程**\n```\n[root@client1 ~]# /etc/init.d/crond restart\nStopping crond: [  OK  ]\nStarting crond: [  OK  ]\n```\n修改定时任务后，可以不重启cron进程。\n\n## iostat\n用于打印块设备、分区以及NFS的IO的统计信息。此外，还会打印CPU的统计信息。\n\n**1、用法**\n```\n[root@client1 ~]# iostat -d /dev/sda1\nDevice:            tps   Blk_read/s   Blk_wrtn/s   Blk_read   Blk_wrtn\nsda1              0.02         0.69         0.00      21232         32\n[root@client1 ~]# iostat -d /dev/sda1 -k\nDevice:            tps    kB_read/s    kB_wrtn/s    kB_read    kB_wrtn\nsda1              0.02         0.34         0.00      10616         16\n[root@client1 ~]# iostat -d -x /dev/sda1 10 2\nDevice:         rrqm/s   wrqm/s     r/s     w/s   rsec/s   wsec/s avgrq-sz avgqu-sz   await  svctm  %util\nsda1              0.01     0.00    0.02    0.00     0.77     0.00    32.41     0.00    0.68   0.52   0.00\nDevice:         rrqm/s   wrqm/s     r/s     w/s   rsec/s   wsec/s avgrq-sz avgqu-sz   await  svctm  %util\nsda1              0.00     0.00    0.00    0.00     0.00     0.00     0.00     0.00    0.00   0.00   0.00\n```\n例子打印了2次*/dev/sda1*块设备的IO统计信息，每次间隔为*10*秒钟。命令参数*-d*代表值输出设备的IO信息不打印CPU的统计信息；*-x*代表打印扩展信息; *-k*代表以kB_read的格式输出。\n\n**2、输出说明**\n\n*一般信息(读写数据量)*\n\ntps: 每秒发送到设备的transfer数目，transfer由多个逻辑的请求合并而成，它的大小是不确定的。\n\nBlk_read/s:  每秒从设备中读取的数据的大小，以块为单位。块大小和文件系统有关，一般为4K字节。        \nBlk_write/s: 每秒写入到设备的数据量，以块为单位。\n\nBlk_read: 从设备读取的Block数目；注意这不是每秒的平均值，而是整个测试过程。\nBlk_wrtn: 写入到设备的Block数目；\n\n*扩展信息(IO请求)*\n\nrrqm/s: r(read)rq(request)m(merge)  每秒合并的读请求数目;     \nwrqm/s: w(write)rq(request)m(merge) 每秒合并的写请求数目;\n\nr/s:  r(read)  每秒发送到设备的读请求数目;      \nw/s:  w(write) 每秒发送到设备的写请求数目；\n\nrsec/s:  r(read)sec(sector)  每秒读扇区的数目；       \nwsec/s:  w(write)sec(sector) 每秒写扇区的数目；\n\navgrq_sz: avg(average)rq(request)-sz(size) 发送到扇区的请求的平均大小；       \navgqu_sz: avg(average)qu(queue)-sz(size)   请求队列的平均大小；\n\nawait:  发送到设备的IO的平均时间，包括请求在队列中的等待时间和请求处理时间两部分。时间单位是毫秒(millisecond)      \nutil:  IO请求发送到设备占用的CPU时间。当这个值接近100%时，说明设备接近饱和。\n\n## netstat\n\n打印网络链接、路由表、接口统计等信息。\n\n**1、连接的协议类型**\n\n-t：TCP连接        \n-u: UDP连接\n\n**2、连接状态**\n\n-l: 只打印监听状态的连接；    \n-a: 显示所有状态的连接，默认不打印监听状态的连接。\n\n```\nshanno@taurus-p245 ~ $ netstat -tnp        ## 默认不打印监听端口\n    Proto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program name\n    tcp        0      0 192.168.1.104:44531     203.208.41.153:80       ESTABLISHED 13765/firefox   \nshanno@taurus-p245 ~ $ netstat -tnpl       ## 只打印监听端口 \n    Proto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program name\n    tcp        0      0 127.0.1.1:53            0.0.0.0:*               LISTEN      -               \n    tcp        0      0 127.0.0.1:3306          0.0.0.0:*               LISTEN      -               \nshanno@taurus-p245 ~ $ netstat -tnpa       ## 全打印，包括监听端口\n    Proto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program name\n    tcp        0      0 127.0.0.1:3306          0.0.0.0:*               LISTEN      -               \n    tcp        0      0 0.0.0.0:139             0.0.0.0:*               LISTEN      -               \n    tcp        0      0 192.168.1.104:44531     203.208.41.153:80       ESTABLISHED 13765/firefox   \n```\n\n**3、其它**\n\n-p: 进程\n-c: 每隔1秒钟持续输出连接状态\n-n: 以点分四段（例如192.168.0.1）的格式打印IP地址\n\n**4、用法**\n\na. 通过端口号查询进程号；\nb. 查询进程的端口号。\n\n## sar\n\n打印系统的活动信息，用于监视网络流量、磁盘IO等。\n\n**1、安装**\n\nsudo apt-get install sysstat \n\n**2、命令格式**\n\nsar [options] [interval [count]]\n\ninterval代表采样间隔，count代表采样次数。\n\n**3、监视网络流量**\n\n```\nshanno@taurus-p245 ~ $ sar -n DEV 3\n00时08分52秒     IFACE   rxpck/s   txpck/s    rxkB/s    txkB/s   rxcmp/s   txcmp/s  rxmcst/s   %ifutil\n00时08分55秒      eth0      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00\n00时08分55秒        lo      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00\n00时08分55秒     wlan0      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00\n00时08分55秒     IFACE   rxpck/s   txpck/s    rxkB/s    txkB/s   rxcmp/s   txcmp/s  rxmcst/s   %ifutil\n00时08分58秒      eth0      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00\n00时08分58秒        lo      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00\n00时08分58秒     wlan0      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00\nAverage:        IFACE   rxpck/s   txpck/s    rxkB/s    txkB/s   rxcmp/s   txcmp/s  rxmcst/s   %ifutil\nAverage:         eth0      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00\nAverage:           lo      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00\nAverage:        wlan0      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00\n```\n例子中-n代表network，它后面可以跟DEV，代表网络设备。\n\n**错误信息**       \n```\nroot@cvknode20221:~# date\nTue Sep  2 11:38:26 CST 2014\nroot@cvknode20221:~# sar -n DEV\nInvalid system activity file: /var/log/sysstat/sa02\n```\n\n**解决方法**     \n```\nroot@cvknode20221:~# sar -o 02\nroot@cvknode20221:~# ls /var/log/sysstat/\nsa02\nroot@cvknode20221:~# sar -n DEV\n11:38:46 AM     IFACE   rxpck/s   txpck/s    rxkB/s    txkB/s   rxcmp/s   txcmp/s  rxmcst/s\n11:38:48 AM     vnet4      0.00     45.50      0.00      8.82      0.00      0.00      0.00\n11:38:48 AM     vnet1      0.00     45.50      0.00      8.82      0.00      0.00      0.00\n11:38:48 AM      eth0     52.50      3.00      9.68      0.34      0.00      0.00      8.50\n11:38:48 AM      eth1      0.00      0.00      0.00      0.00      0.00      0.00      0.00\n```\n","slug":"Linux/crontab-netstat-iostat-sar","published":1,"updated":"2016-12-15T07:18:40.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cix1kgaui0018daeily4id7za","content":"<p>总结几个监控IO流量的命令。</p>\n<a id=\"more\"></a>\n<h2 id=\"crontab\"><a href=\"#crontab\" class=\"headerlink\" title=\"crontab\"></a>crontab</h2><p>cron可以设定在指定的时间运行任务。</p>\n<p><strong>1、查看定时任务</strong></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">[root@client1 ~]# crontab -l -u root</div><div class=\"line\">*/1 * * * * date &gt;&gt; /root/1.txt</div><div class=\"line\">[root@client1 ~]# cat /var/spool/cron/root </div><div class=\"line\">*/1 * * * * date &gt;&gt; /root/1.txt</div></pre></td></tr></table></figure>\n<p>查看root用户的定时任务。上面的例子中只有一个定时任务，它每隔1分钟时间将当时的时间追加到<em>/root/1.txt</em>文件。实际上，定时任务被写入到<em>/var/spool/cron/</em>目录中。</p>\n<p><strong>2、编辑定时任务</strong><br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">crontab -e</div></pre></td></tr></table></figure></p>\n<p>编辑定时任务。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">[root@client1 ~]# cat /etc/crontab </div><div class=\"line\"># .---------------- minute (0 - 59)</div><div class=\"line\"># |  .------------- hour (0 - 23)</div><div class=\"line\"># |  |  .---------- day of month (1 - 31)</div><div class=\"line\"># |  |  |  .------- month (1 - 12) OR jan,feb,mar,apr ...</div><div class=\"line\"># |  |  |  |  .---- day of week (0 - 6) (Sunday=0 or 7) OR sun,mon,tue,wed,thu,fri,sat</div><div class=\"line\"># |  |  |  |  |</div><div class=\"line\"># *  *  *  *  * user-name command to be executed</div></pre></td></tr></table></figure>\n<p>定时任务的设置格式如上所示，它包含执行命令的时间以及命令的内容两部分内容。命令执行时间分成分、时、天、月以及星期5个部分。实际上，cron进程除了从<em>var/spool/cron</em>目录中读取定时任务外，还从<em>/etc/crontab</em>中读取。后者是针对系统的定时任务，因此命令执行时间和命令内容外还多了项用户。</p>\n<p><strong>3、cron进程</strong><br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">[root@client1 ~]# /etc/init.d/crond restart</div><div class=\"line\">Stopping crond: [  OK  ]</div><div class=\"line\">Starting crond: [  OK  ]</div></pre></td></tr></table></figure></p>\n<p>修改定时任务后，可以不重启cron进程。</p>\n<h2 id=\"iostat\"><a href=\"#iostat\" class=\"headerlink\" title=\"iostat\"></a>iostat</h2><p>用于打印块设备、分区以及NFS的IO的统计信息。此外，还会打印CPU的统计信息。</p>\n<p><strong>1、用法</strong><br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">[root@client1 ~]# iostat -d /dev/sda1</div><div class=\"line\">Device:            tps   Blk_read/s   Blk_wrtn/s   Blk_read   Blk_wrtn</div><div class=\"line\">sda1              0.02         0.69         0.00      21232         32</div><div class=\"line\">[root@client1 ~]# iostat -d /dev/sda1 -k</div><div class=\"line\">Device:            tps    kB_read/s    kB_wrtn/s    kB_read    kB_wrtn</div><div class=\"line\">sda1              0.02         0.34         0.00      10616         16</div><div class=\"line\">[root@client1 ~]# iostat -d -x /dev/sda1 10 2</div><div class=\"line\">Device:         rrqm/s   wrqm/s     r/s     w/s   rsec/s   wsec/s avgrq-sz avgqu-sz   await  svctm  %util</div><div class=\"line\">sda1              0.01     0.00    0.02    0.00     0.77     0.00    32.41     0.00    0.68   0.52   0.00</div><div class=\"line\">Device:         rrqm/s   wrqm/s     r/s     w/s   rsec/s   wsec/s avgrq-sz avgqu-sz   await  svctm  %util</div><div class=\"line\">sda1              0.00     0.00    0.00    0.00     0.00     0.00     0.00     0.00    0.00   0.00   0.00</div></pre></td></tr></table></figure></p>\n<p>例子打印了2次<em>/dev/sda1</em>块设备的IO统计信息，每次间隔为<em>10</em>秒钟。命令参数<em>-d</em>代表值输出设备的IO信息不打印CPU的统计信息；<em>-x</em>代表打印扩展信息; <em>-k</em>代表以kB_read的格式输出。</p>\n<p><strong>2、输出说明</strong></p>\n<p><em>一般信息(读写数据量)</em></p>\n<p>tps: 每秒发送到设备的transfer数目，transfer由多个逻辑的请求合并而成，它的大小是不确定的。</p>\n<p>Blk_read/s:  每秒从设备中读取的数据的大小，以块为单位。块大小和文件系统有关，一般为4K字节。<br>Blk_write/s: 每秒写入到设备的数据量，以块为单位。</p>\n<p>Blk_read: 从设备读取的Block数目；注意这不是每秒的平均值，而是整个测试过程。<br>Blk_wrtn: 写入到设备的Block数目；</p>\n<p><em>扩展信息(IO请求)</em></p>\n<p>rrqm/s: r(read)rq(request)m(merge)  每秒合并的读请求数目;<br>wrqm/s: w(write)rq(request)m(merge) 每秒合并的写请求数目;</p>\n<p>r/s:  r(read)  每秒发送到设备的读请求数目;<br>w/s:  w(write) 每秒发送到设备的写请求数目；</p>\n<p>rsec/s:  r(read)sec(sector)  每秒读扇区的数目；<br>wsec/s:  w(write)sec(sector) 每秒写扇区的数目；</p>\n<p>avgrq_sz: avg(average)rq(request)-sz(size) 发送到扇区的请求的平均大小；<br>avgqu_sz: avg(average)qu(queue)-sz(size)   请求队列的平均大小；</p>\n<p>await:  发送到设备的IO的平均时间，包括请求在队列中的等待时间和请求处理时间两部分。时间单位是毫秒(millisecond)<br>util:  IO请求发送到设备占用的CPU时间。当这个值接近100%时，说明设备接近饱和。</p>\n<h2 id=\"netstat\"><a href=\"#netstat\" class=\"headerlink\" title=\"netstat\"></a>netstat</h2><p>打印网络链接、路由表、接口统计等信息。</p>\n<p><strong>1、连接的协议类型</strong></p>\n<p>-t：TCP连接<br>-u: UDP连接</p>\n<p><strong>2、连接状态</strong></p>\n<p>-l: 只打印监听状态的连接；<br>-a: 显示所有状态的连接，默认不打印监听状态的连接。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">shanno@taurus-p245 ~ $ netstat -tnp        ## 默认不打印监听端口</div><div class=\"line\">    Proto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program name</div><div class=\"line\">    tcp        0      0 192.168.1.104:44531     203.208.41.153:80       ESTABLISHED 13765/firefox   </div><div class=\"line\">shanno@taurus-p245 ~ $ netstat -tnpl       ## 只打印监听端口 </div><div class=\"line\">    Proto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program name</div><div class=\"line\">    tcp        0      0 127.0.1.1:53            0.0.0.0:*               LISTEN      -               </div><div class=\"line\">    tcp        0      0 127.0.0.1:3306          0.0.0.0:*               LISTEN      -               </div><div class=\"line\">shanno@taurus-p245 ~ $ netstat -tnpa       ## 全打印，包括监听端口</div><div class=\"line\">    Proto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program name</div><div class=\"line\">    tcp        0      0 127.0.0.1:3306          0.0.0.0:*               LISTEN      -               </div><div class=\"line\">    tcp        0      0 0.0.0.0:139             0.0.0.0:*               LISTEN      -               </div><div class=\"line\">    tcp        0      0 192.168.1.104:44531     203.208.41.153:80       ESTABLISHED 13765/firefox</div></pre></td></tr></table></figure>\n<p><strong>3、其它</strong></p>\n<p>-p: 进程<br>-c: 每隔1秒钟持续输出连接状态<br>-n: 以点分四段（例如192.168.0.1）的格式打印IP地址</p>\n<p><strong>4、用法</strong></p>\n<p>a. 通过端口号查询进程号；<br>b. 查询进程的端口号。</p>\n<h2 id=\"sar\"><a href=\"#sar\" class=\"headerlink\" title=\"sar\"></a>sar</h2><p>打印系统的活动信息，用于监视网络流量、磁盘IO等。</p>\n<p><strong>1、安装</strong></p>\n<p>sudo apt-get install sysstat </p>\n<p><strong>2、命令格式</strong></p>\n<p>sar [options] [interval [count]]</p>\n<p>interval代表采样间隔，count代表采样次数。</p>\n<p><strong>3、监视网络流量</strong></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">shanno@taurus-p245 ~ $ sar -n DEV 3</div><div class=\"line\">00时08分52秒     IFACE   rxpck/s   txpck/s    rxkB/s    txkB/s   rxcmp/s   txcmp/s  rxmcst/s   %ifutil</div><div class=\"line\">00时08分55秒      eth0      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00</div><div class=\"line\">00时08分55秒        lo      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00</div><div class=\"line\">00时08分55秒     wlan0      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00</div><div class=\"line\">00时08分55秒     IFACE   rxpck/s   txpck/s    rxkB/s    txkB/s   rxcmp/s   txcmp/s  rxmcst/s   %ifutil</div><div class=\"line\">00时08分58秒      eth0      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00</div><div class=\"line\">00时08分58秒        lo      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00</div><div class=\"line\">00时08分58秒     wlan0      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00</div><div class=\"line\">Average:        IFACE   rxpck/s   txpck/s    rxkB/s    txkB/s   rxcmp/s   txcmp/s  rxmcst/s   %ifutil</div><div class=\"line\">Average:         eth0      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00</div><div class=\"line\">Average:           lo      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00</div><div class=\"line\">Average:        wlan0      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00</div></pre></td></tr></table></figure>\n<p>例子中-n代表network，它后面可以跟DEV，代表网络设备。</p>\n<p><strong>错误信息</strong><br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">root@cvknode20221:~# date</div><div class=\"line\">Tue Sep  2 11:38:26 CST 2014</div><div class=\"line\">root@cvknode20221:~# sar -n DEV</div><div class=\"line\">Invalid system activity file: /var/log/sysstat/sa02</div></pre></td></tr></table></figure></p>\n<p><strong>解决方法</strong><br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">root@cvknode20221:~# sar -o 02</div><div class=\"line\">root@cvknode20221:~# ls /var/log/sysstat/</div><div class=\"line\">sa02</div><div class=\"line\">root@cvknode20221:~# sar -n DEV</div><div class=\"line\">11:38:46 AM     IFACE   rxpck/s   txpck/s    rxkB/s    txkB/s   rxcmp/s   txcmp/s  rxmcst/s</div><div class=\"line\">11:38:48 AM     vnet4      0.00     45.50      0.00      8.82      0.00      0.00      0.00</div><div class=\"line\">11:38:48 AM     vnet1      0.00     45.50      0.00      8.82      0.00      0.00      0.00</div><div class=\"line\">11:38:48 AM      eth0     52.50      3.00      9.68      0.34      0.00      0.00      8.50</div><div class=\"line\">11:38:48 AM      eth1      0.00      0.00      0.00      0.00      0.00      0.00      0.00</div></pre></td></tr></table></figure></p>\n<hr>\n<p><strong>版权声明</strong></p>\n<p>本作品采用<a href=\"https://creativecommons.org/licenses/by/2.5/cn/\" target=\"_blank\" rel=\"external\">知识共享署名2.5 中国大陆许可协议</a>进行许可，欢迎转载，但转载请注明来自<a href=\"https://wuxiangwei.cn/\"><em>Shanno’s Blog</em></a>，并保持转载后文章内容的完整。本人保留所有版权相关权利，侵权必究。</p>\n","excerpt":"<p>总结几个监控IO流量的命令。</p>","more":"<h2 id=\"crontab\"><a href=\"#crontab\" class=\"headerlink\" title=\"crontab\"></a>crontab</h2><p>cron可以设定在指定的时间运行任务。</p>\n<p><strong>1、查看定时任务</strong></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">[root@client1 ~]# crontab -l -u root</div><div class=\"line\">*/1 * * * * date &gt;&gt; /root/1.txt</div><div class=\"line\">[root@client1 ~]# cat /var/spool/cron/root </div><div class=\"line\">*/1 * * * * date &gt;&gt; /root/1.txt</div></pre></td></tr></table></figure>\n<p>查看root用户的定时任务。上面的例子中只有一个定时任务，它每隔1分钟时间将当时的时间追加到<em>/root/1.txt</em>文件。实际上，定时任务被写入到<em>/var/spool/cron/</em>目录中。</p>\n<p><strong>2、编辑定时任务</strong><br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">crontab -e</div></pre></td></tr></table></figure></p>\n<p>编辑定时任务。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">[root@client1 ~]# cat /etc/crontab </div><div class=\"line\"># .---------------- minute (0 - 59)</div><div class=\"line\"># |  .------------- hour (0 - 23)</div><div class=\"line\"># |  |  .---------- day of month (1 - 31)</div><div class=\"line\"># |  |  |  .------- month (1 - 12) OR jan,feb,mar,apr ...</div><div class=\"line\"># |  |  |  |  .---- day of week (0 - 6) (Sunday=0 or 7) OR sun,mon,tue,wed,thu,fri,sat</div><div class=\"line\"># |  |  |  |  |</div><div class=\"line\"># *  *  *  *  * user-name command to be executed</div></pre></td></tr></table></figure>\n<p>定时任务的设置格式如上所示，它包含执行命令的时间以及命令的内容两部分内容。命令执行时间分成分、时、天、月以及星期5个部分。实际上，cron进程除了从<em>var/spool/cron</em>目录中读取定时任务外，还从<em>/etc/crontab</em>中读取。后者是针对系统的定时任务，因此命令执行时间和命令内容外还多了项用户。</p>\n<p><strong>3、cron进程</strong><br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">[root@client1 ~]# /etc/init.d/crond restart</div><div class=\"line\">Stopping crond: [  OK  ]</div><div class=\"line\">Starting crond: [  OK  ]</div></pre></td></tr></table></figure></p>\n<p>修改定时任务后，可以不重启cron进程。</p>\n<h2 id=\"iostat\"><a href=\"#iostat\" class=\"headerlink\" title=\"iostat\"></a>iostat</h2><p>用于打印块设备、分区以及NFS的IO的统计信息。此外，还会打印CPU的统计信息。</p>\n<p><strong>1、用法</strong><br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">[root@client1 ~]# iostat -d /dev/sda1</div><div class=\"line\">Device:            tps   Blk_read/s   Blk_wrtn/s   Blk_read   Blk_wrtn</div><div class=\"line\">sda1              0.02         0.69         0.00      21232         32</div><div class=\"line\">[root@client1 ~]# iostat -d /dev/sda1 -k</div><div class=\"line\">Device:            tps    kB_read/s    kB_wrtn/s    kB_read    kB_wrtn</div><div class=\"line\">sda1              0.02         0.34         0.00      10616         16</div><div class=\"line\">[root@client1 ~]# iostat -d -x /dev/sda1 10 2</div><div class=\"line\">Device:         rrqm/s   wrqm/s     r/s     w/s   rsec/s   wsec/s avgrq-sz avgqu-sz   await  svctm  %util</div><div class=\"line\">sda1              0.01     0.00    0.02    0.00     0.77     0.00    32.41     0.00    0.68   0.52   0.00</div><div class=\"line\">Device:         rrqm/s   wrqm/s     r/s     w/s   rsec/s   wsec/s avgrq-sz avgqu-sz   await  svctm  %util</div><div class=\"line\">sda1              0.00     0.00    0.00    0.00     0.00     0.00     0.00     0.00    0.00   0.00   0.00</div></pre></td></tr></table></figure></p>\n<p>例子打印了2次<em>/dev/sda1</em>块设备的IO统计信息，每次间隔为<em>10</em>秒钟。命令参数<em>-d</em>代表值输出设备的IO信息不打印CPU的统计信息；<em>-x</em>代表打印扩展信息; <em>-k</em>代表以kB_read的格式输出。</p>\n<p><strong>2、输出说明</strong></p>\n<p><em>一般信息(读写数据量)</em></p>\n<p>tps: 每秒发送到设备的transfer数目，transfer由多个逻辑的请求合并而成，它的大小是不确定的。</p>\n<p>Blk_read/s:  每秒从设备中读取的数据的大小，以块为单位。块大小和文件系统有关，一般为4K字节。<br>Blk_write/s: 每秒写入到设备的数据量，以块为单位。</p>\n<p>Blk_read: 从设备读取的Block数目；注意这不是每秒的平均值，而是整个测试过程。<br>Blk_wrtn: 写入到设备的Block数目；</p>\n<p><em>扩展信息(IO请求)</em></p>\n<p>rrqm/s: r(read)rq(request)m(merge)  每秒合并的读请求数目;<br>wrqm/s: w(write)rq(request)m(merge) 每秒合并的写请求数目;</p>\n<p>r/s:  r(read)  每秒发送到设备的读请求数目;<br>w/s:  w(write) 每秒发送到设备的写请求数目；</p>\n<p>rsec/s:  r(read)sec(sector)  每秒读扇区的数目；<br>wsec/s:  w(write)sec(sector) 每秒写扇区的数目；</p>\n<p>avgrq_sz: avg(average)rq(request)-sz(size) 发送到扇区的请求的平均大小；<br>avgqu_sz: avg(average)qu(queue)-sz(size)   请求队列的平均大小；</p>\n<p>await:  发送到设备的IO的平均时间，包括请求在队列中的等待时间和请求处理时间两部分。时间单位是毫秒(millisecond)<br>util:  IO请求发送到设备占用的CPU时间。当这个值接近100%时，说明设备接近饱和。</p>\n<h2 id=\"netstat\"><a href=\"#netstat\" class=\"headerlink\" title=\"netstat\"></a>netstat</h2><p>打印网络链接、路由表、接口统计等信息。</p>\n<p><strong>1、连接的协议类型</strong></p>\n<p>-t：TCP连接<br>-u: UDP连接</p>\n<p><strong>2、连接状态</strong></p>\n<p>-l: 只打印监听状态的连接；<br>-a: 显示所有状态的连接，默认不打印监听状态的连接。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">shanno@taurus-p245 ~ $ netstat -tnp        ## 默认不打印监听端口</div><div class=\"line\">    Proto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program name</div><div class=\"line\">    tcp        0      0 192.168.1.104:44531     203.208.41.153:80       ESTABLISHED 13765/firefox   </div><div class=\"line\">shanno@taurus-p245 ~ $ netstat -tnpl       ## 只打印监听端口 </div><div class=\"line\">    Proto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program name</div><div class=\"line\">    tcp        0      0 127.0.1.1:53            0.0.0.0:*               LISTEN      -               </div><div class=\"line\">    tcp        0      0 127.0.0.1:3306          0.0.0.0:*               LISTEN      -               </div><div class=\"line\">shanno@taurus-p245 ~ $ netstat -tnpa       ## 全打印，包括监听端口</div><div class=\"line\">    Proto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program name</div><div class=\"line\">    tcp        0      0 127.0.0.1:3306          0.0.0.0:*               LISTEN      -               </div><div class=\"line\">    tcp        0      0 0.0.0.0:139             0.0.0.0:*               LISTEN      -               </div><div class=\"line\">    tcp        0      0 192.168.1.104:44531     203.208.41.153:80       ESTABLISHED 13765/firefox</div></pre></td></tr></table></figure>\n<p><strong>3、其它</strong></p>\n<p>-p: 进程<br>-c: 每隔1秒钟持续输出连接状态<br>-n: 以点分四段（例如192.168.0.1）的格式打印IP地址</p>\n<p><strong>4、用法</strong></p>\n<p>a. 通过端口号查询进程号；<br>b. 查询进程的端口号。</p>\n<h2 id=\"sar\"><a href=\"#sar\" class=\"headerlink\" title=\"sar\"></a>sar</h2><p>打印系统的活动信息，用于监视网络流量、磁盘IO等。</p>\n<p><strong>1、安装</strong></p>\n<p>sudo apt-get install sysstat </p>\n<p><strong>2、命令格式</strong></p>\n<p>sar [options] [interval [count]]</p>\n<p>interval代表采样间隔，count代表采样次数。</p>\n<p><strong>3、监视网络流量</strong></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">shanno@taurus-p245 ~ $ sar -n DEV 3</div><div class=\"line\">00时08分52秒     IFACE   rxpck/s   txpck/s    rxkB/s    txkB/s   rxcmp/s   txcmp/s  rxmcst/s   %ifutil</div><div class=\"line\">00时08分55秒      eth0      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00</div><div class=\"line\">00时08分55秒        lo      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00</div><div class=\"line\">00时08分55秒     wlan0      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00</div><div class=\"line\">00时08分55秒     IFACE   rxpck/s   txpck/s    rxkB/s    txkB/s   rxcmp/s   txcmp/s  rxmcst/s   %ifutil</div><div class=\"line\">00时08分58秒      eth0      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00</div><div class=\"line\">00时08分58秒        lo      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00</div><div class=\"line\">00时08分58秒     wlan0      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00</div><div class=\"line\">Average:        IFACE   rxpck/s   txpck/s    rxkB/s    txkB/s   rxcmp/s   txcmp/s  rxmcst/s   %ifutil</div><div class=\"line\">Average:         eth0      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00</div><div class=\"line\">Average:           lo      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00</div><div class=\"line\">Average:        wlan0      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00</div></pre></td></tr></table></figure>\n<p>例子中-n代表network，它后面可以跟DEV，代表网络设备。</p>\n<p><strong>错误信息</strong><br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">root@cvknode20221:~# date</div><div class=\"line\">Tue Sep  2 11:38:26 CST 2014</div><div class=\"line\">root@cvknode20221:~# sar -n DEV</div><div class=\"line\">Invalid system activity file: /var/log/sysstat/sa02</div></pre></td></tr></table></figure></p>\n<p><strong>解决方法</strong><br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">root@cvknode20221:~# sar -o 02</div><div class=\"line\">root@cvknode20221:~# ls /var/log/sysstat/</div><div class=\"line\">sa02</div><div class=\"line\">root@cvknode20221:~# sar -n DEV</div><div class=\"line\">11:38:46 AM     IFACE   rxpck/s   txpck/s    rxkB/s    txkB/s   rxcmp/s   txcmp/s  rxmcst/s</div><div class=\"line\">11:38:48 AM     vnet4      0.00     45.50      0.00      8.82      0.00      0.00      0.00</div><div class=\"line\">11:38:48 AM     vnet1      0.00     45.50      0.00      8.82      0.00      0.00      0.00</div><div class=\"line\">11:38:48 AM      eth0     52.50      3.00      9.68      0.34      0.00      0.00      8.50</div><div class=\"line\">11:38:48 AM      eth1      0.00      0.00      0.00      0.00      0.00      0.00      0.00</div></pre></td></tr></table></figure></p>\n<hr>\n<p><strong>版权声明</strong></p>\n<p>本作品采用<a href=\"https://creativecommons.org/licenses/by/2.5/cn/\">知识共享署名2.5 中国大陆许可协议</a>进行许可，欢迎转载，但转载请注明来自<a href=\"https://wuxiangwei.cn/\"><em>Shanno’s Blog</em></a>，并保持转载后文章内容的完整。本人保留所有版权相关权利，侵权必究。</p>"},{"title":"ctdb源码分析","date":"2016-10-13T02:48:26.000Z","toc":true,"_content":"\n需求：分析源码，了解ctdb运行机制，定位解决问题。\n\n<!--more-->\n\n## 消息机制\n\nctdb进程入口函数 /ctdb/server/ctdbd.c 文件的main函数。\n\n### 选择1种消息机制\n\n```C\nmain();\nctdb_start_daemon();\ntevent_context_init();\ntevent_context_init_byname();\ntevent_backends; // 全局变量\n```\n\n``` shell\nroot@bs-dev:~/repo/samba# grep \"tevent_register_backend\" -R\nlib/tevent/tevent_poll.c:   return tevent_register_backend(\"poll\", &poll_event_ops);\nlib/tevent/tevent_poll.c:   return tevent_register_backend(\"poll_mt\", &poll_event_mt_ops);\nlib/tevent/tevent_standard.c:   return tevent_register_backend(\"standard\", &std_event_ops);\nlib/tevent/tevent_epoll.c:  return tevent_register_backend(\"epoll\", &epoll_event_ops);\n```\n全局变量 tevent_backends维护一堆消息通知的实现，包括poll、standard和epoll等，默认采用standard机制(实际上就是epoll机制)。\n\n```C\nstatic const struct tevent_ops epoll_event_ops = {\n     .context_init       = epoll_event_context_init,\n     .add_fd         = epoll_event_add_fd,\n     .set_fd_close_fn    = tevent_common_fd_set_close_fn,\n     .get_fd_flags       = tevent_common_fd_get_flags,\n     .set_fd_flags       = epoll_event_set_fd_flags,\n     .add_timer      = tevent_common_add_timer_v2,\n     .schedule_immediate = tevent_common_schedule_immediate,\n     .add_signal     = tevent_common_add_signal,\n     .loop_once      = epoll_event_loop_once,\n     .loop_wait      = tevent_common_loop_wait,\n };\n```\ntevent对epoll的封装函数\n\n### 初始化epoll\n\n监听端口，接受新连接，处理请求。\n\n```C\nctdb_start_daemon();\nfde = tevent_add_fd(\n    ctdb->ev,\n    ctdb,\n    ctdb->daemon.sd,  // 服务fd\n    TEVENT_FD_READ,\n    ctdb_accept_client, // 接受客户端新连接的请求\n    ctdb\n);\n\nctdb_daemon_read_cb(); // 处理请求数据\ndaemon_incoming_packet(); // 依据op，分发请求\n\n// 3种op类型的请求处理函数\ndaemon_request_call_from_client();      // 主要对数据库操作\ndaemon_request_message_from_client();\ndaemon_request_control_from_client();\n\n// 分发消息\ndaemon_request_message_from_client();\nctdb_request_message();\nctdb_dispatch_message();\n\n// \nmessage_list_db_fetch_parser(); \nctdb_client_set_message_handler();\nctdb_register_message_handler();\nstruct ctdb_message_list {\n    struct ctdb_message_list *next, *prev;\n    struct ctdb_message_list_header *h;\n    ctdb_msg_fn_t message_handler;  // 请求处理函数\n    void *message_private;\n};\n\n\n// srvid 消息编号，例如\n// ctdb/include/ctdb_protocol.h\n#define CTDB_SRVID_TAKEOVER_RUN 0xFB00000000000000LL\n```\n\n使用 *struct ctdb_client* 结构体维护客户端连接的相关信息。\n\n\n### 请求格式\n\n``` C\nstruct ctdb_req_message;\nstruct ctdb_req_header;\n```\n\n\n\n\n","source":"_posts/Linux/ctdb.md","raw":"---\ntitle: \"ctdb源码分析\"\ndate: 2016-10-13 10:48:26\ncategories: [Linux]\ntags: [Linux, ctdb]\ntoc: true\n---\n\n需求：分析源码，了解ctdb运行机制，定位解决问题。\n\n<!--more-->\n\n## 消息机制\n\nctdb进程入口函数 /ctdb/server/ctdbd.c 文件的main函数。\n\n### 选择1种消息机制\n\n```C\nmain();\nctdb_start_daemon();\ntevent_context_init();\ntevent_context_init_byname();\ntevent_backends; // 全局变量\n```\n\n``` shell\nroot@bs-dev:~/repo/samba# grep \"tevent_register_backend\" -R\nlib/tevent/tevent_poll.c:   return tevent_register_backend(\"poll\", &poll_event_ops);\nlib/tevent/tevent_poll.c:   return tevent_register_backend(\"poll_mt\", &poll_event_mt_ops);\nlib/tevent/tevent_standard.c:   return tevent_register_backend(\"standard\", &std_event_ops);\nlib/tevent/tevent_epoll.c:  return tevent_register_backend(\"epoll\", &epoll_event_ops);\n```\n全局变量 tevent_backends维护一堆消息通知的实现，包括poll、standard和epoll等，默认采用standard机制(实际上就是epoll机制)。\n\n```C\nstatic const struct tevent_ops epoll_event_ops = {\n     .context_init       = epoll_event_context_init,\n     .add_fd         = epoll_event_add_fd,\n     .set_fd_close_fn    = tevent_common_fd_set_close_fn,\n     .get_fd_flags       = tevent_common_fd_get_flags,\n     .set_fd_flags       = epoll_event_set_fd_flags,\n     .add_timer      = tevent_common_add_timer_v2,\n     .schedule_immediate = tevent_common_schedule_immediate,\n     .add_signal     = tevent_common_add_signal,\n     .loop_once      = epoll_event_loop_once,\n     .loop_wait      = tevent_common_loop_wait,\n };\n```\ntevent对epoll的封装函数\n\n### 初始化epoll\n\n监听端口，接受新连接，处理请求。\n\n```C\nctdb_start_daemon();\nfde = tevent_add_fd(\n    ctdb->ev,\n    ctdb,\n    ctdb->daemon.sd,  // 服务fd\n    TEVENT_FD_READ,\n    ctdb_accept_client, // 接受客户端新连接的请求\n    ctdb\n);\n\nctdb_daemon_read_cb(); // 处理请求数据\ndaemon_incoming_packet(); // 依据op，分发请求\n\n// 3种op类型的请求处理函数\ndaemon_request_call_from_client();      // 主要对数据库操作\ndaemon_request_message_from_client();\ndaemon_request_control_from_client();\n\n// 分发消息\ndaemon_request_message_from_client();\nctdb_request_message();\nctdb_dispatch_message();\n\n// \nmessage_list_db_fetch_parser(); \nctdb_client_set_message_handler();\nctdb_register_message_handler();\nstruct ctdb_message_list {\n    struct ctdb_message_list *next, *prev;\n    struct ctdb_message_list_header *h;\n    ctdb_msg_fn_t message_handler;  // 请求处理函数\n    void *message_private;\n};\n\n\n// srvid 消息编号，例如\n// ctdb/include/ctdb_protocol.h\n#define CTDB_SRVID_TAKEOVER_RUN 0xFB00000000000000LL\n```\n\n使用 *struct ctdb_client* 结构体维护客户端连接的相关信息。\n\n\n### 请求格式\n\n``` C\nstruct ctdb_req_message;\nstruct ctdb_req_header;\n```\n\n\n\n\n","slug":"Linux/ctdb","published":1,"updated":"2016-12-15T07:18:40.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cix1kgaul001bdaei9rva3fi4","content":"<p>需求：分析源码，了解ctdb运行机制，定位解决问题。</p>\n<a id=\"more\"></a>\n<h2 id=\"消息机制\"><a href=\"#消息机制\" class=\"headerlink\" title=\"消息机制\"></a>消息机制</h2><p>ctdb进程入口函数 /ctdb/server/ctdbd.c 文件的main函数。</p>\n<h3 id=\"选择1种消息机制\"><a href=\"#选择1种消息机制\" class=\"headerlink\" title=\"选择1种消息机制\"></a>选择1种消息机制</h3><figure class=\"highlight c\"><table><tr><td class=\"code\"><pre><div class=\"line\">main();</div><div class=\"line\">ctdb_start_daemon();</div><div class=\"line\">tevent_context_init();</div><div class=\"line\">tevent_context_init_byname();</div><div class=\"line\">tevent_backends; <span class=\"comment\">// 全局变量</span></div></pre></td></tr></table></figure>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">root@bs-dev:~/repo/samba# grep &quot;tevent_register_backend&quot; -R</div><div class=\"line\">lib/tevent/tevent_poll.c:   return tevent_register_backend(&quot;poll&quot;, &amp;poll_event_ops);</div><div class=\"line\">lib/tevent/tevent_poll.c:   return tevent_register_backend(&quot;poll_mt&quot;, &amp;poll_event_mt_ops);</div><div class=\"line\">lib/tevent/tevent_standard.c:   return tevent_register_backend(&quot;standard&quot;, &amp;std_event_ops);</div><div class=\"line\">lib/tevent/tevent_epoll.c:  return tevent_register_backend(&quot;epoll&quot;, &amp;epoll_event_ops);</div></pre></td></tr></table></figure>\n<p>全局变量 tevent_backends维护一堆消息通知的实现，包括poll、standard和epoll等，默认采用standard机制(实际上就是epoll机制)。</p>\n<figure class=\"highlight c\"><table><tr><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">static</span> <span class=\"keyword\">const</span> <span class=\"keyword\">struct</span> tevent_ops epoll_event_ops = &#123;</div><div class=\"line\">     .context_init       = epoll_event_context_init,</div><div class=\"line\">     .add_fd         = epoll_event_add_fd,</div><div class=\"line\">     .set_fd_close_fn    = tevent_common_fd_set_close_fn,</div><div class=\"line\">     .get_fd_flags       = tevent_common_fd_get_flags,</div><div class=\"line\">     .set_fd_flags       = epoll_event_set_fd_flags,</div><div class=\"line\">     .add_timer      = tevent_common_add_timer_v2,</div><div class=\"line\">     .schedule_immediate = tevent_common_schedule_immediate,</div><div class=\"line\">     .add_signal     = tevent_common_add_signal,</div><div class=\"line\">     .loop_once      = epoll_event_loop_once,</div><div class=\"line\">     .loop_wait      = tevent_common_loop_wait,</div><div class=\"line\"> &#125;;</div></pre></td></tr></table></figure>\n<p>tevent对epoll的封装函数</p>\n<h3 id=\"初始化epoll\"><a href=\"#初始化epoll\" class=\"headerlink\" title=\"初始化epoll\"></a>初始化epoll</h3><p>监听端口，接受新连接，处理请求。</p>\n<figure class=\"highlight c\"><table><tr><td class=\"code\"><pre><div class=\"line\">ctdb_start_daemon();</div><div class=\"line\">fde = tevent_add_fd(</div><div class=\"line\">    ctdb-&gt;ev,</div><div class=\"line\">    ctdb,</div><div class=\"line\">    ctdb-&gt;daemon.sd,  <span class=\"comment\">// 服务fd</span></div><div class=\"line\">    TEVENT_FD_READ,</div><div class=\"line\">    ctdb_accept_client, <span class=\"comment\">// 接受客户端新连接的请求</span></div><div class=\"line\">    ctdb</div><div class=\"line\">);</div><div class=\"line\"></div><div class=\"line\">ctdb_daemon_read_cb(); <span class=\"comment\">// 处理请求数据</span></div><div class=\"line\">daemon_incoming_packet(); <span class=\"comment\">// 依据op，分发请求</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">// 3种op类型的请求处理函数</span></div><div class=\"line\">daemon_request_call_from_client();      <span class=\"comment\">// 主要对数据库操作</span></div><div class=\"line\">daemon_request_message_from_client();</div><div class=\"line\">daemon_request_control_from_client();</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">// 分发消息</span></div><div class=\"line\">daemon_request_message_from_client();</div><div class=\"line\">ctdb_request_message();</div><div class=\"line\">ctdb_dispatch_message();</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">// </span></div><div class=\"line\">message_list_db_fetch_parser(); </div><div class=\"line\">ctdb_client_set_message_handler();</div><div class=\"line\">ctdb_register_message_handler();</div><div class=\"line\"><span class=\"keyword\">struct</span> ctdb_message_list &#123;</div><div class=\"line\">    <span class=\"keyword\">struct</span> ctdb_message_list *next, *prev;</div><div class=\"line\">    <span class=\"keyword\">struct</span> ctdb_message_list_header *h;</div><div class=\"line\">    <span class=\"keyword\">ctdb_msg_fn_t</span> message_handler;  <span class=\"comment\">// 请求处理函数</span></div><div class=\"line\">    <span class=\"keyword\">void</span> *message_private;</div><div class=\"line\">&#125;;</div><div class=\"line\"></div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">// srvid 消息编号，例如</span></div><div class=\"line\"><span class=\"comment\">// ctdb/include/ctdb_protocol.h</span></div><div class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">define</span> CTDB_SRVID_TAKEOVER_RUN 0xFB00000000000000LL</span></div></pre></td></tr></table></figure>\n<p>使用 <em>struct ctdb_client</em> 结构体维护客户端连接的相关信息。</p>\n<h3 id=\"请求格式\"><a href=\"#请求格式\" class=\"headerlink\" title=\"请求格式\"></a>请求格式</h3><figure class=\"highlight c\"><table><tr><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">struct</span> ctdb_req_message;</div><div class=\"line\"><span class=\"keyword\">struct</span> ctdb_req_header;</div></pre></td></tr></table></figure>\n<hr>\n<p><strong>版权声明</strong></p>\n<p>本作品采用<a href=\"https://creativecommons.org/licenses/by/2.5/cn/\" target=\"_blank\" rel=\"external\">知识共享署名2.5 中国大陆许可协议</a>进行许可，欢迎转载，但转载请注明来自<a href=\"https://wuxiangwei.cn/\"><em>Shanno’s Blog</em></a>，并保持转载后文章内容的完整。本人保留所有版权相关权利，侵权必究。</p>\n","excerpt":"<p>需求：分析源码，了解ctdb运行机制，定位解决问题。</p>","more":"<h2 id=\"消息机制\"><a href=\"#消息机制\" class=\"headerlink\" title=\"消息机制\"></a>消息机制</h2><p>ctdb进程入口函数 /ctdb/server/ctdbd.c 文件的main函数。</p>\n<h3 id=\"选择1种消息机制\"><a href=\"#选择1种消息机制\" class=\"headerlink\" title=\"选择1种消息机制\"></a>选择1种消息机制</h3><figure class=\"highlight c\"><table><tr><td class=\"code\"><pre><div class=\"line\">main();</div><div class=\"line\">ctdb_start_daemon();</div><div class=\"line\">tevent_context_init();</div><div class=\"line\">tevent_context_init_byname();</div><div class=\"line\">tevent_backends; <span class=\"comment\">// 全局变量</span></div></pre></td></tr></table></figure>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">root@bs-dev:~/repo/samba# grep &quot;tevent_register_backend&quot; -R</div><div class=\"line\">lib/tevent/tevent_poll.c:   return tevent_register_backend(&quot;poll&quot;, &amp;poll_event_ops);</div><div class=\"line\">lib/tevent/tevent_poll.c:   return tevent_register_backend(&quot;poll_mt&quot;, &amp;poll_event_mt_ops);</div><div class=\"line\">lib/tevent/tevent_standard.c:   return tevent_register_backend(&quot;standard&quot;, &amp;std_event_ops);</div><div class=\"line\">lib/tevent/tevent_epoll.c:  return tevent_register_backend(&quot;epoll&quot;, &amp;epoll_event_ops);</div></pre></td></tr></table></figure>\n<p>全局变量 tevent_backends维护一堆消息通知的实现，包括poll、standard和epoll等，默认采用standard机制(实际上就是epoll机制)。</p>\n<figure class=\"highlight c\"><table><tr><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">static</span> <span class=\"keyword\">const</span> <span class=\"keyword\">struct</span> tevent_ops epoll_event_ops = &#123;</div><div class=\"line\">     .context_init       = epoll_event_context_init,</div><div class=\"line\">     .add_fd         = epoll_event_add_fd,</div><div class=\"line\">     .set_fd_close_fn    = tevent_common_fd_set_close_fn,</div><div class=\"line\">     .get_fd_flags       = tevent_common_fd_get_flags,</div><div class=\"line\">     .set_fd_flags       = epoll_event_set_fd_flags,</div><div class=\"line\">     .add_timer      = tevent_common_add_timer_v2,</div><div class=\"line\">     .schedule_immediate = tevent_common_schedule_immediate,</div><div class=\"line\">     .add_signal     = tevent_common_add_signal,</div><div class=\"line\">     .loop_once      = epoll_event_loop_once,</div><div class=\"line\">     .loop_wait      = tevent_common_loop_wait,</div><div class=\"line\"> &#125;;</div></pre></td></tr></table></figure>\n<p>tevent对epoll的封装函数</p>\n<h3 id=\"初始化epoll\"><a href=\"#初始化epoll\" class=\"headerlink\" title=\"初始化epoll\"></a>初始化epoll</h3><p>监听端口，接受新连接，处理请求。</p>\n<figure class=\"highlight c\"><table><tr><td class=\"code\"><pre><div class=\"line\">ctdb_start_daemon();</div><div class=\"line\">fde = tevent_add_fd(</div><div class=\"line\">    ctdb-&gt;ev,</div><div class=\"line\">    ctdb,</div><div class=\"line\">    ctdb-&gt;daemon.sd,  <span class=\"comment\">// 服务fd</span></div><div class=\"line\">    TEVENT_FD_READ,</div><div class=\"line\">    ctdb_accept_client, <span class=\"comment\">// 接受客户端新连接的请求</span></div><div class=\"line\">    ctdb</div><div class=\"line\">);</div><div class=\"line\"></div><div class=\"line\">ctdb_daemon_read_cb(); <span class=\"comment\">// 处理请求数据</span></div><div class=\"line\">daemon_incoming_packet(); <span class=\"comment\">// 依据op，分发请求</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">// 3种op类型的请求处理函数</span></div><div class=\"line\">daemon_request_call_from_client();      <span class=\"comment\">// 主要对数据库操作</span></div><div class=\"line\">daemon_request_message_from_client();</div><div class=\"line\">daemon_request_control_from_client();</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">// 分发消息</span></div><div class=\"line\">daemon_request_message_from_client();</div><div class=\"line\">ctdb_request_message();</div><div class=\"line\">ctdb_dispatch_message();</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">// </span></div><div class=\"line\">message_list_db_fetch_parser(); </div><div class=\"line\">ctdb_client_set_message_handler();</div><div class=\"line\">ctdb_register_message_handler();</div><div class=\"line\"><span class=\"keyword\">struct</span> ctdb_message_list &#123;</div><div class=\"line\">    <span class=\"keyword\">struct</span> ctdb_message_list *next, *prev;</div><div class=\"line\">    <span class=\"keyword\">struct</span> ctdb_message_list_header *h;</div><div class=\"line\">    <span class=\"keyword\">ctdb_msg_fn_t</span> message_handler;  <span class=\"comment\">// 请求处理函数</span></div><div class=\"line\">    <span class=\"keyword\">void</span> *message_private;</div><div class=\"line\">&#125;;</div><div class=\"line\"></div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">// srvid 消息编号，例如</span></div><div class=\"line\"><span class=\"comment\">// ctdb/include/ctdb_protocol.h</span></div><div class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">define</span> CTDB_SRVID_TAKEOVER_RUN 0xFB00000000000000LL</span></div></pre></td></tr></table></figure>\n<p>使用 <em>struct ctdb_client</em> 结构体维护客户端连接的相关信息。</p>\n<h3 id=\"请求格式\"><a href=\"#请求格式\" class=\"headerlink\" title=\"请求格式\"></a>请求格式</h3><figure class=\"highlight c\"><table><tr><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">struct</span> ctdb_req_message;</div><div class=\"line\"><span class=\"keyword\">struct</span> ctdb_req_header;</div></pre></td></tr></table></figure>\n<hr>\n<p><strong>版权声明</strong></p>\n<p>本作品采用<a href=\"https://creativecommons.org/licenses/by/2.5/cn/\">知识共享署名2.5 中国大陆许可协议</a>进行许可，欢迎转载，但转载请注明来自<a href=\"https://wuxiangwei.cn/\"><em>Shanno’s Blog</em></a>，并保持转载后文章内容的完整。本人保留所有版权相关权利，侵权必究。</p>"},{"title":"fdisk磁盘分区","date":"2014-09-15T10:26:31.000Z","_content":"\nfdisk命令能够用来对磁盘进行分区，查看磁盘的分区信息。\n\n<!--more-->\n\n#### 查看分区信息\n\n分区信息包括容量、扇区数目、柱面数目、磁头数目和IO大小等信息。\n```\nroot@cvm:/# fdisk -l /dev/sda7\nDisk /dev/sda7: 441.8 GB, 441752485888 bytes\n255 heads, 63 sectors/track, 53706 cylinders, total 862797824 sectors\nUnits = sectors of 1 * 512 = 512 bytes\nSector size (logical/physical): 512 bytes / 512 bytes\nI/O size (minimum/optimal): 512 bytes / 512 bytes\nDisk identifier: 0x00000000\n```\n\n磁盘的CHS编址。C代表柱面数目，也就是磁道数目。H代表磁头数目，也就是盘片数目；S代表每磁道的扇区数目。 \n\n总扇区数目 = 磁头数目 x 每个磁道的扇区数目 x 磁道数目:             \n255 * 63 * 53706 => 862786890。\n\n磁盘容量的计算：扇区大小 x 扇区数目:             \n512 * 862797824 => 441752485888 字节\n\n#### 划分分区\n\n*/dev/sd*代表SCSI设备，*/dev/hd*代表IDE设备。 sda1代表sda盘的一个分区。\n```\nroot@cvm:/# fdisk /dev/sda\n```\n使用fdisk命令对sda重新分区。\n\n```\nCommand (m for help): d \nPartition number (1-7): 7\n```\n删除sda盘的sda7分区。\n\n```\nCommand (m for help): n\nPartition type:\n   p   primary (1 primary, 1 extended, 2 free)\n   l   logical (numbered from 5)\nSelect (default p): l\nAdding logical partition 7\nFirst sector (113973248-976771071, default 113973248): \nUsing default value 113973248\nLast sector, +sectors or +size{K,M,G} (113973248-976771071, default 976771071): +150G  \n```\n创建新分区，分区编号为sda7，分区大小为150G。\n\n```\nCommand (m for help): w\nThe partition table has been altered!\n```\n退出fdisk命令之前，通过*w*命令将分区写入磁盘。\n\n```\nroot@cvm:/# fdisk -l\n   Device Boot      Start         End      Blocks   Id  System\n/dev/sda1   *        2048    58593279    29295616   83  Linux\n/dev/sda2        58595326   976771071   459087873    5  Extended\n/dev/sda5        58595328    97654783    19529728   83  Linux\n/dev/sda6        97656832   113971199     8157184   82  Linux swap / Solaris\n/dev/sda7       113973248   428546047   157286400   83  Linux\n```\n查看新分区sda7，它的大小为 *(428546047 - 113973248) \\* 512 / (1024 \\* 1024 \\* 1024) => 150G*。\n\n\n\n","source":"_posts/Linux/fdisk.md","raw":"title: \"fdisk磁盘分区\"\ndate: 2014-09-15 18:26:31\ncategories: [Linux]\ntags: \n---\n\nfdisk命令能够用来对磁盘进行分区，查看磁盘的分区信息。\n\n<!--more-->\n\n#### 查看分区信息\n\n分区信息包括容量、扇区数目、柱面数目、磁头数目和IO大小等信息。\n```\nroot@cvm:/# fdisk -l /dev/sda7\nDisk /dev/sda7: 441.8 GB, 441752485888 bytes\n255 heads, 63 sectors/track, 53706 cylinders, total 862797824 sectors\nUnits = sectors of 1 * 512 = 512 bytes\nSector size (logical/physical): 512 bytes / 512 bytes\nI/O size (minimum/optimal): 512 bytes / 512 bytes\nDisk identifier: 0x00000000\n```\n\n磁盘的CHS编址。C代表柱面数目，也就是磁道数目。H代表磁头数目，也就是盘片数目；S代表每磁道的扇区数目。 \n\n总扇区数目 = 磁头数目 x 每个磁道的扇区数目 x 磁道数目:             \n255 * 63 * 53706 => 862786890。\n\n磁盘容量的计算：扇区大小 x 扇区数目:             \n512 * 862797824 => 441752485888 字节\n\n#### 划分分区\n\n*/dev/sd*代表SCSI设备，*/dev/hd*代表IDE设备。 sda1代表sda盘的一个分区。\n```\nroot@cvm:/# fdisk /dev/sda\n```\n使用fdisk命令对sda重新分区。\n\n```\nCommand (m for help): d \nPartition number (1-7): 7\n```\n删除sda盘的sda7分区。\n\n```\nCommand (m for help): n\nPartition type:\n   p   primary (1 primary, 1 extended, 2 free)\n   l   logical (numbered from 5)\nSelect (default p): l\nAdding logical partition 7\nFirst sector (113973248-976771071, default 113973248): \nUsing default value 113973248\nLast sector, +sectors or +size{K,M,G} (113973248-976771071, default 976771071): +150G  \n```\n创建新分区，分区编号为sda7，分区大小为150G。\n\n```\nCommand (m for help): w\nThe partition table has been altered!\n```\n退出fdisk命令之前，通过*w*命令将分区写入磁盘。\n\n```\nroot@cvm:/# fdisk -l\n   Device Boot      Start         End      Blocks   Id  System\n/dev/sda1   *        2048    58593279    29295616   83  Linux\n/dev/sda2        58595326   976771071   459087873    5  Extended\n/dev/sda5        58595328    97654783    19529728   83  Linux\n/dev/sda6        97656832   113971199     8157184   82  Linux swap / Solaris\n/dev/sda7       113973248   428546047   157286400   83  Linux\n```\n查看新分区sda7，它的大小为 *(428546047 - 113973248) \\* 512 / (1024 \\* 1024 \\* 1024) => 150G*。\n\n\n\n","slug":"Linux/fdisk","published":1,"updated":"2016-12-15T07:18:40.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cix1kgaun001hdaei9qzzn8ro","content":"<p>fdisk命令能够用来对磁盘进行分区，查看磁盘的分区信息。</p>\n<a id=\"more\"></a>\n<h4 id=\"查看分区信息\"><a href=\"#查看分区信息\" class=\"headerlink\" title=\"查看分区信息\"></a>查看分区信息</h4><p>分区信息包括容量、扇区数目、柱面数目、磁头数目和IO大小等信息。<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">root@cvm:/# fdisk -l /dev/sda7</div><div class=\"line\">Disk /dev/sda7: 441.8 GB, 441752485888 bytes</div><div class=\"line\">255 heads, 63 sectors/track, 53706 cylinders, total 862797824 sectors</div><div class=\"line\">Units = sectors of 1 * 512 = 512 bytes</div><div class=\"line\">Sector size (logical/physical): 512 bytes / 512 bytes</div><div class=\"line\">I/O size (minimum/optimal): 512 bytes / 512 bytes</div><div class=\"line\">Disk identifier: 0x00000000</div></pre></td></tr></table></figure></p>\n<p>磁盘的CHS编址。C代表柱面数目，也就是磁道数目。H代表磁头数目，也就是盘片数目；S代表每磁道的扇区数目。 </p>\n<p>总扇区数目 = 磁头数目 x 每个磁道的扇区数目 x 磁道数目:<br>255 <em> 63 </em> 53706 =&gt; 862786890。</p>\n<p>磁盘容量的计算：扇区大小 x 扇区数目:<br>512 * 862797824 =&gt; 441752485888 字节</p>\n<h4 id=\"划分分区\"><a href=\"#划分分区\" class=\"headerlink\" title=\"划分分区\"></a>划分分区</h4><p><em>/dev/sd</em>代表SCSI设备，<em>/dev/hd</em>代表IDE设备。 sda1代表sda盘的一个分区。<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">root@cvm:/# fdisk /dev/sda</div></pre></td></tr></table></figure></p>\n<p>使用fdisk命令对sda重新分区。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">Command (m for help): d </div><div class=\"line\">Partition number (1-7): 7</div></pre></td></tr></table></figure>\n<p>删除sda盘的sda7分区。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">Command (m for help): n</div><div class=\"line\">Partition type:</div><div class=\"line\">   p   primary (1 primary, 1 extended, 2 free)</div><div class=\"line\">   l   logical (numbered from 5)</div><div class=\"line\">Select (default p): l</div><div class=\"line\">Adding logical partition 7</div><div class=\"line\">First sector (113973248-976771071, default 113973248): </div><div class=\"line\">Using default value 113973248</div><div class=\"line\">Last sector, +sectors or +size&#123;K,M,G&#125; (113973248-976771071, default 976771071): +150G</div></pre></td></tr></table></figure>\n<p>创建新分区，分区编号为sda7，分区大小为150G。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">Command (m for help): w</div><div class=\"line\">The partition table has been altered!</div></pre></td></tr></table></figure>\n<p>退出fdisk命令之前，通过<em>w</em>命令将分区写入磁盘。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">root@cvm:/# fdisk -l</div><div class=\"line\">   Device Boot      Start         End      Blocks   Id  System</div><div class=\"line\">/dev/sda1   *        2048    58593279    29295616   83  Linux</div><div class=\"line\">/dev/sda2        58595326   976771071   459087873    5  Extended</div><div class=\"line\">/dev/sda5        58595328    97654783    19529728   83  Linux</div><div class=\"line\">/dev/sda6        97656832   113971199     8157184   82  Linux swap / Solaris</div><div class=\"line\">/dev/sda7       113973248   428546047   157286400   83  Linux</div></pre></td></tr></table></figure>\n<p>查看新分区sda7，它的大小为 <em>(428546047 - 113973248) \\</em> 512 / (1024 * 1024 * 1024) =&gt; 150G*。</p>\n<hr>\n<p><strong>版权声明</strong></p>\n<p>本作品采用<a href=\"https://creativecommons.org/licenses/by/2.5/cn/\" target=\"_blank\" rel=\"external\">知识共享署名2.5 中国大陆许可协议</a>进行许可，欢迎转载，但转载请注明来自<a href=\"https://wuxiangwei.cn/\"><em>Shanno’s Blog</em></a>，并保持转载后文章内容的完整。本人保留所有版权相关权利，侵权必究。</p>\n","excerpt":"<p>fdisk命令能够用来对磁盘进行分区，查看磁盘的分区信息。</p>","more":"<h4 id=\"查看分区信息\"><a href=\"#查看分区信息\" class=\"headerlink\" title=\"查看分区信息\"></a>查看分区信息</h4><p>分区信息包括容量、扇区数目、柱面数目、磁头数目和IO大小等信息。<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">root@cvm:/# fdisk -l /dev/sda7</div><div class=\"line\">Disk /dev/sda7: 441.8 GB, 441752485888 bytes</div><div class=\"line\">255 heads, 63 sectors/track, 53706 cylinders, total 862797824 sectors</div><div class=\"line\">Units = sectors of 1 * 512 = 512 bytes</div><div class=\"line\">Sector size (logical/physical): 512 bytes / 512 bytes</div><div class=\"line\">I/O size (minimum/optimal): 512 bytes / 512 bytes</div><div class=\"line\">Disk identifier: 0x00000000</div></pre></td></tr></table></figure></p>\n<p>磁盘的CHS编址。C代表柱面数目，也就是磁道数目。H代表磁头数目，也就是盘片数目；S代表每磁道的扇区数目。 </p>\n<p>总扇区数目 = 磁头数目 x 每个磁道的扇区数目 x 磁道数目:<br>255 <em> 63 </em> 53706 =&gt; 862786890。</p>\n<p>磁盘容量的计算：扇区大小 x 扇区数目:<br>512 * 862797824 =&gt; 441752485888 字节</p>\n<h4 id=\"划分分区\"><a href=\"#划分分区\" class=\"headerlink\" title=\"划分分区\"></a>划分分区</h4><p><em>/dev/sd</em>代表SCSI设备，<em>/dev/hd</em>代表IDE设备。 sda1代表sda盘的一个分区。<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">root@cvm:/# fdisk /dev/sda</div></pre></td></tr></table></figure></p>\n<p>使用fdisk命令对sda重新分区。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">Command (m for help): d </div><div class=\"line\">Partition number (1-7): 7</div></pre></td></tr></table></figure>\n<p>删除sda盘的sda7分区。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">Command (m for help): n</div><div class=\"line\">Partition type:</div><div class=\"line\">   p   primary (1 primary, 1 extended, 2 free)</div><div class=\"line\">   l   logical (numbered from 5)</div><div class=\"line\">Select (default p): l</div><div class=\"line\">Adding logical partition 7</div><div class=\"line\">First sector (113973248-976771071, default 113973248): </div><div class=\"line\">Using default value 113973248</div><div class=\"line\">Last sector, +sectors or +size&#123;K,M,G&#125; (113973248-976771071, default 976771071): +150G</div></pre></td></tr></table></figure>\n<p>创建新分区，分区编号为sda7，分区大小为150G。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">Command (m for help): w</div><div class=\"line\">The partition table has been altered!</div></pre></td></tr></table></figure>\n<p>退出fdisk命令之前，通过<em>w</em>命令将分区写入磁盘。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">root@cvm:/# fdisk -l</div><div class=\"line\">   Device Boot      Start         End      Blocks   Id  System</div><div class=\"line\">/dev/sda1   *        2048    58593279    29295616   83  Linux</div><div class=\"line\">/dev/sda2        58595326   976771071   459087873    5  Extended</div><div class=\"line\">/dev/sda5        58595328    97654783    19529728   83  Linux</div><div class=\"line\">/dev/sda6        97656832   113971199     8157184   82  Linux swap / Solaris</div><div class=\"line\">/dev/sda7       113973248   428546047   157286400   83  Linux</div></pre></td></tr></table></figure>\n<p>查看新分区sda7，它的大小为 <em>(428546047 - 113973248) \\</em> 512 / (1024 * 1024 * 1024) =&gt; 150G*。</p>\n<hr>\n<p><strong>版权声明</strong></p>\n<p>本作品采用<a href=\"https://creativecommons.org/licenses/by/2.5/cn/\">知识共享署名2.5 中国大陆许可协议</a>进行许可，欢迎转载，但转载请注明来自<a href=\"https://wuxiangwei.cn/\"><em>Shanno’s Blog</em></a>，并保持转载后文章内容的完整。本人保留所有版权相关权利，侵权必究。</p>"},{"title":"NFS集成LDAP实现","date":"2016-11-01T05:31:21.000Z","toc":true,"_content":"\n          +---------+    \n          |         |        \n          |         |        \n          +---------+ LDAP服务器\n               ^\n               |            \n               |            \n          +---------+        \n          |         |        \n          |         |\n          +---------+ NAS机头\n               ^   (LDAP客户端+NFS服务端)\n               |\n               |\n          +---------+        \n          |         |        \n          |         |\n          +---------+ NFS客户端\n\nNFS服务不提供用户级别的权限控制，但能够提供主机级别的权限控制。也就是，允许或禁止给定的主机访问NFS导出的共享目录。LDAP能够提供集中式的主机管理，将一个或多个主机归为一组，称为网络组(netgroup)。NFS集成LDAP的目的，就是利用LDAP的网络组来按组限制主机对共享目录的访问。如上图所示，当NFS服务端接收到NFS客户端挂载共享目录的请求时，先向LDAP服务器询问目标客户机是否在允许的网络组内，如果目标客户机在允许的网络组内则允许挂载，否则禁止挂载。\n\n<!--more-->\n\n## 一个例子\n\n| 主机名 | IP地址 | 角色 | 说明 |\n|:--|:--|:--|:--|\n| BoreNode1 | 172.16.65.130 | LDAP服务器 | 提供sysadmin网络组|\n| BoreNode3 | 172.16.65.142 | LDAP客户端以及NFS服务器 | 提供NFS服务，并根据网络组检验客户机的合法性 |\n| BoreNode4 | 172.16.65.143 | NFS客户端 | 属于sysadmin网络组 | \n| BoreNode5 | 172.16.65.144 | NFS客户端 | **不**属于sysadmin网络组 | \n\n假设NFS允许访问的网络组的名字为sysadmin，上表给出了例子中将用到的4台主机的不同角色，下面逐个介绍不同角色的配置情况。\n\n### LDAP服务器\n\n#### 安装软件\n\n``` shell\nroot@BoreNode1:~# apt-get install slapd ldap-utils\n```\n安装结束通过*ps -ef | grep slap*命令检查slapd进程。\n\n#### 重新配置\n\n``` shell\nroot@BoreNode1:~# dpkg-reconfigure slapd\n```\n配置过程中的选择：\n\nOmit OpenLDAP server configuration? No\nDNS domain name: h3c.com\nOrganization name: onestor\nDatabase backend to use: BDB\nDo you want the database to be removed when slapd is purged? Yes\nMove old database? Yes\nAllow LDAPv2 protocol? No\n\n#### 基本操作\n\n``` shell\nldapsearch -x -b dc=h3c,dc=com -H ldap://127.0.0.1\n```\n查看目录树结构\n\n``` shell\nldapdelete -x -D cn=admin,dc=h3c,dc=com -W cn=sysadmin,ou=netgroup,dc=h3c,dc=com\n```\n删除目录树中的1个节点\n\n``` shell\nldapadd -x -D cn=admin,dc=h3c,dc=com -W -f add_content.ldif\n```\n从ldif文件添加目录结构\n\n\n#### 构建数据\n\n```\ndc=h3c,dc=com\n    |-- cn=admin\n    |-- ou=netgroup\n        |-- cn=sysadmin\n```\n目标是构建上面的目录结构，网络组sysadmin节点挂在netgroup部门下面。实际上，按照上述步骤配置LDAP服务器时已经建立了dc=h3c,dc=com和cn=admin,dc=h3c,dc=com两个节点。因此接下来只需要添加ou=netgroup,dc=h3c,dc=com和cn=sysadmin,ou=netgroup,dc=h3c,dc=com两个节点。\n\n```\ndn: ou=netgroup,dc=h3c,dc=com                                                                          \nobjectClass: organizationalUnit                                                                        \nou: netgroup                                                                                           \n\ndn: cn=sysadmin,ou=netgroup,dc=h3c,dc=com                                                              \nobjectClass: nisNetgroup                                                                               \nobjectClass: top                                                                                       \ncn: sysadmin                                                                                           \nnisNetgroupTriple: (BoreNode1,-,-)                                                                     \nnisNetgroupTriple: (BoreNode4,-,-)\n```\n准备内容如上的LDIF文件，将其添加到目录结构。简单介绍下和网络组相关几个schema：\n* nisNetgroup 代表一个网络组，必选cn属性，代表网络组的名称。网络组通常作为ou的子节点，本例中挂在ou=netgroup节点下\n* nisNetgroupTriple 为网络组的属性，它的值是个三元组(host, user, NIS-domain)，代表1台主机\n* memberNisNetgroup 为网络组的属性，代表网络组的子网络组\n\n简而言之，一个网络组可以由一组主机构成，也可以由若干网络组组合而成，还可以由网络组加主机构成。nisNetgroupTriple描述主机，memberNisNetgroup描述子网络组。此外，关于这些预定义schema的更详细内容可以在/etc/ldap/schema目录中找到。\n\n### LDAP客户端\n\n#### 安装软件\n\n``` shell\nroot@BoreNode3:~# apt-get install ldap-utils libpam-ldap libnss-ldap nslcd\n```\n安装过程提示的配置：\n\nLDAP server Uniform Resource Identifier: ldap://172.16.65.130/\nDistinguished name of the search base: dc=h3c,dc=com\nLDAP version to use: 3\nMake local root Database admin: Yes\nDoes the LDAP database require login? No\nLDAP account for root: cn=admin,dc=h3c,dc=com\nLDAP server URI: ldap://172.16.65.130/\nLDAP server search base: dc=h3c,dc=com\n\n\n#### 认证方式\n\n**修改nsswitch**\n\n``` shell\nroot@BoreNode5:~# auth-client-config -t nss -p lac_ldap\n```\n执行上述命令的主要作用是修改/etc/nsswitch.conf文件，下表列出了命令执行前后的该文件的差异。\n\n| 认证项 | 命令前 | 命令后 |\n|:--|:--|:--|\n| passwd | compat | files ldap |\n| group | compat | files ldap |\n| shadow | compat | files ldap |\n| netgroup | nis  | ldap |\n\n**注意** 上表最后1行并非由auth-client-config命令自动修改，而是手动修改的。\n\nnsswitch是名字服务的开关，决定了查询名字的顺序。以查询passwd为例，修改后将先在本地查找，然后再到ldap中查找。如果删除“files ldap”中的files，那么查询用户时将只从ldap中查找。\n\n``` shell\nroot@BoreNode3:~# getent netgroup sysadmin\nsysadmin              (BoreNode1,-,-) (BoreNode4,-,-)\n```\n**getent**命令提供了查询各种名字的功能，例如getent passwd用于查询用户，getent netgroup用于查询网络组。但查询网路组时必须提供网络组的名字，因为getent不具备枚举网络组的功能。\n\n\n**修改slap**\n\n```\nnss_base_netgroup   ou=netgroup,dc=h3c,dc=com?one\n```\n修改/etc/sldap.conf文件，添加上面的配置。\n\n\n### NFS服务端\n\n**修改hosts文件**\n\n``` shell\nroot@BoreNode3:~# cat /etc/hosts\n127.0.0.1\tlocalhost\n172.16.65.130    BoreNode1\n172.16.65.142    BoreNode3\n172.16.65.143    BoreNode4\n172.16.65.144    BoreNode5\n```\n\n**修改/etc/exports文件**\n\n```\n/root/nfs_1     @sysadmin(rw,sync,no_subtree_check)\n```\n\n\n### NFS客户端\n\n### 主机名问题\n\n问题：如果NFS服务端的/etc/hosts文件不增加网络组的主机的话，那么即使目标客户机属于给定的网络组也无法访问NFS的共享目录。\n\n```\ndc=h3c,dc=com\n    |-- ou=Hosts\n    |   |-- cn=BoreNode1\n    |   |-- cn=BoreNode4\n    |-- ou=netgroup\n        |-- cn=sysadmin\n```\n\n解决方法依然是LDAP，如下所示，在目录结构中添加Hosts子树，Hosts子树的每个子节点代表一台主机。配置过程如下：\n\n**LDAP服务器**\n\nStep1: 准备ldif文件，文件内容：\n\n```\ndn: ou=Hosts,dc=h3c,dc=com                                                                             \nobjectClass: organizationalUnit                                                                        \nobjectClass: top                                                                                       \nou: Hosts                                                                                              \n                                                                                                       \ndn: cn=BoreNode1,ou=Hosts,dc=h3c,dc=com                                                                \nobjectClass: ipHost                                                                                    \nobjectClass: device                                                                                    \nobjectClass: top                                                                                       \ncn: BoreNode1                                                                                          \nipHostNumber: 172.16.65.130                                                                            \n                                                                                                       \ndn: cn=BoreNode4,ou=Hosts,dc=h3c,dc=com                                                                \nobjectClass: ipHost                                                                                    \nobjectClass: device                                                                                    \nobjectClass: top                                                                                       \ncn: BoreNode4                                                                                          \nipHostNumber: 172.16.65.143\n```\n\nStep2: 导入ldif文件\n\n**LDAP客户端**\n\nStep1 修改/etc/nsswitch.conf文件\n\n```\nhosts:          files ldap\n```\n查找主机名时，先查找本地文件，然后再到LDAP服务器中查找。\n\n2. 修改/etc/ldap.conf文件\n\n```\nnss_base_hosts      ou=Hosts,dc=h3c,dc=com?one\n```\n\n3. 验证结果\n\n``` shell\nroot@BoreNode10:~# getent hosts\n127.0.0.1       localhost\n172.16.73.233   ubuntu\n127.0.0.1       localhost ip6-localhost ip6-loopback\n172.16.65.130   BoreNode1\n172.16.65.143   BoreNode4\n```\n输出结果的最后两条记录来自LDAP服务器。\n\n## 参考资料\n\n1. [6.8 Netgroup](http://etutorials.org/Server+Administration/ldap+system+administration/Part+II+Application+Integration/Chapter+6.+Replacing+NIS/6.8+Netgroups/)\n2. [LDAP Hosts](https://wiki.archlinux.org/index.php/LDAP_Hosts)\n\n","source":"_posts/Linux/LDAP_netgroup.md","raw":"---\ntitle: \"NFS集成LDAP实现\"\ndate: 2016-11-1 13:31:21\ncategories: [Linux]\ntags: [Linux, ldap, nfs]\ntoc: true\n---\n\n          +---------+    \n          |         |        \n          |         |        \n          +---------+ LDAP服务器\n               ^\n               |            \n               |            \n          +---------+        \n          |         |        \n          |         |\n          +---------+ NAS机头\n               ^   (LDAP客户端+NFS服务端)\n               |\n               |\n          +---------+        \n          |         |        \n          |         |\n          +---------+ NFS客户端\n\nNFS服务不提供用户级别的权限控制，但能够提供主机级别的权限控制。也就是，允许或禁止给定的主机访问NFS导出的共享目录。LDAP能够提供集中式的主机管理，将一个或多个主机归为一组，称为网络组(netgroup)。NFS集成LDAP的目的，就是利用LDAP的网络组来按组限制主机对共享目录的访问。如上图所示，当NFS服务端接收到NFS客户端挂载共享目录的请求时，先向LDAP服务器询问目标客户机是否在允许的网络组内，如果目标客户机在允许的网络组内则允许挂载，否则禁止挂载。\n\n<!--more-->\n\n## 一个例子\n\n| 主机名 | IP地址 | 角色 | 说明 |\n|:--|:--|:--|:--|\n| BoreNode1 | 172.16.65.130 | LDAP服务器 | 提供sysadmin网络组|\n| BoreNode3 | 172.16.65.142 | LDAP客户端以及NFS服务器 | 提供NFS服务，并根据网络组检验客户机的合法性 |\n| BoreNode4 | 172.16.65.143 | NFS客户端 | 属于sysadmin网络组 | \n| BoreNode5 | 172.16.65.144 | NFS客户端 | **不**属于sysadmin网络组 | \n\n假设NFS允许访问的网络组的名字为sysadmin，上表给出了例子中将用到的4台主机的不同角色，下面逐个介绍不同角色的配置情况。\n\n### LDAP服务器\n\n#### 安装软件\n\n``` shell\nroot@BoreNode1:~# apt-get install slapd ldap-utils\n```\n安装结束通过*ps -ef | grep slap*命令检查slapd进程。\n\n#### 重新配置\n\n``` shell\nroot@BoreNode1:~# dpkg-reconfigure slapd\n```\n配置过程中的选择：\n\nOmit OpenLDAP server configuration? No\nDNS domain name: h3c.com\nOrganization name: onestor\nDatabase backend to use: BDB\nDo you want the database to be removed when slapd is purged? Yes\nMove old database? Yes\nAllow LDAPv2 protocol? No\n\n#### 基本操作\n\n``` shell\nldapsearch -x -b dc=h3c,dc=com -H ldap://127.0.0.1\n```\n查看目录树结构\n\n``` shell\nldapdelete -x -D cn=admin,dc=h3c,dc=com -W cn=sysadmin,ou=netgroup,dc=h3c,dc=com\n```\n删除目录树中的1个节点\n\n``` shell\nldapadd -x -D cn=admin,dc=h3c,dc=com -W -f add_content.ldif\n```\n从ldif文件添加目录结构\n\n\n#### 构建数据\n\n```\ndc=h3c,dc=com\n    |-- cn=admin\n    |-- ou=netgroup\n        |-- cn=sysadmin\n```\n目标是构建上面的目录结构，网络组sysadmin节点挂在netgroup部门下面。实际上，按照上述步骤配置LDAP服务器时已经建立了dc=h3c,dc=com和cn=admin,dc=h3c,dc=com两个节点。因此接下来只需要添加ou=netgroup,dc=h3c,dc=com和cn=sysadmin,ou=netgroup,dc=h3c,dc=com两个节点。\n\n```\ndn: ou=netgroup,dc=h3c,dc=com                                                                          \nobjectClass: organizationalUnit                                                                        \nou: netgroup                                                                                           \n\ndn: cn=sysadmin,ou=netgroup,dc=h3c,dc=com                                                              \nobjectClass: nisNetgroup                                                                               \nobjectClass: top                                                                                       \ncn: sysadmin                                                                                           \nnisNetgroupTriple: (BoreNode1,-,-)                                                                     \nnisNetgroupTriple: (BoreNode4,-,-)\n```\n准备内容如上的LDIF文件，将其添加到目录结构。简单介绍下和网络组相关几个schema：\n* nisNetgroup 代表一个网络组，必选cn属性，代表网络组的名称。网络组通常作为ou的子节点，本例中挂在ou=netgroup节点下\n* nisNetgroupTriple 为网络组的属性，它的值是个三元组(host, user, NIS-domain)，代表1台主机\n* memberNisNetgroup 为网络组的属性，代表网络组的子网络组\n\n简而言之，一个网络组可以由一组主机构成，也可以由若干网络组组合而成，还可以由网络组加主机构成。nisNetgroupTriple描述主机，memberNisNetgroup描述子网络组。此外，关于这些预定义schema的更详细内容可以在/etc/ldap/schema目录中找到。\n\n### LDAP客户端\n\n#### 安装软件\n\n``` shell\nroot@BoreNode3:~# apt-get install ldap-utils libpam-ldap libnss-ldap nslcd\n```\n安装过程提示的配置：\n\nLDAP server Uniform Resource Identifier: ldap://172.16.65.130/\nDistinguished name of the search base: dc=h3c,dc=com\nLDAP version to use: 3\nMake local root Database admin: Yes\nDoes the LDAP database require login? No\nLDAP account for root: cn=admin,dc=h3c,dc=com\nLDAP server URI: ldap://172.16.65.130/\nLDAP server search base: dc=h3c,dc=com\n\n\n#### 认证方式\n\n**修改nsswitch**\n\n``` shell\nroot@BoreNode5:~# auth-client-config -t nss -p lac_ldap\n```\n执行上述命令的主要作用是修改/etc/nsswitch.conf文件，下表列出了命令执行前后的该文件的差异。\n\n| 认证项 | 命令前 | 命令后 |\n|:--|:--|:--|\n| passwd | compat | files ldap |\n| group | compat | files ldap |\n| shadow | compat | files ldap |\n| netgroup | nis  | ldap |\n\n**注意** 上表最后1行并非由auth-client-config命令自动修改，而是手动修改的。\n\nnsswitch是名字服务的开关，决定了查询名字的顺序。以查询passwd为例，修改后将先在本地查找，然后再到ldap中查找。如果删除“files ldap”中的files，那么查询用户时将只从ldap中查找。\n\n``` shell\nroot@BoreNode3:~# getent netgroup sysadmin\nsysadmin              (BoreNode1,-,-) (BoreNode4,-,-)\n```\n**getent**命令提供了查询各种名字的功能，例如getent passwd用于查询用户，getent netgroup用于查询网络组。但查询网路组时必须提供网络组的名字，因为getent不具备枚举网络组的功能。\n\n\n**修改slap**\n\n```\nnss_base_netgroup   ou=netgroup,dc=h3c,dc=com?one\n```\n修改/etc/sldap.conf文件，添加上面的配置。\n\n\n### NFS服务端\n\n**修改hosts文件**\n\n``` shell\nroot@BoreNode3:~# cat /etc/hosts\n127.0.0.1\tlocalhost\n172.16.65.130    BoreNode1\n172.16.65.142    BoreNode3\n172.16.65.143    BoreNode4\n172.16.65.144    BoreNode5\n```\n\n**修改/etc/exports文件**\n\n```\n/root/nfs_1     @sysadmin(rw,sync,no_subtree_check)\n```\n\n\n### NFS客户端\n\n### 主机名问题\n\n问题：如果NFS服务端的/etc/hosts文件不增加网络组的主机的话，那么即使目标客户机属于给定的网络组也无法访问NFS的共享目录。\n\n```\ndc=h3c,dc=com\n    |-- ou=Hosts\n    |   |-- cn=BoreNode1\n    |   |-- cn=BoreNode4\n    |-- ou=netgroup\n        |-- cn=sysadmin\n```\n\n解决方法依然是LDAP，如下所示，在目录结构中添加Hosts子树，Hosts子树的每个子节点代表一台主机。配置过程如下：\n\n**LDAP服务器**\n\nStep1: 准备ldif文件，文件内容：\n\n```\ndn: ou=Hosts,dc=h3c,dc=com                                                                             \nobjectClass: organizationalUnit                                                                        \nobjectClass: top                                                                                       \nou: Hosts                                                                                              \n                                                                                                       \ndn: cn=BoreNode1,ou=Hosts,dc=h3c,dc=com                                                                \nobjectClass: ipHost                                                                                    \nobjectClass: device                                                                                    \nobjectClass: top                                                                                       \ncn: BoreNode1                                                                                          \nipHostNumber: 172.16.65.130                                                                            \n                                                                                                       \ndn: cn=BoreNode4,ou=Hosts,dc=h3c,dc=com                                                                \nobjectClass: ipHost                                                                                    \nobjectClass: device                                                                                    \nobjectClass: top                                                                                       \ncn: BoreNode4                                                                                          \nipHostNumber: 172.16.65.143\n```\n\nStep2: 导入ldif文件\n\n**LDAP客户端**\n\nStep1 修改/etc/nsswitch.conf文件\n\n```\nhosts:          files ldap\n```\n查找主机名时，先查找本地文件，然后再到LDAP服务器中查找。\n\n2. 修改/etc/ldap.conf文件\n\n```\nnss_base_hosts      ou=Hosts,dc=h3c,dc=com?one\n```\n\n3. 验证结果\n\n``` shell\nroot@BoreNode10:~# getent hosts\n127.0.0.1       localhost\n172.16.73.233   ubuntu\n127.0.0.1       localhost ip6-localhost ip6-loopback\n172.16.65.130   BoreNode1\n172.16.65.143   BoreNode4\n```\n输出结果的最后两条记录来自LDAP服务器。\n\n## 参考资料\n\n1. [6.8 Netgroup](http://etutorials.org/Server+Administration/ldap+system+administration/Part+II+Application+Integration/Chapter+6.+Replacing+NIS/6.8+Netgroups/)\n2. [LDAP Hosts](https://wiki.archlinux.org/index.php/LDAP_Hosts)\n\n","slug":"Linux/LDAP_netgroup","published":1,"updated":"2016-12-15T07:18:40.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cix1kgaup001jdaeie3mu230k","content":"<pre><code>+---------+    \n|         |        \n|         |        \n+---------+ LDAP服务器\n     ^\n     |            \n     |            \n+---------+        \n|         |        \n|         |\n+---------+ NAS机头\n     ^   (LDAP客户端+NFS服务端)\n     |\n     |\n+---------+        \n|         |        \n|         |\n+---------+ NFS客户端\n</code></pre><p>NFS服务不提供用户级别的权限控制，但能够提供主机级别的权限控制。也就是，允许或禁止给定的主机访问NFS导出的共享目录。LDAP能够提供集中式的主机管理，将一个或多个主机归为一组，称为网络组(netgroup)。NFS集成LDAP的目的，就是利用LDAP的网络组来按组限制主机对共享目录的访问。如上图所示，当NFS服务端接收到NFS客户端挂载共享目录的请求时，先向LDAP服务器询问目标客户机是否在允许的网络组内，如果目标客户机在允许的网络组内则允许挂载，否则禁止挂载。</p>\n<a id=\"more\"></a>\n<h2 id=\"一个例子\"><a href=\"#一个例子\" class=\"headerlink\" title=\"一个例子\"></a>一个例子</h2><table>\n<thead>\n<tr>\n<th style=\"text-align:left\">主机名</th>\n<th style=\"text-align:left\">IP地址</th>\n<th style=\"text-align:left\">角色</th>\n<th style=\"text-align:left\">说明</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:left\">BoreNode1</td>\n<td style=\"text-align:left\">172.16.65.130</td>\n<td style=\"text-align:left\">LDAP服务器</td>\n<td style=\"text-align:left\">提供sysadmin网络组</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">BoreNode3</td>\n<td style=\"text-align:left\">172.16.65.142</td>\n<td style=\"text-align:left\">LDAP客户端以及NFS服务器</td>\n<td style=\"text-align:left\">提供NFS服务，并根据网络组检验客户机的合法性</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">BoreNode4</td>\n<td style=\"text-align:left\">172.16.65.143</td>\n<td style=\"text-align:left\">NFS客户端</td>\n<td style=\"text-align:left\">属于sysadmin网络组</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">BoreNode5</td>\n<td style=\"text-align:left\">172.16.65.144</td>\n<td style=\"text-align:left\">NFS客户端</td>\n<td style=\"text-align:left\"><strong>不</strong>属于sysadmin网络组</td>\n</tr>\n</tbody>\n</table>\n<p>假设NFS允许访问的网络组的名字为sysadmin，上表给出了例子中将用到的4台主机的不同角色，下面逐个介绍不同角色的配置情况。</p>\n<h3 id=\"LDAP服务器\"><a href=\"#LDAP服务器\" class=\"headerlink\" title=\"LDAP服务器\"></a>LDAP服务器</h3><h4 id=\"安装软件\"><a href=\"#安装软件\" class=\"headerlink\" title=\"安装软件\"></a>安装软件</h4><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">root@BoreNode1:~# apt-get install slapd ldap-utils</div></pre></td></tr></table></figure>\n<p>安装结束通过<em>ps -ef | grep slap</em>命令检查slapd进程。</p>\n<h4 id=\"重新配置\"><a href=\"#重新配置\" class=\"headerlink\" title=\"重新配置\"></a>重新配置</h4><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">root@BoreNode1:~# dpkg-reconfigure slapd</div></pre></td></tr></table></figure>\n<p>配置过程中的选择：</p>\n<p>Omit OpenLDAP server configuration? No<br>DNS domain name: h3c.com<br>Organization name: onestor<br>Database backend to use: BDB<br>Do you want the database to be removed when slapd is purged? Yes<br>Move old database? Yes<br>Allow LDAPv2 protocol? No</p>\n<h4 id=\"基本操作\"><a href=\"#基本操作\" class=\"headerlink\" title=\"基本操作\"></a>基本操作</h4><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">ldapsearch -x -b dc=h3c,dc=com -H ldap://127.0.0.1</div></pre></td></tr></table></figure>\n<p>查看目录树结构</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">ldapdelete -x -D cn=admin,dc=h3c,dc=com -W cn=sysadmin,ou=netgroup,dc=h3c,dc=com</div></pre></td></tr></table></figure>\n<p>删除目录树中的1个节点</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">ldapadd -x -D cn=admin,dc=h3c,dc=com -W -f add_content.ldif</div></pre></td></tr></table></figure>\n<p>从ldif文件添加目录结构</p>\n<h4 id=\"构建数据\"><a href=\"#构建数据\" class=\"headerlink\" title=\"构建数据\"></a>构建数据</h4><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">dc=h3c,dc=com</div><div class=\"line\">    |-- cn=admin</div><div class=\"line\">    |-- ou=netgroup</div><div class=\"line\">        |-- cn=sysadmin</div></pre></td></tr></table></figure>\n<p>目标是构建上面的目录结构，网络组sysadmin节点挂在netgroup部门下面。实际上，按照上述步骤配置LDAP服务器时已经建立了dc=h3c,dc=com和cn=admin,dc=h3c,dc=com两个节点。因此接下来只需要添加ou=netgroup,dc=h3c,dc=com和cn=sysadmin,ou=netgroup,dc=h3c,dc=com两个节点。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">dn: ou=netgroup,dc=h3c,dc=com                                                                          </div><div class=\"line\">objectClass: organizationalUnit                                                                        </div><div class=\"line\">ou: netgroup                                                                                           </div><div class=\"line\"></div><div class=\"line\">dn: cn=sysadmin,ou=netgroup,dc=h3c,dc=com                                                              </div><div class=\"line\">objectClass: nisNetgroup                                                                               </div><div class=\"line\">objectClass: top                                                                                       </div><div class=\"line\">cn: sysadmin                                                                                           </div><div class=\"line\">nisNetgroupTriple: (BoreNode1,-,-)                                                                     </div><div class=\"line\">nisNetgroupTriple: (BoreNode4,-,-)</div></pre></td></tr></table></figure>\n<p>准备内容如上的LDIF文件，将其添加到目录结构。简单介绍下和网络组相关几个schema：</p>\n<ul>\n<li>nisNetgroup 代表一个网络组，必选cn属性，代表网络组的名称。网络组通常作为ou的子节点，本例中挂在ou=netgroup节点下</li>\n<li>nisNetgroupTriple 为网络组的属性，它的值是个三元组(host, user, NIS-domain)，代表1台主机</li>\n<li>memberNisNetgroup 为网络组的属性，代表网络组的子网络组</li>\n</ul>\n<p>简而言之，一个网络组可以由一组主机构成，也可以由若干网络组组合而成，还可以由网络组加主机构成。nisNetgroupTriple描述主机，memberNisNetgroup描述子网络组。此外，关于这些预定义schema的更详细内容可以在/etc/ldap/schema目录中找到。</p>\n<h3 id=\"LDAP客户端\"><a href=\"#LDAP客户端\" class=\"headerlink\" title=\"LDAP客户端\"></a>LDAP客户端</h3><h4 id=\"安装软件-1\"><a href=\"#安装软件-1\" class=\"headerlink\" title=\"安装软件\"></a>安装软件</h4><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">root@BoreNode3:~# apt-get install ldap-utils libpam-ldap libnss-ldap nslcd</div></pre></td></tr></table></figure>\n<p>安装过程提示的配置：</p>\n<p>LDAP server Uniform Resource Identifier: ldap://172.16.65.130/<br>Distinguished name of the search base: dc=h3c,dc=com<br>LDAP version to use: 3<br>Make local root Database admin: Yes<br>Does the LDAP database require login? No<br>LDAP account for root: cn=admin,dc=h3c,dc=com<br>LDAP server URI: ldap://172.16.65.130/<br>LDAP server search base: dc=h3c,dc=com</p>\n<h4 id=\"认证方式\"><a href=\"#认证方式\" class=\"headerlink\" title=\"认证方式\"></a>认证方式</h4><p><strong>修改nsswitch</strong></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">root@BoreNode5:~# auth-client-config -t nss -p lac_ldap</div></pre></td></tr></table></figure>\n<p>执行上述命令的主要作用是修改/etc/nsswitch.conf文件，下表列出了命令执行前后的该文件的差异。</p>\n<table>\n<thead>\n<tr>\n<th style=\"text-align:left\">认证项</th>\n<th style=\"text-align:left\">命令前</th>\n<th style=\"text-align:left\">命令后</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:left\">passwd</td>\n<td style=\"text-align:left\">compat</td>\n<td style=\"text-align:left\">files ldap</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">group</td>\n<td style=\"text-align:left\">compat</td>\n<td style=\"text-align:left\">files ldap</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">shadow</td>\n<td style=\"text-align:left\">compat</td>\n<td style=\"text-align:left\">files ldap</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">netgroup</td>\n<td style=\"text-align:left\">nis</td>\n<td style=\"text-align:left\">ldap</td>\n</tr>\n</tbody>\n</table>\n<p><strong>注意</strong> 上表最后1行并非由auth-client-config命令自动修改，而是手动修改的。</p>\n<p>nsswitch是名字服务的开关，决定了查询名字的顺序。以查询passwd为例，修改后将先在本地查找，然后再到ldap中查找。如果删除“files ldap”中的files，那么查询用户时将只从ldap中查找。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">root@BoreNode3:~# getent netgroup sysadmin</div><div class=\"line\">sysadmin              (BoreNode1,-,-) (BoreNode4,-,-)</div></pre></td></tr></table></figure>\n<p><strong>getent</strong>命令提供了查询各种名字的功能，例如getent passwd用于查询用户，getent netgroup用于查询网络组。但查询网路组时必须提供网络组的名字，因为getent不具备枚举网络组的功能。</p>\n<p><strong>修改slap</strong></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">nss_base_netgroup   ou=netgroup,dc=h3c,dc=com?one</div></pre></td></tr></table></figure>\n<p>修改/etc/sldap.conf文件，添加上面的配置。</p>\n<h3 id=\"NFS服务端\"><a href=\"#NFS服务端\" class=\"headerlink\" title=\"NFS服务端\"></a>NFS服务端</h3><p><strong>修改hosts文件</strong></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">root@BoreNode3:~# cat /etc/hosts</div><div class=\"line\">127.0.0.1\tlocalhost</div><div class=\"line\">172.16.65.130    BoreNode1</div><div class=\"line\">172.16.65.142    BoreNode3</div><div class=\"line\">172.16.65.143    BoreNode4</div><div class=\"line\">172.16.65.144    BoreNode5</div></pre></td></tr></table></figure>\n<p><strong>修改/etc/exports文件</strong></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">/root/nfs_1     @sysadmin(rw,sync,no_subtree_check)</div></pre></td></tr></table></figure>\n<h3 id=\"NFS客户端\"><a href=\"#NFS客户端\" class=\"headerlink\" title=\"NFS客户端\"></a>NFS客户端</h3><h3 id=\"主机名问题\"><a href=\"#主机名问题\" class=\"headerlink\" title=\"主机名问题\"></a>主机名问题</h3><p>问题：如果NFS服务端的/etc/hosts文件不增加网络组的主机的话，那么即使目标客户机属于给定的网络组也无法访问NFS的共享目录。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">dc=h3c,dc=com</div><div class=\"line\">    |-- ou=Hosts</div><div class=\"line\">    |   |-- cn=BoreNode1</div><div class=\"line\">    |   |-- cn=BoreNode4</div><div class=\"line\">    |-- ou=netgroup</div><div class=\"line\">        |-- cn=sysadmin</div></pre></td></tr></table></figure>\n<p>解决方法依然是LDAP，如下所示，在目录结构中添加Hosts子树，Hosts子树的每个子节点代表一台主机。配置过程如下：</p>\n<p><strong>LDAP服务器</strong></p>\n<p>Step1: 准备ldif文件，文件内容：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">dn: ou=Hosts,dc=h3c,dc=com                                                                             </div><div class=\"line\">objectClass: organizationalUnit                                                                        </div><div class=\"line\">objectClass: top                                                                                       </div><div class=\"line\">ou: Hosts                                                                                              </div><div class=\"line\">                                                                                                       </div><div class=\"line\">dn: cn=BoreNode1,ou=Hosts,dc=h3c,dc=com                                                                </div><div class=\"line\">objectClass: ipHost                                                                                    </div><div class=\"line\">objectClass: device                                                                                    </div><div class=\"line\">objectClass: top                                                                                       </div><div class=\"line\">cn: BoreNode1                                                                                          </div><div class=\"line\">ipHostNumber: 172.16.65.130                                                                            </div><div class=\"line\">                                                                                                       </div><div class=\"line\">dn: cn=BoreNode4,ou=Hosts,dc=h3c,dc=com                                                                </div><div class=\"line\">objectClass: ipHost                                                                                    </div><div class=\"line\">objectClass: device                                                                                    </div><div class=\"line\">objectClass: top                                                                                       </div><div class=\"line\">cn: BoreNode4                                                                                          </div><div class=\"line\">ipHostNumber: 172.16.65.143</div></pre></td></tr></table></figure>\n<p>Step2: 导入ldif文件</p>\n<p><strong>LDAP客户端</strong></p>\n<p>Step1 修改/etc/nsswitch.conf文件</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">hosts:          files ldap</div></pre></td></tr></table></figure>\n<p>查找主机名时，先查找本地文件，然后再到LDAP服务器中查找。</p>\n<ol>\n<li>修改/etc/ldap.conf文件</li>\n</ol>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">nss_base_hosts      ou=Hosts,dc=h3c,dc=com?one</div></pre></td></tr></table></figure>\n<ol>\n<li>验证结果</li>\n</ol>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">root@BoreNode10:~# getent hosts</div><div class=\"line\">127.0.0.1       localhost</div><div class=\"line\">172.16.73.233   ubuntu</div><div class=\"line\">127.0.0.1       localhost ip6-localhost ip6-loopback</div><div class=\"line\">172.16.65.130   BoreNode1</div><div class=\"line\">172.16.65.143   BoreNode4</div></pre></td></tr></table></figure>\n<p>输出结果的最后两条记录来自LDAP服务器。</p>\n<h2 id=\"参考资料\"><a href=\"#参考资料\" class=\"headerlink\" title=\"参考资料\"></a>参考资料</h2><ol>\n<li><a href=\"http://etutorials.org/Server+Administration/ldap+system+administration/Part+II+Application+Integration/Chapter+6.+Replacing+NIS/6.8+Netgroups/\" target=\"_blank\" rel=\"external\">6.8 Netgroup</a></li>\n<li><a href=\"https://wiki.archlinux.org/index.php/LDAP_Hosts\" target=\"_blank\" rel=\"external\">LDAP Hosts</a></li>\n</ol>\n<hr>\n<p><strong>版权声明</strong></p>\n<p>本作品采用<a href=\"https://creativecommons.org/licenses/by/2.5/cn/\" target=\"_blank\" rel=\"external\">知识共享署名2.5 中国大陆许可协议</a>进行许可，欢迎转载，但转载请注明来自<a href=\"https://wuxiangwei.cn/\"><em>Shanno’s Blog</em></a>，并保持转载后文章内容的完整。本人保留所有版权相关权利，侵权必究。</p>\n","excerpt":"<pre><code>+---------+    \n|         |        \n|         |        \n+---------+ LDAP服务器\n     ^\n     |            \n     |            \n+---------+        \n|         |        \n|         |\n+---------+ NAS机头\n     ^   (LDAP客户端+NFS服务端)\n     |\n     |\n+---------+        \n|         |        \n|         |\n+---------+ NFS客户端\n</code></pre><p>NFS服务不提供用户级别的权限控制，但能够提供主机级别的权限控制。也就是，允许或禁止给定的主机访问NFS导出的共享目录。LDAP能够提供集中式的主机管理，将一个或多个主机归为一组，称为网络组(netgroup)。NFS集成LDAP的目的，就是利用LDAP的网络组来按组限制主机对共享目录的访问。如上图所示，当NFS服务端接收到NFS客户端挂载共享目录的请求时，先向LDAP服务器询问目标客户机是否在允许的网络组内，如果目标客户机在允许的网络组内则允许挂载，否则禁止挂载。</p>","more":"<h2 id=\"一个例子\"><a href=\"#一个例子\" class=\"headerlink\" title=\"一个例子\"></a>一个例子</h2><table>\n<thead>\n<tr>\n<th style=\"text-align:left\">主机名</th>\n<th style=\"text-align:left\">IP地址</th>\n<th style=\"text-align:left\">角色</th>\n<th style=\"text-align:left\">说明</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:left\">BoreNode1</td>\n<td style=\"text-align:left\">172.16.65.130</td>\n<td style=\"text-align:left\">LDAP服务器</td>\n<td style=\"text-align:left\">提供sysadmin网络组</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">BoreNode3</td>\n<td style=\"text-align:left\">172.16.65.142</td>\n<td style=\"text-align:left\">LDAP客户端以及NFS服务器</td>\n<td style=\"text-align:left\">提供NFS服务，并根据网络组检验客户机的合法性</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">BoreNode4</td>\n<td style=\"text-align:left\">172.16.65.143</td>\n<td style=\"text-align:left\">NFS客户端</td>\n<td style=\"text-align:left\">属于sysadmin网络组</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">BoreNode5</td>\n<td style=\"text-align:left\">172.16.65.144</td>\n<td style=\"text-align:left\">NFS客户端</td>\n<td style=\"text-align:left\"><strong>不</strong>属于sysadmin网络组</td>\n</tr>\n</tbody>\n</table>\n<p>假设NFS允许访问的网络组的名字为sysadmin，上表给出了例子中将用到的4台主机的不同角色，下面逐个介绍不同角色的配置情况。</p>\n<h3 id=\"LDAP服务器\"><a href=\"#LDAP服务器\" class=\"headerlink\" title=\"LDAP服务器\"></a>LDAP服务器</h3><h4 id=\"安装软件\"><a href=\"#安装软件\" class=\"headerlink\" title=\"安装软件\"></a>安装软件</h4><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">root@BoreNode1:~# apt-get install slapd ldap-utils</div></pre></td></tr></table></figure>\n<p>安装结束通过<em>ps -ef | grep slap</em>命令检查slapd进程。</p>\n<h4 id=\"重新配置\"><a href=\"#重新配置\" class=\"headerlink\" title=\"重新配置\"></a>重新配置</h4><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">root@BoreNode1:~# dpkg-reconfigure slapd</div></pre></td></tr></table></figure>\n<p>配置过程中的选择：</p>\n<p>Omit OpenLDAP server configuration? No<br>DNS domain name: h3c.com<br>Organization name: onestor<br>Database backend to use: BDB<br>Do you want the database to be removed when slapd is purged? Yes<br>Move old database? Yes<br>Allow LDAPv2 protocol? No</p>\n<h4 id=\"基本操作\"><a href=\"#基本操作\" class=\"headerlink\" title=\"基本操作\"></a>基本操作</h4><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">ldapsearch -x -b dc=h3c,dc=com -H ldap://127.0.0.1</div></pre></td></tr></table></figure>\n<p>查看目录树结构</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">ldapdelete -x -D cn=admin,dc=h3c,dc=com -W cn=sysadmin,ou=netgroup,dc=h3c,dc=com</div></pre></td></tr></table></figure>\n<p>删除目录树中的1个节点</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">ldapadd -x -D cn=admin,dc=h3c,dc=com -W -f add_content.ldif</div></pre></td></tr></table></figure>\n<p>从ldif文件添加目录结构</p>\n<h4 id=\"构建数据\"><a href=\"#构建数据\" class=\"headerlink\" title=\"构建数据\"></a>构建数据</h4><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">dc=h3c,dc=com</div><div class=\"line\">    |-- cn=admin</div><div class=\"line\">    |-- ou=netgroup</div><div class=\"line\">        |-- cn=sysadmin</div></pre></td></tr></table></figure>\n<p>目标是构建上面的目录结构，网络组sysadmin节点挂在netgroup部门下面。实际上，按照上述步骤配置LDAP服务器时已经建立了dc=h3c,dc=com和cn=admin,dc=h3c,dc=com两个节点。因此接下来只需要添加ou=netgroup,dc=h3c,dc=com和cn=sysadmin,ou=netgroup,dc=h3c,dc=com两个节点。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">dn: ou=netgroup,dc=h3c,dc=com                                                                          </div><div class=\"line\">objectClass: organizationalUnit                                                                        </div><div class=\"line\">ou: netgroup                                                                                           </div><div class=\"line\"></div><div class=\"line\">dn: cn=sysadmin,ou=netgroup,dc=h3c,dc=com                                                              </div><div class=\"line\">objectClass: nisNetgroup                                                                               </div><div class=\"line\">objectClass: top                                                                                       </div><div class=\"line\">cn: sysadmin                                                                                           </div><div class=\"line\">nisNetgroupTriple: (BoreNode1,-,-)                                                                     </div><div class=\"line\">nisNetgroupTriple: (BoreNode4,-,-)</div></pre></td></tr></table></figure>\n<p>准备内容如上的LDIF文件，将其添加到目录结构。简单介绍下和网络组相关几个schema：</p>\n<ul>\n<li>nisNetgroup 代表一个网络组，必选cn属性，代表网络组的名称。网络组通常作为ou的子节点，本例中挂在ou=netgroup节点下</li>\n<li>nisNetgroupTriple 为网络组的属性，它的值是个三元组(host, user, NIS-domain)，代表1台主机</li>\n<li>memberNisNetgroup 为网络组的属性，代表网络组的子网络组</li>\n</ul>\n<p>简而言之，一个网络组可以由一组主机构成，也可以由若干网络组组合而成，还可以由网络组加主机构成。nisNetgroupTriple描述主机，memberNisNetgroup描述子网络组。此外，关于这些预定义schema的更详细内容可以在/etc/ldap/schema目录中找到。</p>\n<h3 id=\"LDAP客户端\"><a href=\"#LDAP客户端\" class=\"headerlink\" title=\"LDAP客户端\"></a>LDAP客户端</h3><h4 id=\"安装软件-1\"><a href=\"#安装软件-1\" class=\"headerlink\" title=\"安装软件\"></a>安装软件</h4><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">root@BoreNode3:~# apt-get install ldap-utils libpam-ldap libnss-ldap nslcd</div></pre></td></tr></table></figure>\n<p>安装过程提示的配置：</p>\n<p>LDAP server Uniform Resource Identifier: ldap://172.16.65.130/<br>Distinguished name of the search base: dc=h3c,dc=com<br>LDAP version to use: 3<br>Make local root Database admin: Yes<br>Does the LDAP database require login? No<br>LDAP account for root: cn=admin,dc=h3c,dc=com<br>LDAP server URI: ldap://172.16.65.130/<br>LDAP server search base: dc=h3c,dc=com</p>\n<h4 id=\"认证方式\"><a href=\"#认证方式\" class=\"headerlink\" title=\"认证方式\"></a>认证方式</h4><p><strong>修改nsswitch</strong></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">root@BoreNode5:~# auth-client-config -t nss -p lac_ldap</div></pre></td></tr></table></figure>\n<p>执行上述命令的主要作用是修改/etc/nsswitch.conf文件，下表列出了命令执行前后的该文件的差异。</p>\n<table>\n<thead>\n<tr>\n<th style=\"text-align:left\">认证项</th>\n<th style=\"text-align:left\">命令前</th>\n<th style=\"text-align:left\">命令后</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:left\">passwd</td>\n<td style=\"text-align:left\">compat</td>\n<td style=\"text-align:left\">files ldap</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">group</td>\n<td style=\"text-align:left\">compat</td>\n<td style=\"text-align:left\">files ldap</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">shadow</td>\n<td style=\"text-align:left\">compat</td>\n<td style=\"text-align:left\">files ldap</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">netgroup</td>\n<td style=\"text-align:left\">nis</td>\n<td style=\"text-align:left\">ldap</td>\n</tr>\n</tbody>\n</table>\n<p><strong>注意</strong> 上表最后1行并非由auth-client-config命令自动修改，而是手动修改的。</p>\n<p>nsswitch是名字服务的开关，决定了查询名字的顺序。以查询passwd为例，修改后将先在本地查找，然后再到ldap中查找。如果删除“files ldap”中的files，那么查询用户时将只从ldap中查找。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">root@BoreNode3:~# getent netgroup sysadmin</div><div class=\"line\">sysadmin              (BoreNode1,-,-) (BoreNode4,-,-)</div></pre></td></tr></table></figure>\n<p><strong>getent</strong>命令提供了查询各种名字的功能，例如getent passwd用于查询用户，getent netgroup用于查询网络组。但查询网路组时必须提供网络组的名字，因为getent不具备枚举网络组的功能。</p>\n<p><strong>修改slap</strong></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">nss_base_netgroup   ou=netgroup,dc=h3c,dc=com?one</div></pre></td></tr></table></figure>\n<p>修改/etc/sldap.conf文件，添加上面的配置。</p>\n<h3 id=\"NFS服务端\"><a href=\"#NFS服务端\" class=\"headerlink\" title=\"NFS服务端\"></a>NFS服务端</h3><p><strong>修改hosts文件</strong></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">root@BoreNode3:~# cat /etc/hosts</div><div class=\"line\">127.0.0.1\tlocalhost</div><div class=\"line\">172.16.65.130    BoreNode1</div><div class=\"line\">172.16.65.142    BoreNode3</div><div class=\"line\">172.16.65.143    BoreNode4</div><div class=\"line\">172.16.65.144    BoreNode5</div></pre></td></tr></table></figure>\n<p><strong>修改/etc/exports文件</strong></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">/root/nfs_1     @sysadmin(rw,sync,no_subtree_check)</div></pre></td></tr></table></figure>\n<h3 id=\"NFS客户端\"><a href=\"#NFS客户端\" class=\"headerlink\" title=\"NFS客户端\"></a>NFS客户端</h3><h3 id=\"主机名问题\"><a href=\"#主机名问题\" class=\"headerlink\" title=\"主机名问题\"></a>主机名问题</h3><p>问题：如果NFS服务端的/etc/hosts文件不增加网络组的主机的话，那么即使目标客户机属于给定的网络组也无法访问NFS的共享目录。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">dc=h3c,dc=com</div><div class=\"line\">    |-- ou=Hosts</div><div class=\"line\">    |   |-- cn=BoreNode1</div><div class=\"line\">    |   |-- cn=BoreNode4</div><div class=\"line\">    |-- ou=netgroup</div><div class=\"line\">        |-- cn=sysadmin</div></pre></td></tr></table></figure>\n<p>解决方法依然是LDAP，如下所示，在目录结构中添加Hosts子树，Hosts子树的每个子节点代表一台主机。配置过程如下：</p>\n<p><strong>LDAP服务器</strong></p>\n<p>Step1: 准备ldif文件，文件内容：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">dn: ou=Hosts,dc=h3c,dc=com                                                                             </div><div class=\"line\">objectClass: organizationalUnit                                                                        </div><div class=\"line\">objectClass: top                                                                                       </div><div class=\"line\">ou: Hosts                                                                                              </div><div class=\"line\">                                                                                                       </div><div class=\"line\">dn: cn=BoreNode1,ou=Hosts,dc=h3c,dc=com                                                                </div><div class=\"line\">objectClass: ipHost                                                                                    </div><div class=\"line\">objectClass: device                                                                                    </div><div class=\"line\">objectClass: top                                                                                       </div><div class=\"line\">cn: BoreNode1                                                                                          </div><div class=\"line\">ipHostNumber: 172.16.65.130                                                                            </div><div class=\"line\">                                                                                                       </div><div class=\"line\">dn: cn=BoreNode4,ou=Hosts,dc=h3c,dc=com                                                                </div><div class=\"line\">objectClass: ipHost                                                                                    </div><div class=\"line\">objectClass: device                                                                                    </div><div class=\"line\">objectClass: top                                                                                       </div><div class=\"line\">cn: BoreNode4                                                                                          </div><div class=\"line\">ipHostNumber: 172.16.65.143</div></pre></td></tr></table></figure>\n<p>Step2: 导入ldif文件</p>\n<p><strong>LDAP客户端</strong></p>\n<p>Step1 修改/etc/nsswitch.conf文件</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">hosts:          files ldap</div></pre></td></tr></table></figure>\n<p>查找主机名时，先查找本地文件，然后再到LDAP服务器中查找。</p>\n<ol>\n<li>修改/etc/ldap.conf文件</li>\n</ol>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">nss_base_hosts      ou=Hosts,dc=h3c,dc=com?one</div></pre></td></tr></table></figure>\n<ol>\n<li>验证结果</li>\n</ol>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">root@BoreNode10:~# getent hosts</div><div class=\"line\">127.0.0.1       localhost</div><div class=\"line\">172.16.73.233   ubuntu</div><div class=\"line\">127.0.0.1       localhost ip6-localhost ip6-loopback</div><div class=\"line\">172.16.65.130   BoreNode1</div><div class=\"line\">172.16.65.143   BoreNode4</div></pre></td></tr></table></figure>\n<p>输出结果的最后两条记录来自LDAP服务器。</p>\n<h2 id=\"参考资料\"><a href=\"#参考资料\" class=\"headerlink\" title=\"参考资料\"></a>参考资料</h2><ol>\n<li><a href=\"http://etutorials.org/Server+Administration/ldap+system+administration/Part+II+Application+Integration/Chapter+6.+Replacing+NIS/6.8+Netgroups/\">6.8 Netgroup</a></li>\n<li><a href=\"https://wiki.archlinux.org/index.php/LDAP_Hosts\">LDAP Hosts</a></li>\n</ol>\n<hr>\n<p><strong>版权声明</strong></p>\n<p>本作品采用<a href=\"https://creativecommons.org/licenses/by/2.5/cn/\">知识共享署名2.5 中国大陆许可协议</a>进行许可，欢迎转载，但转载请注明来自<a href=\"https://wuxiangwei.cn/\"><em>Shanno’s Blog</em></a>，并保持转载后文章内容的完整。本人保留所有版权相关权利，侵权必究。</p>"},{"title":"msys2安装配置","date":"2016-10-11T01:47:45.000Z","toc":true,"_content":"\n需求：使用MinGW工具链编译出原生的Window应用程序。\n\n[MSYS2](https://sourceforge.net/p/msys2/wiki/MSYS2%20introduction/)由3个子系统组成：msys2、mingw32、mingw64。msys2子系统提供软件编译、软件包管理以及shell功能。\n\n<!--more-->\n\n## MSYS2安装\n\n参考\nhttp://msys2.github.io/\n\n安装结束\nmsys2安装目录和普通的Linux目录相同。\n\n## Pacman软件包管理\n\n**启动**\n进入msys2安装目录（D:\\msys64），双击msys2.exe文件，进入交互界面。\n\n**修改源**（可选）\n参考 https://lug.ustc.edu.cn/wiki/mirrors/help/msys2\n不修改源，经测试，速度还行\n\n**配置代理**\n```\nexport http_proxy=http://172.16.31.2:808 && export https_proxy=http://172.16.31.2:808\n```\n\n**pacman详细用法**\n参考 https://wiki.archlinux.org/index.php/pacman\n\n### 更新软件包\n\n```\npacman -Syu\n```\n\n### 列出已安装软件\n\n```\npacman -Qe\n```\n\n### 搜索软件包\n\n```\npacman -Ss mingw | grep gcc\n```\n\n### 安装软件\n\n```\npacman -S mingw64/mingw-w64-x86_64-gcc\n```\n安装mingw-gcc软件包\n\n### 卸载软件\n\n```\npacman -R gcc\n```\n卸载gcc软件包\n\n\n## 准备开发环境\n\n### 安装mingw-gcc\n\n```\npacman -S mingw-w64-x86_64-gcc\n```\n注意，msys2存在msys和mingw两种版本的gcc软件包，默认是msys版本的软件包。该软件包编译出的应用程序依赖msys-2.0.dll文件，因此安装mingw64版本的gcc软件包。安装mingw版本的gcc时，先重命名msys2安装目录下的mingw64.exe和mingw64.ini两个文件(本文在这两个文件名前添加_前缀，如果不修改mingw64.ini文件，也可以正常安装gcc软件包，但无法执行重命名后的mingw64.exe文件)，否则将报错。\n\n安装完 mingw-w64-x86_64-gcc后，通过_mingw64.exe文件启动交互界面，否则如果通过msys2.exe启动交互界面，将发现gcc命令无法执行的错误。\n\n**目标**：测试gcc是否正常工作\n创建如下工程：\n```\n$ tree demo\ndemo\n└── demo.c\n```\n\ndemo.c文件的内容：\n\n```C\n#include <stdio>\n\nint main(int argc, char *argv[])\n{\n    printf(\"Hello, baby\");\n    return 0;\n}\n```\n\n编译工程：\n\n```shell\ngcc demo.c -o demo.exe\n```\n\n双击demo.exe文件，不会提示依赖 msys-2.0.dll文件，可以正常运行。\n\n### 编译ceph-dokan\n\n**编译boost库**\n\n[下载](https://sourceforge.net/projects/boost/?source=typ_redirect)boost1.60.0代码\n\n```\nunzip boost_1_60_0.zip\ncd boost_1_60_0/\n./bootstrap.bat mingw\n./b2 toolset=gcc --with-system\n```\n此处，只编译boost的system模块\n\n\n**编译ceph-dokan**\n\n修改Makefile，指定boost路径\n\n修改编译选项 -fpermi\n\n错误1：\n\n```\nIn file included from ./include/int_types.h:30:0,\n                 from ./include/types.h:18,\n                 from auth/Crypto.h:18,\n                 from libcephfs.cc:20:\n./mingw_include/linux/types.h:13:24: fatal error: parts/time.h: No such file or directory\n #include <parts/time.h>\n                        ^\ncompilation terminated.\nmake: *** [Makefile:13：libcephfs.o] 错误 1\n```\n\n\n\n","source":"_posts/Linux/msys2.md","raw":"---\ntitle: \"msys2安装配置\"\ndate: 2016-10-11 9:47:45\ncategories: [Linux]\ntags: [Linux, msys2]\ntoc: true\n---\n\n需求：使用MinGW工具链编译出原生的Window应用程序。\n\n[MSYS2](https://sourceforge.net/p/msys2/wiki/MSYS2%20introduction/)由3个子系统组成：msys2、mingw32、mingw64。msys2子系统提供软件编译、软件包管理以及shell功能。\n\n<!--more-->\n\n## MSYS2安装\n\n参考\nhttp://msys2.github.io/\n\n安装结束\nmsys2安装目录和普通的Linux目录相同。\n\n## Pacman软件包管理\n\n**启动**\n进入msys2安装目录（D:\\msys64），双击msys2.exe文件，进入交互界面。\n\n**修改源**（可选）\n参考 https://lug.ustc.edu.cn/wiki/mirrors/help/msys2\n不修改源，经测试，速度还行\n\n**配置代理**\n```\nexport http_proxy=http://172.16.31.2:808 && export https_proxy=http://172.16.31.2:808\n```\n\n**pacman详细用法**\n参考 https://wiki.archlinux.org/index.php/pacman\n\n### 更新软件包\n\n```\npacman -Syu\n```\n\n### 列出已安装软件\n\n```\npacman -Qe\n```\n\n### 搜索软件包\n\n```\npacman -Ss mingw | grep gcc\n```\n\n### 安装软件\n\n```\npacman -S mingw64/mingw-w64-x86_64-gcc\n```\n安装mingw-gcc软件包\n\n### 卸载软件\n\n```\npacman -R gcc\n```\n卸载gcc软件包\n\n\n## 准备开发环境\n\n### 安装mingw-gcc\n\n```\npacman -S mingw-w64-x86_64-gcc\n```\n注意，msys2存在msys和mingw两种版本的gcc软件包，默认是msys版本的软件包。该软件包编译出的应用程序依赖msys-2.0.dll文件，因此安装mingw64版本的gcc软件包。安装mingw版本的gcc时，先重命名msys2安装目录下的mingw64.exe和mingw64.ini两个文件(本文在这两个文件名前添加_前缀，如果不修改mingw64.ini文件，也可以正常安装gcc软件包，但无法执行重命名后的mingw64.exe文件)，否则将报错。\n\n安装完 mingw-w64-x86_64-gcc后，通过_mingw64.exe文件启动交互界面，否则如果通过msys2.exe启动交互界面，将发现gcc命令无法执行的错误。\n\n**目标**：测试gcc是否正常工作\n创建如下工程：\n```\n$ tree demo\ndemo\n└── demo.c\n```\n\ndemo.c文件的内容：\n\n```C\n#include <stdio>\n\nint main(int argc, char *argv[])\n{\n    printf(\"Hello, baby\");\n    return 0;\n}\n```\n\n编译工程：\n\n```shell\ngcc demo.c -o demo.exe\n```\n\n双击demo.exe文件，不会提示依赖 msys-2.0.dll文件，可以正常运行。\n\n### 编译ceph-dokan\n\n**编译boost库**\n\n[下载](https://sourceforge.net/projects/boost/?source=typ_redirect)boost1.60.0代码\n\n```\nunzip boost_1_60_0.zip\ncd boost_1_60_0/\n./bootstrap.bat mingw\n./b2 toolset=gcc --with-system\n```\n此处，只编译boost的system模块\n\n\n**编译ceph-dokan**\n\n修改Makefile，指定boost路径\n\n修改编译选项 -fpermi\n\n错误1：\n\n```\nIn file included from ./include/int_types.h:30:0,\n                 from ./include/types.h:18,\n                 from auth/Crypto.h:18,\n                 from libcephfs.cc:20:\n./mingw_include/linux/types.h:13:24: fatal error: parts/time.h: No such file or directory\n #include <parts/time.h>\n                        ^\ncompilation terminated.\nmake: *** [Makefile:13：libcephfs.o] 错误 1\n```\n\n\n\n","slug":"Linux/msys2","published":1,"updated":"2016-12-15T07:18:40.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cix1kgaut001qdaei9fes9zfe","content":"<p>需求：使用MinGW工具链编译出原生的Window应用程序。</p>\n<p><a href=\"https://sourceforge.net/p/msys2/wiki/MSYS2%20introduction/\" target=\"_blank\" rel=\"external\">MSYS2</a>由3个子系统组成：msys2、mingw32、mingw64。msys2子系统提供软件编译、软件包管理以及shell功能。</p>\n<a id=\"more\"></a>\n<h2 id=\"MSYS2安装\"><a href=\"#MSYS2安装\" class=\"headerlink\" title=\"MSYS2安装\"></a>MSYS2安装</h2><p>参考<br><a href=\"http://msys2.github.io/\" target=\"_blank\" rel=\"external\">http://msys2.github.io/</a></p>\n<p>安装结束<br>msys2安装目录和普通的Linux目录相同。</p>\n<h2 id=\"Pacman软件包管理\"><a href=\"#Pacman软件包管理\" class=\"headerlink\" title=\"Pacman软件包管理\"></a>Pacman软件包管理</h2><p><strong>启动</strong><br>进入msys2安装目录（D:\\msys64），双击msys2.exe文件，进入交互界面。</p>\n<p><strong>修改源</strong>（可选）<br>参考 <a href=\"https://lug.ustc.edu.cn/wiki/mirrors/help/msys2\" target=\"_blank\" rel=\"external\">https://lug.ustc.edu.cn/wiki/mirrors/help/msys2</a><br>不修改源，经测试，速度还行</p>\n<p><strong>配置代理</strong><br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">export http_proxy=http://172.16.31.2:808 &amp;&amp; export https_proxy=http://172.16.31.2:808</div></pre></td></tr></table></figure></p>\n<p><strong>pacman详细用法</strong><br>参考 <a href=\"https://wiki.archlinux.org/index.php/pacman\" target=\"_blank\" rel=\"external\">https://wiki.archlinux.org/index.php/pacman</a></p>\n<h3 id=\"更新软件包\"><a href=\"#更新软件包\" class=\"headerlink\" title=\"更新软件包\"></a>更新软件包</h3><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">pacman -Syu</div></pre></td></tr></table></figure>\n<h3 id=\"列出已安装软件\"><a href=\"#列出已安装软件\" class=\"headerlink\" title=\"列出已安装软件\"></a>列出已安装软件</h3><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">pacman -Qe</div></pre></td></tr></table></figure>\n<h3 id=\"搜索软件包\"><a href=\"#搜索软件包\" class=\"headerlink\" title=\"搜索软件包\"></a>搜索软件包</h3><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">pacman -Ss mingw | grep gcc</div></pre></td></tr></table></figure>\n<h3 id=\"安装软件\"><a href=\"#安装软件\" class=\"headerlink\" title=\"安装软件\"></a>安装软件</h3><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">pacman -S mingw64/mingw-w64-x86_64-gcc</div></pre></td></tr></table></figure>\n<p>安装mingw-gcc软件包</p>\n<h3 id=\"卸载软件\"><a href=\"#卸载软件\" class=\"headerlink\" title=\"卸载软件\"></a>卸载软件</h3><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">pacman -R gcc</div></pre></td></tr></table></figure>\n<p>卸载gcc软件包</p>\n<h2 id=\"准备开发环境\"><a href=\"#准备开发环境\" class=\"headerlink\" title=\"准备开发环境\"></a>准备开发环境</h2><h3 id=\"安装mingw-gcc\"><a href=\"#安装mingw-gcc\" class=\"headerlink\" title=\"安装mingw-gcc\"></a>安装mingw-gcc</h3><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">pacman -S mingw-w64-x86_64-gcc</div></pre></td></tr></table></figure>\n<p>注意，msys2存在msys和mingw两种版本的gcc软件包，默认是msys版本的软件包。该软件包编译出的应用程序依赖msys-2.0.dll文件，因此安装mingw64版本的gcc软件包。安装mingw版本的gcc时，先重命名msys2安装目录下的mingw64.exe和mingw64.ini两个文件(本文在这两个文件名前添加_前缀，如果不修改mingw64.ini文件，也可以正常安装gcc软件包，但无法执行重命名后的mingw64.exe文件)，否则将报错。</p>\n<p>安装完 mingw-w64-x86_64-gcc后，通过_mingw64.exe文件启动交互界面，否则如果通过msys2.exe启动交互界面，将发现gcc命令无法执行的错误。</p>\n<p><strong>目标</strong>：测试gcc是否正常工作<br>创建如下工程：<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">$ tree demo</div><div class=\"line\">demo</div><div class=\"line\">└── demo.c</div></pre></td></tr></table></figure></p>\n<p>demo.c文件的内容：</p>\n<figure class=\"highlight c\"><table><tr><td class=\"code\"><pre><div class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">include</span> <span class=\"meta-string\">&lt;stdio&gt;</span></span></div><div class=\"line\"></div><div class=\"line\"><span class=\"function\"><span class=\"keyword\">int</span> <span class=\"title\">main</span><span class=\"params\">(<span class=\"keyword\">int</span> argc, <span class=\"keyword\">char</span> *argv[])</span></span></div><div class=\"line\">&#123;</div><div class=\"line\">    <span class=\"built_in\">printf</span>(<span class=\"string\">\"Hello, baby\"</span>);</div><div class=\"line\">    <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<p>编译工程：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">gcc demo.c -o demo.exe</div></pre></td></tr></table></figure>\n<p>双击demo.exe文件，不会提示依赖 msys-2.0.dll文件，可以正常运行。</p>\n<h3 id=\"编译ceph-dokan\"><a href=\"#编译ceph-dokan\" class=\"headerlink\" title=\"编译ceph-dokan\"></a>编译ceph-dokan</h3><p><strong>编译boost库</strong></p>\n<p><a href=\"https://sourceforge.net/projects/boost/?source=typ_redirect\" target=\"_blank\" rel=\"external\">下载</a>boost1.60.0代码</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">unzip boost_1_60_0.zip</div><div class=\"line\">cd boost_1_60_0/</div><div class=\"line\">./bootstrap.bat mingw</div><div class=\"line\">./b2 toolset=gcc --with-system</div></pre></td></tr></table></figure>\n<p>此处，只编译boost的system模块</p>\n<p><strong>编译ceph-dokan</strong></p>\n<p>修改Makefile，指定boost路径</p>\n<p>修改编译选项 -fpermi</p>\n<p>错误1：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">In file included from ./include/int_types.h:30:0,</div><div class=\"line\">                 from ./include/types.h:18,</div><div class=\"line\">                 from auth/Crypto.h:18,</div><div class=\"line\">                 from libcephfs.cc:20:</div><div class=\"line\">./mingw_include/linux/types.h:13:24: fatal error: parts/time.h: No such file or directory</div><div class=\"line\"> #include &lt;parts/time.h&gt;</div><div class=\"line\">                        ^</div><div class=\"line\">compilation terminated.</div><div class=\"line\">make: *** [Makefile:13：libcephfs.o] 错误 1</div></pre></td></tr></table></figure>\n<hr>\n<p><strong>版权声明</strong></p>\n<p>本作品采用<a href=\"https://creativecommons.org/licenses/by/2.5/cn/\" target=\"_blank\" rel=\"external\">知识共享署名2.5 中国大陆许可协议</a>进行许可，欢迎转载，但转载请注明来自<a href=\"https://wuxiangwei.cn/\"><em>Shanno’s Blog</em></a>，并保持转载后文章内容的完整。本人保留所有版权相关权利，侵权必究。</p>\n","excerpt":"<p>需求：使用MinGW工具链编译出原生的Window应用程序。</p>\n<p><a href=\"https://sourceforge.net/p/msys2/wiki/MSYS2%20introduction/\">MSYS2</a>由3个子系统组成：msys2、mingw32、mingw64。msys2子系统提供软件编译、软件包管理以及shell功能。</p>","more":"<h2 id=\"MSYS2安装\"><a href=\"#MSYS2安装\" class=\"headerlink\" title=\"MSYS2安装\"></a>MSYS2安装</h2><p>参考<br><a href=\"http://msys2.github.io/\">http://msys2.github.io/</a></p>\n<p>安装结束<br>msys2安装目录和普通的Linux目录相同。</p>\n<h2 id=\"Pacman软件包管理\"><a href=\"#Pacman软件包管理\" class=\"headerlink\" title=\"Pacman软件包管理\"></a>Pacman软件包管理</h2><p><strong>启动</strong><br>进入msys2安装目录（D:\\msys64），双击msys2.exe文件，进入交互界面。</p>\n<p><strong>修改源</strong>（可选）<br>参考 <a href=\"https://lug.ustc.edu.cn/wiki/mirrors/help/msys2\">https://lug.ustc.edu.cn/wiki/mirrors/help/msys2</a><br>不修改源，经测试，速度还行</p>\n<p><strong>配置代理</strong><br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">export http_proxy=http://172.16.31.2:808 &amp;&amp; export https_proxy=http://172.16.31.2:808</div></pre></td></tr></table></figure></p>\n<p><strong>pacman详细用法</strong><br>参考 <a href=\"https://wiki.archlinux.org/index.php/pacman\">https://wiki.archlinux.org/index.php/pacman</a></p>\n<h3 id=\"更新软件包\"><a href=\"#更新软件包\" class=\"headerlink\" title=\"更新软件包\"></a>更新软件包</h3><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">pacman -Syu</div></pre></td></tr></table></figure>\n<h3 id=\"列出已安装软件\"><a href=\"#列出已安装软件\" class=\"headerlink\" title=\"列出已安装软件\"></a>列出已安装软件</h3><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">pacman -Qe</div></pre></td></tr></table></figure>\n<h3 id=\"搜索软件包\"><a href=\"#搜索软件包\" class=\"headerlink\" title=\"搜索软件包\"></a>搜索软件包</h3><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">pacman -Ss mingw | grep gcc</div></pre></td></tr></table></figure>\n<h3 id=\"安装软件\"><a href=\"#安装软件\" class=\"headerlink\" title=\"安装软件\"></a>安装软件</h3><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">pacman -S mingw64/mingw-w64-x86_64-gcc</div></pre></td></tr></table></figure>\n<p>安装mingw-gcc软件包</p>\n<h3 id=\"卸载软件\"><a href=\"#卸载软件\" class=\"headerlink\" title=\"卸载软件\"></a>卸载软件</h3><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">pacman -R gcc</div></pre></td></tr></table></figure>\n<p>卸载gcc软件包</p>\n<h2 id=\"准备开发环境\"><a href=\"#准备开发环境\" class=\"headerlink\" title=\"准备开发环境\"></a>准备开发环境</h2><h3 id=\"安装mingw-gcc\"><a href=\"#安装mingw-gcc\" class=\"headerlink\" title=\"安装mingw-gcc\"></a>安装mingw-gcc</h3><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">pacman -S mingw-w64-x86_64-gcc</div></pre></td></tr></table></figure>\n<p>注意，msys2存在msys和mingw两种版本的gcc软件包，默认是msys版本的软件包。该软件包编译出的应用程序依赖msys-2.0.dll文件，因此安装mingw64版本的gcc软件包。安装mingw版本的gcc时，先重命名msys2安装目录下的mingw64.exe和mingw64.ini两个文件(本文在这两个文件名前添加_前缀，如果不修改mingw64.ini文件，也可以正常安装gcc软件包，但无法执行重命名后的mingw64.exe文件)，否则将报错。</p>\n<p>安装完 mingw-w64-x86_64-gcc后，通过_mingw64.exe文件启动交互界面，否则如果通过msys2.exe启动交互界面，将发现gcc命令无法执行的错误。</p>\n<p><strong>目标</strong>：测试gcc是否正常工作<br>创建如下工程：<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">$ tree demo</div><div class=\"line\">demo</div><div class=\"line\">└── demo.c</div></pre></td></tr></table></figure></p>\n<p>demo.c文件的内容：</p>\n<figure class=\"highlight c\"><table><tr><td class=\"code\"><pre><div class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">include</span> <span class=\"meta-string\">&lt;stdio&gt;</span></span></div><div class=\"line\"></div><div class=\"line\"><span class=\"function\"><span class=\"keyword\">int</span> <span class=\"title\">main</span><span class=\"params\">(<span class=\"keyword\">int</span> argc, <span class=\"keyword\">char</span> *argv[])</span></div><div class=\"line\"></span>&#123;</div><div class=\"line\">    <span class=\"built_in\">printf</span>(<span class=\"string\">\"Hello, baby\"</span>);</div><div class=\"line\">    <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<p>编译工程：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">gcc demo.c -o demo.exe</div></pre></td></tr></table></figure>\n<p>双击demo.exe文件，不会提示依赖 msys-2.0.dll文件，可以正常运行。</p>\n<h3 id=\"编译ceph-dokan\"><a href=\"#编译ceph-dokan\" class=\"headerlink\" title=\"编译ceph-dokan\"></a>编译ceph-dokan</h3><p><strong>编译boost库</strong></p>\n<p><a href=\"https://sourceforge.net/projects/boost/?source=typ_redirect\">下载</a>boost1.60.0代码</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">unzip boost_1_60_0.zip</div><div class=\"line\">cd boost_1_60_0/</div><div class=\"line\">./bootstrap.bat mingw</div><div class=\"line\">./b2 toolset=gcc --with-system</div></pre></td></tr></table></figure>\n<p>此处，只编译boost的system模块</p>\n<p><strong>编译ceph-dokan</strong></p>\n<p>修改Makefile，指定boost路径</p>\n<p>修改编译选项 -fpermi</p>\n<p>错误1：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">In file included from ./include/int_types.h:30:0,</div><div class=\"line\">                 from ./include/types.h:18,</div><div class=\"line\">                 from auth/Crypto.h:18,</div><div class=\"line\">                 from libcephfs.cc:20:</div><div class=\"line\">./mingw_include/linux/types.h:13:24: fatal error: parts/time.h: No such file or directory</div><div class=\"line\"> #include &lt;parts/time.h&gt;</div><div class=\"line\">                        ^</div><div class=\"line\">compilation terminated.</div><div class=\"line\">make: *** [Makefile:13：libcephfs.o] 错误 1</div></pre></td></tr></table></figure>\n<hr>\n<p><strong>版权声明</strong></p>\n<p>本作品采用<a href=\"https://creativecommons.org/licenses/by/2.5/cn/\">知识共享署名2.5 中国大陆许可协议</a>进行许可，欢迎转载，但转载请注明来自<a href=\"https://wuxiangwei.cn/\"><em>Shanno’s Blog</em></a>，并保持转载后文章内容的完整。本人保留所有版权相关权利，侵权必究。</p>"},{"title":"搭建nfs环境","date":"2014-09-06T09:15:45.000Z","_content":"\n\n通过VirtualBox创建两台虚拟机client1和client2，这两台虚拟机和物理主机组成一个网络。将物理主机作为NFS的服务端，虚拟机client1和client2作为NFS的客户端。 物理主机装Mint系统，虚拟机中装Fedora 20系统。\n\n<!--more-->\n\n\n### 修改client的运行级别\n\n安装的Fedora系统启动时默认进入图形界面，由于图形界面比较消耗资源，并且我电脑的性能有限。为此，希望系统启动时可以进入控制台界面。\n\n### 修改虚拟机的网络连接\n\nVirtualBox创建的虚拟机默认使用NAT方式连接网络，这种模式下虚拟机可以访问主机，并且通过主机访问外网。但是，主机不能访问虚拟机，虚拟机之间也不能相互访问。修改虚拟机的连接方式为桥接模式，这种方式虚拟机可以直接连接到主机连接的交换机上。\n\n设置方法参考[《VirtualBox网络设置详解》](http://reverland.bitbucket.org/VirtualBox_net.html)。\n\n\n## NFS服务端配置\n\n### 安装nfs-kernel-server\n\n```\nsudo apt-get install nfs-kernel-server\nshanno@taurus-p245 ~ $ nfs\nnfsdcltrack  nfsidmap     nfsiostat    nfsstat\n```\n安装完成后，可以发现多了上面这些和nfs相关的可执行命令。\n\n### 指定共享目录\n\n```\nshanno@taurus-p245 ~ $ cat /etc/exports \n# /etc/exports: the access control list for filesystems which may be exported\n#       to NFS clients.  See exports(5).\n#\n# Example for NFSv2 and NFSv3:\n# /srv/homes       hostname1(rw,sync,no_subtree_check) hostname2(ro,sync,no_subtree_check)\n#\n# Example for NFSv4:\n# /srv/nfs4        gss/krb5i(rw,sync,fsid=0,crossmnt,no_subtree_check)\n# /srv/nfs4/homes  gss/krb5i(rw,sync,no_subtree_check)\n#\n/home/shanno/vshared *(insecure,rw,sync,no_root_squash)\n```\n指定共享目录以及访问权限。\n\n\n```\nshanno@taurus-p245 ~ $ sudo /etc/init.d/nfs-kernel-server restart\nshanno@taurus-p245 ~/tmp $ sudo mount -t nfs localhost:/home/shanno/vshared vshared_nfs/\n```\n重启nfs进程，并在本地挂载测试。\n\n\n## NFS客户端操作\n\n### 查看NFS共享目录\n\n```\n[root@client1 ~]# rpm -q showmount\npackage showmount is not installed\n[root@client1 ~]# yum install showmount\n```\n注意：fedora查询命令是否已经安装和安装命令的命令。\n\n```\n[root@client1 ~]# showmount -e 192.168.1.102\nExport list for 192.168.1.102:\n/home/shanno/vshared 192.168.1.*\n```\n通过showmount命令，我们可以查看给定的NFS服务器(192.168.1.102)导出的共享目录。\n\n### 挂载共享目录到本地\n\n```\n[root@client2 temp]# pwd\n/root/temp\n[root@client2 temp]# ls\nvshared_nfs\n[root@client2 temp]# mount -t nfs 192.168.1.100:/home/shanno/vshared vshared_nfs/\n```\n将NFS共享目录挂载到本地的*/root/temp/vshared_nfs*目录，使用*mount*命令挂载时通过*-t*选项指定文件系统类型为nfs。\n\n```\n[root@client2 temp]# mount -t nfs 192.168.1.100:/home/shanno/vshared/白马湖车展 vshared_nfs/\n```\n我们也可以将NFS导出目录下的某个子目录挂载到本地，如上，将导出目录下的*白马湖车展*子目录挂载到*/root/temp/vshared_nfs*目录。\n\n```\n[root@client2 vshared_nfs]# mount | egrep nfs\n192.168.1.100:/home/shanno/vshared on /root/temp/vshared_nfs type nfs4 (rw,relatime,vers=4.0,rsize=524288,wsize=524288,namlen=255,hard,proto=tcp,port=0,timeo=600,retrans=2,sec=sys,clientaddr=192.168.1.103,local_lock=none,addr=192.168.1.100)\n```\n查看nfs目录挂载情况。\n\n**参考资料**\n\n1、[鳥哥的 Linux 私房菜](http://linux.vbird.org/linux_server/0330nfs.php)\n\n\n\n","source":"_posts/Linux/nfs-setup.md","raw":"title: 搭建nfs环境\ndate: 2014-09-06 17:15:45\ncategories: [Linux]\ntags: nfs\n---\n\n\n通过VirtualBox创建两台虚拟机client1和client2，这两台虚拟机和物理主机组成一个网络。将物理主机作为NFS的服务端，虚拟机client1和client2作为NFS的客户端。 物理主机装Mint系统，虚拟机中装Fedora 20系统。\n\n<!--more-->\n\n\n### 修改client的运行级别\n\n安装的Fedora系统启动时默认进入图形界面，由于图形界面比较消耗资源，并且我电脑的性能有限。为此，希望系统启动时可以进入控制台界面。\n\n### 修改虚拟机的网络连接\n\nVirtualBox创建的虚拟机默认使用NAT方式连接网络，这种模式下虚拟机可以访问主机，并且通过主机访问外网。但是，主机不能访问虚拟机，虚拟机之间也不能相互访问。修改虚拟机的连接方式为桥接模式，这种方式虚拟机可以直接连接到主机连接的交换机上。\n\n设置方法参考[《VirtualBox网络设置详解》](http://reverland.bitbucket.org/VirtualBox_net.html)。\n\n\n## NFS服务端配置\n\n### 安装nfs-kernel-server\n\n```\nsudo apt-get install nfs-kernel-server\nshanno@taurus-p245 ~ $ nfs\nnfsdcltrack  nfsidmap     nfsiostat    nfsstat\n```\n安装完成后，可以发现多了上面这些和nfs相关的可执行命令。\n\n### 指定共享目录\n\n```\nshanno@taurus-p245 ~ $ cat /etc/exports \n# /etc/exports: the access control list for filesystems which may be exported\n#       to NFS clients.  See exports(5).\n#\n# Example for NFSv2 and NFSv3:\n# /srv/homes       hostname1(rw,sync,no_subtree_check) hostname2(ro,sync,no_subtree_check)\n#\n# Example for NFSv4:\n# /srv/nfs4        gss/krb5i(rw,sync,fsid=0,crossmnt,no_subtree_check)\n# /srv/nfs4/homes  gss/krb5i(rw,sync,no_subtree_check)\n#\n/home/shanno/vshared *(insecure,rw,sync,no_root_squash)\n```\n指定共享目录以及访问权限。\n\n\n```\nshanno@taurus-p245 ~ $ sudo /etc/init.d/nfs-kernel-server restart\nshanno@taurus-p245 ~/tmp $ sudo mount -t nfs localhost:/home/shanno/vshared vshared_nfs/\n```\n重启nfs进程，并在本地挂载测试。\n\n\n## NFS客户端操作\n\n### 查看NFS共享目录\n\n```\n[root@client1 ~]# rpm -q showmount\npackage showmount is not installed\n[root@client1 ~]# yum install showmount\n```\n注意：fedora查询命令是否已经安装和安装命令的命令。\n\n```\n[root@client1 ~]# showmount -e 192.168.1.102\nExport list for 192.168.1.102:\n/home/shanno/vshared 192.168.1.*\n```\n通过showmount命令，我们可以查看给定的NFS服务器(192.168.1.102)导出的共享目录。\n\n### 挂载共享目录到本地\n\n```\n[root@client2 temp]# pwd\n/root/temp\n[root@client2 temp]# ls\nvshared_nfs\n[root@client2 temp]# mount -t nfs 192.168.1.100:/home/shanno/vshared vshared_nfs/\n```\n将NFS共享目录挂载到本地的*/root/temp/vshared_nfs*目录，使用*mount*命令挂载时通过*-t*选项指定文件系统类型为nfs。\n\n```\n[root@client2 temp]# mount -t nfs 192.168.1.100:/home/shanno/vshared/白马湖车展 vshared_nfs/\n```\n我们也可以将NFS导出目录下的某个子目录挂载到本地，如上，将导出目录下的*白马湖车展*子目录挂载到*/root/temp/vshared_nfs*目录。\n\n```\n[root@client2 vshared_nfs]# mount | egrep nfs\n192.168.1.100:/home/shanno/vshared on /root/temp/vshared_nfs type nfs4 (rw,relatime,vers=4.0,rsize=524288,wsize=524288,namlen=255,hard,proto=tcp,port=0,timeo=600,retrans=2,sec=sys,clientaddr=192.168.1.103,local_lock=none,addr=192.168.1.100)\n```\n查看nfs目录挂载情况。\n\n**参考资料**\n\n1、[鳥哥的 Linux 私房菜](http://linux.vbird.org/linux_server/0330nfs.php)\n\n\n\n","slug":"Linux/nfs-setup","published":1,"updated":"2016-12-15T07:18:40.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cix1kgauv001vdaei0sffj8h4","content":"<p>通过VirtualBox创建两台虚拟机client1和client2，这两台虚拟机和物理主机组成一个网络。将物理主机作为NFS的服务端，虚拟机client1和client2作为NFS的客户端。 物理主机装Mint系统，虚拟机中装Fedora 20系统。</p>\n<a id=\"more\"></a>\n<h3 id=\"修改client的运行级别\"><a href=\"#修改client的运行级别\" class=\"headerlink\" title=\"修改client的运行级别\"></a>修改client的运行级别</h3><p>安装的Fedora系统启动时默认进入图形界面，由于图形界面比较消耗资源，并且我电脑的性能有限。为此，希望系统启动时可以进入控制台界面。</p>\n<h3 id=\"修改虚拟机的网络连接\"><a href=\"#修改虚拟机的网络连接\" class=\"headerlink\" title=\"修改虚拟机的网络连接\"></a>修改虚拟机的网络连接</h3><p>VirtualBox创建的虚拟机默认使用NAT方式连接网络，这种模式下虚拟机可以访问主机，并且通过主机访问外网。但是，主机不能访问虚拟机，虚拟机之间也不能相互访问。修改虚拟机的连接方式为桥接模式，这种方式虚拟机可以直接连接到主机连接的交换机上。</p>\n<p>设置方法参考<a href=\"http://reverland.bitbucket.org/VirtualBox_net.html\" target=\"_blank\" rel=\"external\">《VirtualBox网络设置详解》</a>。</p>\n<h2 id=\"NFS服务端配置\"><a href=\"#NFS服务端配置\" class=\"headerlink\" title=\"NFS服务端配置\"></a>NFS服务端配置</h2><h3 id=\"安装nfs-kernel-server\"><a href=\"#安装nfs-kernel-server\" class=\"headerlink\" title=\"安装nfs-kernel-server\"></a>安装nfs-kernel-server</h3><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">sudo apt-get install nfs-kernel-server</div><div class=\"line\">shanno@taurus-p245 ~ $ nfs</div><div class=\"line\">nfsdcltrack  nfsidmap     nfsiostat    nfsstat</div></pre></td></tr></table></figure>\n<p>安装完成后，可以发现多了上面这些和nfs相关的可执行命令。</p>\n<h3 id=\"指定共享目录\"><a href=\"#指定共享目录\" class=\"headerlink\" title=\"指定共享目录\"></a>指定共享目录</h3><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">shanno@taurus-p245 ~ $ cat /etc/exports </div><div class=\"line\"># /etc/exports: the access control list for filesystems which may be exported</div><div class=\"line\">#       to NFS clients.  See exports(5).</div><div class=\"line\">#</div><div class=\"line\"># Example for NFSv2 and NFSv3:</div><div class=\"line\"># /srv/homes       hostname1(rw,sync,no_subtree_check) hostname2(ro,sync,no_subtree_check)</div><div class=\"line\">#</div><div class=\"line\"># Example for NFSv4:</div><div class=\"line\"># /srv/nfs4        gss/krb5i(rw,sync,fsid=0,crossmnt,no_subtree_check)</div><div class=\"line\"># /srv/nfs4/homes  gss/krb5i(rw,sync,no_subtree_check)</div><div class=\"line\">#</div><div class=\"line\">/home/shanno/vshared *(insecure,rw,sync,no_root_squash)</div></pre></td></tr></table></figure>\n<p>指定共享目录以及访问权限。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">shanno@taurus-p245 ~ $ sudo /etc/init.d/nfs-kernel-server restart</div><div class=\"line\">shanno@taurus-p245 ~/tmp $ sudo mount -t nfs localhost:/home/shanno/vshared vshared_nfs/</div></pre></td></tr></table></figure>\n<p>重启nfs进程，并在本地挂载测试。</p>\n<h2 id=\"NFS客户端操作\"><a href=\"#NFS客户端操作\" class=\"headerlink\" title=\"NFS客户端操作\"></a>NFS客户端操作</h2><h3 id=\"查看NFS共享目录\"><a href=\"#查看NFS共享目录\" class=\"headerlink\" title=\"查看NFS共享目录\"></a>查看NFS共享目录</h3><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">[root@client1 ~]# rpm -q showmount</div><div class=\"line\">package showmount is not installed</div><div class=\"line\">[root@client1 ~]# yum install showmount</div></pre></td></tr></table></figure>\n<p>注意：fedora查询命令是否已经安装和安装命令的命令。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">[root@client1 ~]# showmount -e 192.168.1.102</div><div class=\"line\">Export list for 192.168.1.102:</div><div class=\"line\">/home/shanno/vshared 192.168.1.*</div></pre></td></tr></table></figure>\n<p>通过showmount命令，我们可以查看给定的NFS服务器(192.168.1.102)导出的共享目录。</p>\n<h3 id=\"挂载共享目录到本地\"><a href=\"#挂载共享目录到本地\" class=\"headerlink\" title=\"挂载共享目录到本地\"></a>挂载共享目录到本地</h3><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">[root@client2 temp]# pwd</div><div class=\"line\">/root/temp</div><div class=\"line\">[root@client2 temp]# ls</div><div class=\"line\">vshared_nfs</div><div class=\"line\">[root@client2 temp]# mount -t nfs 192.168.1.100:/home/shanno/vshared vshared_nfs/</div></pre></td></tr></table></figure>\n<p>将NFS共享目录挂载到本地的<em>/root/temp/vshared_nfs</em>目录，使用<em>mount</em>命令挂载时通过<em>-t</em>选项指定文件系统类型为nfs。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">[root@client2 temp]# mount -t nfs 192.168.1.100:/home/shanno/vshared/白马湖车展 vshared_nfs/</div></pre></td></tr></table></figure>\n<p>我们也可以将NFS导出目录下的某个子目录挂载到本地，如上，将导出目录下的<em>白马湖车展</em>子目录挂载到<em>/root/temp/vshared_nfs</em>目录。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">[root@client2 vshared_nfs]# mount | egrep nfs</div><div class=\"line\">192.168.1.100:/home/shanno/vshared on /root/temp/vshared_nfs type nfs4 (rw,relatime,vers=4.0,rsize=524288,wsize=524288,namlen=255,hard,proto=tcp,port=0,timeo=600,retrans=2,sec=sys,clientaddr=192.168.1.103,local_lock=none,addr=192.168.1.100)</div></pre></td></tr></table></figure>\n<p>查看nfs目录挂载情况。</p>\n<p><strong>参考资料</strong></p>\n<p>1、<a href=\"http://linux.vbird.org/linux_server/0330nfs.php\" target=\"_blank\" rel=\"external\">鳥哥的 Linux 私房菜</a></p>\n<hr>\n<p><strong>版权声明</strong></p>\n<p>本作品采用<a href=\"https://creativecommons.org/licenses/by/2.5/cn/\" target=\"_blank\" rel=\"external\">知识共享署名2.5 中国大陆许可协议</a>进行许可，欢迎转载，但转载请注明来自<a href=\"https://wuxiangwei.cn/\"><em>Shanno’s Blog</em></a>，并保持转载后文章内容的完整。本人保留所有版权相关权利，侵权必究。</p>\n","excerpt":"<p>通过VirtualBox创建两台虚拟机client1和client2，这两台虚拟机和物理主机组成一个网络。将物理主机作为NFS的服务端，虚拟机client1和client2作为NFS的客户端。 物理主机装Mint系统，虚拟机中装Fedora 20系统。</p>","more":"<h3 id=\"修改client的运行级别\"><a href=\"#修改client的运行级别\" class=\"headerlink\" title=\"修改client的运行级别\"></a>修改client的运行级别</h3><p>安装的Fedora系统启动时默认进入图形界面，由于图形界面比较消耗资源，并且我电脑的性能有限。为此，希望系统启动时可以进入控制台界面。</p>\n<h3 id=\"修改虚拟机的网络连接\"><a href=\"#修改虚拟机的网络连接\" class=\"headerlink\" title=\"修改虚拟机的网络连接\"></a>修改虚拟机的网络连接</h3><p>VirtualBox创建的虚拟机默认使用NAT方式连接网络，这种模式下虚拟机可以访问主机，并且通过主机访问外网。但是，主机不能访问虚拟机，虚拟机之间也不能相互访问。修改虚拟机的连接方式为桥接模式，这种方式虚拟机可以直接连接到主机连接的交换机上。</p>\n<p>设置方法参考<a href=\"http://reverland.bitbucket.org/VirtualBox_net.html\">《VirtualBox网络设置详解》</a>。</p>\n<h2 id=\"NFS服务端配置\"><a href=\"#NFS服务端配置\" class=\"headerlink\" title=\"NFS服务端配置\"></a>NFS服务端配置</h2><h3 id=\"安装nfs-kernel-server\"><a href=\"#安装nfs-kernel-server\" class=\"headerlink\" title=\"安装nfs-kernel-server\"></a>安装nfs-kernel-server</h3><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">sudo apt-get install nfs-kernel-server</div><div class=\"line\">shanno@taurus-p245 ~ $ nfs</div><div class=\"line\">nfsdcltrack  nfsidmap     nfsiostat    nfsstat</div></pre></td></tr></table></figure>\n<p>安装完成后，可以发现多了上面这些和nfs相关的可执行命令。</p>\n<h3 id=\"指定共享目录\"><a href=\"#指定共享目录\" class=\"headerlink\" title=\"指定共享目录\"></a>指定共享目录</h3><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">shanno@taurus-p245 ~ $ cat /etc/exports </div><div class=\"line\"># /etc/exports: the access control list for filesystems which may be exported</div><div class=\"line\">#       to NFS clients.  See exports(5).</div><div class=\"line\">#</div><div class=\"line\"># Example for NFSv2 and NFSv3:</div><div class=\"line\"># /srv/homes       hostname1(rw,sync,no_subtree_check) hostname2(ro,sync,no_subtree_check)</div><div class=\"line\">#</div><div class=\"line\"># Example for NFSv4:</div><div class=\"line\"># /srv/nfs4        gss/krb5i(rw,sync,fsid=0,crossmnt,no_subtree_check)</div><div class=\"line\"># /srv/nfs4/homes  gss/krb5i(rw,sync,no_subtree_check)</div><div class=\"line\">#</div><div class=\"line\">/home/shanno/vshared *(insecure,rw,sync,no_root_squash)</div></pre></td></tr></table></figure>\n<p>指定共享目录以及访问权限。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">shanno@taurus-p245 ~ $ sudo /etc/init.d/nfs-kernel-server restart</div><div class=\"line\">shanno@taurus-p245 ~/tmp $ sudo mount -t nfs localhost:/home/shanno/vshared vshared_nfs/</div></pre></td></tr></table></figure>\n<p>重启nfs进程，并在本地挂载测试。</p>\n<h2 id=\"NFS客户端操作\"><a href=\"#NFS客户端操作\" class=\"headerlink\" title=\"NFS客户端操作\"></a>NFS客户端操作</h2><h3 id=\"查看NFS共享目录\"><a href=\"#查看NFS共享目录\" class=\"headerlink\" title=\"查看NFS共享目录\"></a>查看NFS共享目录</h3><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">[root@client1 ~]# rpm -q showmount</div><div class=\"line\">package showmount is not installed</div><div class=\"line\">[root@client1 ~]# yum install showmount</div></pre></td></tr></table></figure>\n<p>注意：fedora查询命令是否已经安装和安装命令的命令。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">[root@client1 ~]# showmount -e 192.168.1.102</div><div class=\"line\">Export list for 192.168.1.102:</div><div class=\"line\">/home/shanno/vshared 192.168.1.*</div></pre></td></tr></table></figure>\n<p>通过showmount命令，我们可以查看给定的NFS服务器(192.168.1.102)导出的共享目录。</p>\n<h3 id=\"挂载共享目录到本地\"><a href=\"#挂载共享目录到本地\" class=\"headerlink\" title=\"挂载共享目录到本地\"></a>挂载共享目录到本地</h3><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">[root@client2 temp]# pwd</div><div class=\"line\">/root/temp</div><div class=\"line\">[root@client2 temp]# ls</div><div class=\"line\">vshared_nfs</div><div class=\"line\">[root@client2 temp]# mount -t nfs 192.168.1.100:/home/shanno/vshared vshared_nfs/</div></pre></td></tr></table></figure>\n<p>将NFS共享目录挂载到本地的<em>/root/temp/vshared_nfs</em>目录，使用<em>mount</em>命令挂载时通过<em>-t</em>选项指定文件系统类型为nfs。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">[root@client2 temp]# mount -t nfs 192.168.1.100:/home/shanno/vshared/白马湖车展 vshared_nfs/</div></pre></td></tr></table></figure>\n<p>我们也可以将NFS导出目录下的某个子目录挂载到本地，如上，将导出目录下的<em>白马湖车展</em>子目录挂载到<em>/root/temp/vshared_nfs</em>目录。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">[root@client2 vshared_nfs]# mount | egrep nfs</div><div class=\"line\">192.168.1.100:/home/shanno/vshared on /root/temp/vshared_nfs type nfs4 (rw,relatime,vers=4.0,rsize=524288,wsize=524288,namlen=255,hard,proto=tcp,port=0,timeo=600,retrans=2,sec=sys,clientaddr=192.168.1.103,local_lock=none,addr=192.168.1.100)</div></pre></td></tr></table></figure>\n<p>查看nfs目录挂载情况。</p>\n<p><strong>参考资料</strong></p>\n<p>1、<a href=\"http://linux.vbird.org/linux_server/0330nfs.php\">鳥哥的 Linux 私房菜</a></p>\n<hr>\n<p><strong>版权声明</strong></p>\n<p>本作品采用<a href=\"https://creativecommons.org/licenses/by/2.5/cn/\">知识共享署名2.5 中国大陆许可协议</a>进行许可，欢迎转载，但转载请注明来自<a href=\"https://wuxiangwei.cn/\"><em>Shanno’s Blog</em></a>，并保持转载后文章内容的完整。本人保留所有版权相关权利，侵权必究。</p>"},{"title":"ntp时间同步","date":"2014-12-01T11:40:45.000Z","_content":"\n同步时间是分布式系统中最基础的要求，不论是openstack还是ceph都必须先让节点间的时间保持一致。\n\n<!--more-->\n\n下载ntp软件包\n```\nroot@rgw01:~# apt-get install ntp\n\n```\n\n调整ntp server时间\n```\nroot@rgw01:~# date\nMon Dec  1 17:02:03 EST 2014\n```\n\n修改ntp服务配置\n```\nroot@rgw01:~# cat /etc/ntp.conf \nrestrict 192.168.1.0 mask 255.255.0.0 nomodify\nserver 127.127.1.0\nfudge 127.127.1.0 stratum 10\n```\n重启ntp服务\n```\nroot@rgw01:~# service ntp restart \n```\n\n客户端设置定时器，定时同ntp服务端同步时间。\n```\nroot@snode01:cat /etc/crontab \n# m h dom mon dow user  command\n  1 *  *   *   *  root  /usr/sbin/ntpdate 192.168.7.201\n```\n\n\n\n","source":"_posts/Linux/ntp.md","raw":"title: \"ntp时间同步\"\ndate: 2014-12-01 19:40:45\ncategories: [Linux]\ntags: \n---\n\n同步时间是分布式系统中最基础的要求，不论是openstack还是ceph都必须先让节点间的时间保持一致。\n\n<!--more-->\n\n下载ntp软件包\n```\nroot@rgw01:~# apt-get install ntp\n\n```\n\n调整ntp server时间\n```\nroot@rgw01:~# date\nMon Dec  1 17:02:03 EST 2014\n```\n\n修改ntp服务配置\n```\nroot@rgw01:~# cat /etc/ntp.conf \nrestrict 192.168.1.0 mask 255.255.0.0 nomodify\nserver 127.127.1.0\nfudge 127.127.1.0 stratum 10\n```\n重启ntp服务\n```\nroot@rgw01:~# service ntp restart \n```\n\n客户端设置定时器，定时同ntp服务端同步时间。\n```\nroot@snode01:cat /etc/crontab \n# m h dom mon dow user  command\n  1 *  *   *   *  root  /usr/sbin/ntpdate 192.168.7.201\n```\n\n\n\n","slug":"Linux/ntp","published":1,"updated":"2016-12-15T07:18:40.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cix1kgauy001ydaeinerqmylh","content":"<p>同步时间是分布式系统中最基础的要求，不论是openstack还是ceph都必须先让节点间的时间保持一致。</p>\n<a id=\"more\"></a>\n<p>下载ntp软件包<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">root@rgw01:~# apt-get install ntp</div></pre></td></tr></table></figure></p>\n<p>调整ntp server时间<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">root@rgw01:~# date</div><div class=\"line\">Mon Dec  1 17:02:03 EST 2014</div></pre></td></tr></table></figure></p>\n<p>修改ntp服务配置<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">root@rgw01:~# cat /etc/ntp.conf </div><div class=\"line\">restrict 192.168.1.0 mask 255.255.0.0 nomodify</div><div class=\"line\">server 127.127.1.0</div><div class=\"line\">fudge 127.127.1.0 stratum 10</div></pre></td></tr></table></figure></p>\n<p>重启ntp服务<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">root@rgw01:~# service ntp restart</div></pre></td></tr></table></figure></p>\n<p>客户端设置定时器，定时同ntp服务端同步时间。<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">root@snode01:cat /etc/crontab </div><div class=\"line\"># m h dom mon dow user  command</div><div class=\"line\">  1 *  *   *   *  root  /usr/sbin/ntpdate 192.168.7.201</div></pre></td></tr></table></figure></p>\n<hr>\n<p><strong>版权声明</strong></p>\n<p>本作品采用<a href=\"https://creativecommons.org/licenses/by/2.5/cn/\" target=\"_blank\" rel=\"external\">知识共享署名2.5 中国大陆许可协议</a>进行许可，欢迎转载，但转载请注明来自<a href=\"https://wuxiangwei.cn/\"><em>Shanno’s Blog</em></a>，并保持转载后文章内容的完整。本人保留所有版权相关权利，侵权必究。</p>\n","excerpt":"<p>同步时间是分布式系统中最基础的要求，不论是openstack还是ceph都必须先让节点间的时间保持一致。</p>","more":"<p>下载ntp软件包<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">root@rgw01:~# apt-get install ntp</div></pre></td></tr></table></figure></p>\n<p>调整ntp server时间<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">root@rgw01:~# date</div><div class=\"line\">Mon Dec  1 17:02:03 EST 2014</div></pre></td></tr></table></figure></p>\n<p>修改ntp服务配置<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">root@rgw01:~# cat /etc/ntp.conf </div><div class=\"line\">restrict 192.168.1.0 mask 255.255.0.0 nomodify</div><div class=\"line\">server 127.127.1.0</div><div class=\"line\">fudge 127.127.1.0 stratum 10</div></pre></td></tr></table></figure></p>\n<p>重启ntp服务<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">root@rgw01:~# service ntp restart</div></pre></td></tr></table></figure></p>\n<p>客户端设置定时器，定时同ntp服务端同步时间。<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">root@snode01:cat /etc/crontab </div><div class=\"line\"># m h dom mon dow user  command</div><div class=\"line\">  1 *  *   *   *  root  /usr/sbin/ntpdate 192.168.7.201</div></pre></td></tr></table></figure></p>\n<hr>\n<p><strong>版权声明</strong></p>\n<p>本作品采用<a href=\"https://creativecommons.org/licenses/by/2.5/cn/\">知识共享署名2.5 中国大陆许可协议</a>进行许可，欢迎转载，但转载请注明来自<a href=\"https://wuxiangwei.cn/\"><em>Shanno’s Blog</em></a>，并保持转载后文章内容的完整。本人保留所有版权相关权利，侵权必究。</p>"},{"title":"ocfs2使用ceph块设备","date":"2015-03-05T04:50:57.000Z","_content":"\n\n系统最底层是ceph，ceph之上是tgt，tgt为ceph的rbd块提供iSCSI接口。最后，将iSCSI的lun作为ocfs2的共享磁盘。实际上，也可以不使用tgt，直接将rbd块通过内核模块映射到本地后作为ocfs2的共享磁盘。\n\n<!--more-->\n\n### tgt连接ceph块\n\n```\n<target iqn.2015-02.onestore.com:tc>\n    driver iscsi\n    bs-type rbd             # lun类型为rbd\n    backing-store test/tc05 # 导出ceph的test pool中tc05、tc06两个块\n    backing-store test/tc06\n</target>\n```\n在tgt的配置文件中增加上述配置，然后重启tgt即可。重启tgt后，使用tgt-admin -s命令查看target导出的lun。\n\n### 关闭rbd缓存\n\n关闭rbd缓存，否则mount文件系统时会报下面的错误。因为在当前节点格式化共享磁盘后，ocfs2文件系统的信息没有完全写入到磁盘而是停留在rbd客户端，其它节点无法感知。\n```\ninternal error Child process (/bin/mount -t ocfs2 /dev/disk/by-path/ip-127.0.0.1:3260-iscsi-iqn.2015-02.onestore.com:tc-lun-1 /vms/w01) unexpected exit status 1: mount.ocfs2: Stale NFS file handle while mounting /dev/sdh on /vms/w01. Check 'dmesg' for more information on this error.\n```\n\n关闭rbd缓存的方法是，在ceph.conf文件中添加下面配置。默认情况下缓存是开启的。\n```\n[client]\nrbd cache = false\n```\n\n### 参考资料\n\n1. [Start With the RBD Support for TGT](http://www.sebastien-han.fr/blog/2014/07/07/start-with-the-rbd-support-for-tgt/) \n\n","source":"_posts/Linux/ocfs2-ceph-tgt.md","raw":"title: ocfs2使用ceph块设备\ndate: 2015-03-05 12:50:57\ncategories: [Linux]\ntags: [OCFS2, Ceph, iSCSI]\n---\n\n\n系统最底层是ceph，ceph之上是tgt，tgt为ceph的rbd块提供iSCSI接口。最后，将iSCSI的lun作为ocfs2的共享磁盘。实际上，也可以不使用tgt，直接将rbd块通过内核模块映射到本地后作为ocfs2的共享磁盘。\n\n<!--more-->\n\n### tgt连接ceph块\n\n```\n<target iqn.2015-02.onestore.com:tc>\n    driver iscsi\n    bs-type rbd             # lun类型为rbd\n    backing-store test/tc05 # 导出ceph的test pool中tc05、tc06两个块\n    backing-store test/tc06\n</target>\n```\n在tgt的配置文件中增加上述配置，然后重启tgt即可。重启tgt后，使用tgt-admin -s命令查看target导出的lun。\n\n### 关闭rbd缓存\n\n关闭rbd缓存，否则mount文件系统时会报下面的错误。因为在当前节点格式化共享磁盘后，ocfs2文件系统的信息没有完全写入到磁盘而是停留在rbd客户端，其它节点无法感知。\n```\ninternal error Child process (/bin/mount -t ocfs2 /dev/disk/by-path/ip-127.0.0.1:3260-iscsi-iqn.2015-02.onestore.com:tc-lun-1 /vms/w01) unexpected exit status 1: mount.ocfs2: Stale NFS file handle while mounting /dev/sdh on /vms/w01. Check 'dmesg' for more information on this error.\n```\n\n关闭rbd缓存的方法是，在ceph.conf文件中添加下面配置。默认情况下缓存是开启的。\n```\n[client]\nrbd cache = false\n```\n\n### 参考资料\n\n1. [Start With the RBD Support for TGT](http://www.sebastien-han.fr/blog/2014/07/07/start-with-the-rbd-support-for-tgt/) \n\n","slug":"Linux/ocfs2-ceph-tgt","published":1,"updated":"2016-12-15T07:18:40.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cix1kgauz0020daeiuj76dni7","content":"<p>系统最底层是ceph，ceph之上是tgt，tgt为ceph的rbd块提供iSCSI接口。最后，将iSCSI的lun作为ocfs2的共享磁盘。实际上，也可以不使用tgt，直接将rbd块通过内核模块映射到本地后作为ocfs2的共享磁盘。</p>\n<a id=\"more\"></a>\n<h3 id=\"tgt连接ceph块\"><a href=\"#tgt连接ceph块\" class=\"headerlink\" title=\"tgt连接ceph块\"></a>tgt连接ceph块</h3><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">&lt;target iqn.2015-02.onestore.com:tc&gt;</div><div class=\"line\">    driver iscsi</div><div class=\"line\">    bs-type rbd             # lun类型为rbd</div><div class=\"line\">    backing-store test/tc05 # 导出ceph的test pool中tc05、tc06两个块</div><div class=\"line\">    backing-store test/tc06</div><div class=\"line\">&lt;/target&gt;</div></pre></td></tr></table></figure>\n<p>在tgt的配置文件中增加上述配置，然后重启tgt即可。重启tgt后，使用tgt-admin -s命令查看target导出的lun。</p>\n<h3 id=\"关闭rbd缓存\"><a href=\"#关闭rbd缓存\" class=\"headerlink\" title=\"关闭rbd缓存\"></a>关闭rbd缓存</h3><p>关闭rbd缓存，否则mount文件系统时会报下面的错误。因为在当前节点格式化共享磁盘后，ocfs2文件系统的信息没有完全写入到磁盘而是停留在rbd客户端，其它节点无法感知。<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">internal error Child process (/bin/mount -t ocfs2 /dev/disk/by-path/ip-127.0.0.1:3260-iscsi-iqn.2015-02.onestore.com:tc-lun-1 /vms/w01) unexpected exit status 1: mount.ocfs2: Stale NFS file handle while mounting /dev/sdh on /vms/w01. Check &apos;dmesg&apos; for more information on this error.</div></pre></td></tr></table></figure></p>\n<p>关闭rbd缓存的方法是，在ceph.conf文件中添加下面配置。默认情况下缓存是开启的。<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">[client]</div><div class=\"line\">rbd cache = false</div></pre></td></tr></table></figure></p>\n<h3 id=\"参考资料\"><a href=\"#参考资料\" class=\"headerlink\" title=\"参考资料\"></a>参考资料</h3><ol>\n<li><a href=\"http://www.sebastien-han.fr/blog/2014/07/07/start-with-the-rbd-support-for-tgt/\" target=\"_blank\" rel=\"external\">Start With the RBD Support for TGT</a> </li>\n</ol>\n<hr>\n<p><strong>版权声明</strong></p>\n<p>本作品采用<a href=\"https://creativecommons.org/licenses/by/2.5/cn/\" target=\"_blank\" rel=\"external\">知识共享署名2.5 中国大陆许可协议</a>进行许可，欢迎转载，但转载请注明来自<a href=\"https://wuxiangwei.cn/\"><em>Shanno’s Blog</em></a>，并保持转载后文章内容的完整。本人保留所有版权相关权利，侵权必究。</p>\n","excerpt":"<p>系统最底层是ceph，ceph之上是tgt，tgt为ceph的rbd块提供iSCSI接口。最后，将iSCSI的lun作为ocfs2的共享磁盘。实际上，也可以不使用tgt，直接将rbd块通过内核模块映射到本地后作为ocfs2的共享磁盘。</p>","more":"<h3 id=\"tgt连接ceph块\"><a href=\"#tgt连接ceph块\" class=\"headerlink\" title=\"tgt连接ceph块\"></a>tgt连接ceph块</h3><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">&lt;target iqn.2015-02.onestore.com:tc&gt;</div><div class=\"line\">    driver iscsi</div><div class=\"line\">    bs-type rbd             # lun类型为rbd</div><div class=\"line\">    backing-store test/tc05 # 导出ceph的test pool中tc05、tc06两个块</div><div class=\"line\">    backing-store test/tc06</div><div class=\"line\">&lt;/target&gt;</div></pre></td></tr></table></figure>\n<p>在tgt的配置文件中增加上述配置，然后重启tgt即可。重启tgt后，使用tgt-admin -s命令查看target导出的lun。</p>\n<h3 id=\"关闭rbd缓存\"><a href=\"#关闭rbd缓存\" class=\"headerlink\" title=\"关闭rbd缓存\"></a>关闭rbd缓存</h3><p>关闭rbd缓存，否则mount文件系统时会报下面的错误。因为在当前节点格式化共享磁盘后，ocfs2文件系统的信息没有完全写入到磁盘而是停留在rbd客户端，其它节点无法感知。<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">internal error Child process (/bin/mount -t ocfs2 /dev/disk/by-path/ip-127.0.0.1:3260-iscsi-iqn.2015-02.onestore.com:tc-lun-1 /vms/w01) unexpected exit status 1: mount.ocfs2: Stale NFS file handle while mounting /dev/sdh on /vms/w01. Check &apos;dmesg&apos; for more information on this error.</div></pre></td></tr></table></figure></p>\n<p>关闭rbd缓存的方法是，在ceph.conf文件中添加下面配置。默认情况下缓存是开启的。<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">[client]</div><div class=\"line\">rbd cache = false</div></pre></td></tr></table></figure></p>\n<h3 id=\"参考资料\"><a href=\"#参考资料\" class=\"headerlink\" title=\"参考资料\"></a>参考资料</h3><ol>\n<li><a href=\"http://www.sebastien-han.fr/blog/2014/07/07/start-with-the-rbd-support-for-tgt/\">Start With the RBD Support for TGT</a> </li>\n</ol>\n<hr>\n<p><strong>版权声明</strong></p>\n<p>本作品采用<a href=\"https://creativecommons.org/licenses/by/2.5/cn/\">知识共享署名2.5 中国大陆许可协议</a>进行许可，欢迎转载，但转载请注明来自<a href=\"https://wuxiangwei.cn/\"><em>Shanno’s Blog</em></a>，并保持转载后文章内容的完整。本人保留所有版权相关权利，侵权必究。</p>"},{"title":"搭建ocfs2环境","date":"2014-09-19T09:05:11.000Z","_content":"\nOCFS2是基于共享磁盘的集群文件系统，它在一块共享磁盘上创建OCFS2文件系统，让集群中的其它节点可以对磁盘进行读写操作。OCFS2由两部分内容构成，一部分实现文件系统功能，位于VFS之下和Ext4同级别；另一部分实现集群节点的管理。\n\n![](http://images.cnitblog.com/blog/571795/201409/191946064093699.jpg)\n\n如上图所示，测试环境中OCFS2集群由三台服务器组成。我们将cvm服务器中的/dev/sda8分区作为共享磁盘，共享磁盘通过iSCSI共享给client01和client02服务器。\n\n<!--more-->\n\n### 共享磁盘\n\n如果cvm节点中没有单独的磁盘分区，可以参考[Linux命令总结：fdisk](http://www.cnblogs.com/shanno/p/3973366.html)从现有的磁盘分区中划分出一个新分区。测试环境中，我们将cvm节点的/dev/sd8分区作为共享磁盘。磁盘分区通过iSCSI挂给集群中的其它节点，关于iSCSI的用法可以参考[iSCSI:环境搭建](http://www.cnblogs.com/shanno/p/3979675.html)一文。\n\n另外，值得注意的是**cvm节点自己也要通过iSCSI连接共享磁盘，并且mount时挂载iscsi共享磁盘sdb而不是sda8**。否则，在cvm节点写入的数据不能同步到其它节点，并可能导致节点重启。\n\n### 配置集群\n\nOCFS2集群中每个节点的配置都相同，因此只要在其中一个节点中准备好配置，然后scp到其余节点即可。\n```\nroot@cvm:~# cat /etc/ocfs2/cluster.conf \ncluster:\n        node_count = 3            <== 集群节点数目\n        name = ocfs2              <== 集群名字\nnode:\n        ip_port = 777\n        ip_address = 192.168.7.10\n        number = 0                <== 节点编号\n        name = client01           <== 节点名字\n        cluster = ocfs2\nnode:\n        ip_port = 777\n        ip_address = 192.168.7.11\n        number = 1\n        name = client02\n        cluster = ocfs2\nnode:\n        ip_port = 777\n        ip_address = 192.168.7.8\n        number = 2\n        name = cvm\n        cluster = ocfs2\n```\n注意：如果粗心大意地将任意两个节点的编号写成一样，那么执行*/etc/init.d/o2cb online ocfs2*命令时会出现**o2cb_ctl: Internal logic failure while adding node cvm**的错误信息。\n\n### 启动OCFS2服务\n\n加载OCFS2服务\n```\nroot@cvm:~# /etc/init.d/o2cb load\nLoading filesystem \"configfs\": OK\nMounting configfs filesystem at /sys/kernel/config: OK\nLoading stack plugin \"o2cb\": OK\nLoading filesystem \"ocfs2_dlmfs\": OK\nCreating directory '/dlm': OK\nMounting ocfs2_dlmfs filesystem at /dlm: OK\n```\n\n启动集群，只有启动集群后才可以格式化共享磁盘。\n```\nroot@cvm:~# /etc/init.d/o2cb online \nSetting cluster stack \"o2cb\": OK\nStarting O2CB cluster ocfs2: OK\nroot@cvm:~# /etc/init.d/o2cb start \n```\n\n查看集群状态\n```\nroot@client02:~# /etc/init.d/o2cb status \nDriver for \"configfs\": Loaded\nFilesystem \"configfs\": Mounted\nStack glue driver: Loaded\nStack plugin \"o2cb\": Loaded\nDriver for \"ocfs2_dlmfs\": Loaded\nFilesystem \"ocfs2_dlmfs\": Mounted\nChecking O2CB cluster ocfs2: Online\nHeartbeat dead threshold = 31\n  Network idle timeout: 30000\n  Network keepalive delay: 2000\n  Network reconnect delay: 2000\nChecking O2CB heartbeat: Active\n```\n如果状态中提示**Checking O2CB heartbeat: Not active**信息，那么说明还没挂载共享磁盘。\n\n### 挂载磁盘\n\n在其中一个节点上，将共享磁盘格式化成ocfs2格式。 格式化命令中，-N代表集群允许的最大节点数目。\n```\nroot@cvm:~# mkfs -t ocfs2 -N 4 /dev/sda8\n```\n\n在每个节点中将共享磁盘挂载到一个挂载点上，这跟挂载Ext4文件系统一致。测试中，我们将磁盘都挂载到test_ocfs2目录。\n```\nroot@cvm:~# mount -t ocfs2 /dev/sdb test_ocfs2/\nroot@client01:~# mount -t ocfs2 /dev/sdb test_ocfs2/\nroot@client02:~# mount -t ocfs2 /dev/sdb test_ocfs2/\n```\n\n查看挂载情况\n```\nroot@client01:~# mounted.ocfs2 -f\nDevice                FS     Nodes\n/dev/sdb              ocfs2  client01, client02, cvm\n```\n\n最后，在每个节点中创建一个和自己主机名相同的目录，任意一个节点都可以看到其它节点创建的目录。\n```\nroot@cvm:# ls test_ocfs2 \nclient01  client02  cvm\n```\n目录client01在主机client01中创建，目录client02在主机client02中创建，目录cvm在主机cvm中创建。但在每个节点中都能够看到其它节点创建的目录。\n\n### 其它\n\n日志路径： /var/log/syslog\n\n![](http://images.cnitblog.com/blog/571795/201409/191946290501081.jpg)\n\n实际应用中，单独使用一台服务器的磁盘总是捉襟见肘，并且存在单点故障的问题。这时，可以通过Ceph提供虚拟共享磁盘(即**rbd块**)给ocfs2集群使用。\n\n参考文献\n\n1. [OCFS2在Linux下的配置文档](http://www.cnblogs.com/zhangpengme/archive/2011/12/29/2306362.html)          \n","source":"_posts/Linux/ocfs2-setup.md","raw":"title: 搭建ocfs2环境\ndate: 2014-09-19 17:05:11\ncategories: [Linux]\ntags: [OCFS2]\n---\n\nOCFS2是基于共享磁盘的集群文件系统，它在一块共享磁盘上创建OCFS2文件系统，让集群中的其它节点可以对磁盘进行读写操作。OCFS2由两部分内容构成，一部分实现文件系统功能，位于VFS之下和Ext4同级别；另一部分实现集群节点的管理。\n\n![](http://images.cnitblog.com/blog/571795/201409/191946064093699.jpg)\n\n如上图所示，测试环境中OCFS2集群由三台服务器组成。我们将cvm服务器中的/dev/sda8分区作为共享磁盘，共享磁盘通过iSCSI共享给client01和client02服务器。\n\n<!--more-->\n\n### 共享磁盘\n\n如果cvm节点中没有单独的磁盘分区，可以参考[Linux命令总结：fdisk](http://www.cnblogs.com/shanno/p/3973366.html)从现有的磁盘分区中划分出一个新分区。测试环境中，我们将cvm节点的/dev/sd8分区作为共享磁盘。磁盘分区通过iSCSI挂给集群中的其它节点，关于iSCSI的用法可以参考[iSCSI:环境搭建](http://www.cnblogs.com/shanno/p/3979675.html)一文。\n\n另外，值得注意的是**cvm节点自己也要通过iSCSI连接共享磁盘，并且mount时挂载iscsi共享磁盘sdb而不是sda8**。否则，在cvm节点写入的数据不能同步到其它节点，并可能导致节点重启。\n\n### 配置集群\n\nOCFS2集群中每个节点的配置都相同，因此只要在其中一个节点中准备好配置，然后scp到其余节点即可。\n```\nroot@cvm:~# cat /etc/ocfs2/cluster.conf \ncluster:\n        node_count = 3            <== 集群节点数目\n        name = ocfs2              <== 集群名字\nnode:\n        ip_port = 777\n        ip_address = 192.168.7.10\n        number = 0                <== 节点编号\n        name = client01           <== 节点名字\n        cluster = ocfs2\nnode:\n        ip_port = 777\n        ip_address = 192.168.7.11\n        number = 1\n        name = client02\n        cluster = ocfs2\nnode:\n        ip_port = 777\n        ip_address = 192.168.7.8\n        number = 2\n        name = cvm\n        cluster = ocfs2\n```\n注意：如果粗心大意地将任意两个节点的编号写成一样，那么执行*/etc/init.d/o2cb online ocfs2*命令时会出现**o2cb_ctl: Internal logic failure while adding node cvm**的错误信息。\n\n### 启动OCFS2服务\n\n加载OCFS2服务\n```\nroot@cvm:~# /etc/init.d/o2cb load\nLoading filesystem \"configfs\": OK\nMounting configfs filesystem at /sys/kernel/config: OK\nLoading stack plugin \"o2cb\": OK\nLoading filesystem \"ocfs2_dlmfs\": OK\nCreating directory '/dlm': OK\nMounting ocfs2_dlmfs filesystem at /dlm: OK\n```\n\n启动集群，只有启动集群后才可以格式化共享磁盘。\n```\nroot@cvm:~# /etc/init.d/o2cb online \nSetting cluster stack \"o2cb\": OK\nStarting O2CB cluster ocfs2: OK\nroot@cvm:~# /etc/init.d/o2cb start \n```\n\n查看集群状态\n```\nroot@client02:~# /etc/init.d/o2cb status \nDriver for \"configfs\": Loaded\nFilesystem \"configfs\": Mounted\nStack glue driver: Loaded\nStack plugin \"o2cb\": Loaded\nDriver for \"ocfs2_dlmfs\": Loaded\nFilesystem \"ocfs2_dlmfs\": Mounted\nChecking O2CB cluster ocfs2: Online\nHeartbeat dead threshold = 31\n  Network idle timeout: 30000\n  Network keepalive delay: 2000\n  Network reconnect delay: 2000\nChecking O2CB heartbeat: Active\n```\n如果状态中提示**Checking O2CB heartbeat: Not active**信息，那么说明还没挂载共享磁盘。\n\n### 挂载磁盘\n\n在其中一个节点上，将共享磁盘格式化成ocfs2格式。 格式化命令中，-N代表集群允许的最大节点数目。\n```\nroot@cvm:~# mkfs -t ocfs2 -N 4 /dev/sda8\n```\n\n在每个节点中将共享磁盘挂载到一个挂载点上，这跟挂载Ext4文件系统一致。测试中，我们将磁盘都挂载到test_ocfs2目录。\n```\nroot@cvm:~# mount -t ocfs2 /dev/sdb test_ocfs2/\nroot@client01:~# mount -t ocfs2 /dev/sdb test_ocfs2/\nroot@client02:~# mount -t ocfs2 /dev/sdb test_ocfs2/\n```\n\n查看挂载情况\n```\nroot@client01:~# mounted.ocfs2 -f\nDevice                FS     Nodes\n/dev/sdb              ocfs2  client01, client02, cvm\n```\n\n最后，在每个节点中创建一个和自己主机名相同的目录，任意一个节点都可以看到其它节点创建的目录。\n```\nroot@cvm:# ls test_ocfs2 \nclient01  client02  cvm\n```\n目录client01在主机client01中创建，目录client02在主机client02中创建，目录cvm在主机cvm中创建。但在每个节点中都能够看到其它节点创建的目录。\n\n### 其它\n\n日志路径： /var/log/syslog\n\n![](http://images.cnitblog.com/blog/571795/201409/191946290501081.jpg)\n\n实际应用中，单独使用一台服务器的磁盘总是捉襟见肘，并且存在单点故障的问题。这时，可以通过Ceph提供虚拟共享磁盘(即**rbd块**)给ocfs2集群使用。\n\n参考文献\n\n1. [OCFS2在Linux下的配置文档](http://www.cnblogs.com/zhangpengme/archive/2011/12/29/2306362.html)          \n","slug":"Linux/ocfs2-setup","published":1,"updated":"2016-12-15T07:18:40.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cix1kgav30027daeih8mx5njv","content":"<p>OCFS2是基于共享磁盘的集群文件系统，它在一块共享磁盘上创建OCFS2文件系统，让集群中的其它节点可以对磁盘进行读写操作。OCFS2由两部分内容构成，一部分实现文件系统功能，位于VFS之下和Ext4同级别；另一部分实现集群节点的管理。</p>\n<p><img src=\"http://images.cnitblog.com/blog/571795/201409/191946064093699.jpg\" alt=\"\"></p>\n<p>如上图所示，测试环境中OCFS2集群由三台服务器组成。我们将cvm服务器中的/dev/sda8分区作为共享磁盘，共享磁盘通过iSCSI共享给client01和client02服务器。</p>\n<a id=\"more\"></a>\n<h3 id=\"共享磁盘\"><a href=\"#共享磁盘\" class=\"headerlink\" title=\"共享磁盘\"></a>共享磁盘</h3><p>如果cvm节点中没有单独的磁盘分区，可以参考<a href=\"http://www.cnblogs.com/shanno/p/3973366.html\" target=\"_blank\" rel=\"external\">Linux命令总结：fdisk</a>从现有的磁盘分区中划分出一个新分区。测试环境中，我们将cvm节点的/dev/sd8分区作为共享磁盘。磁盘分区通过iSCSI挂给集群中的其它节点，关于iSCSI的用法可以参考<a href=\"http://www.cnblogs.com/shanno/p/3979675.html\" target=\"_blank\" rel=\"external\">iSCSI:环境搭建</a>一文。</p>\n<p>另外，值得注意的是<strong>cvm节点自己也要通过iSCSI连接共享磁盘，并且mount时挂载iscsi共享磁盘sdb而不是sda8</strong>。否则，在cvm节点写入的数据不能同步到其它节点，并可能导致节点重启。</p>\n<h3 id=\"配置集群\"><a href=\"#配置集群\" class=\"headerlink\" title=\"配置集群\"></a>配置集群</h3><p>OCFS2集群中每个节点的配置都相同，因此只要在其中一个节点中准备好配置，然后scp到其余节点即可。<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">root@cvm:~# cat /etc/ocfs2/cluster.conf </div><div class=\"line\">cluster:</div><div class=\"line\">        node_count = 3            &lt;== 集群节点数目</div><div class=\"line\">        name = ocfs2              &lt;== 集群名字</div><div class=\"line\">node:</div><div class=\"line\">        ip_port = 777</div><div class=\"line\">        ip_address = 192.168.7.10</div><div class=\"line\">        number = 0                &lt;== 节点编号</div><div class=\"line\">        name = client01           &lt;== 节点名字</div><div class=\"line\">        cluster = ocfs2</div><div class=\"line\">node:</div><div class=\"line\">        ip_port = 777</div><div class=\"line\">        ip_address = 192.168.7.11</div><div class=\"line\">        number = 1</div><div class=\"line\">        name = client02</div><div class=\"line\">        cluster = ocfs2</div><div class=\"line\">node:</div><div class=\"line\">        ip_port = 777</div><div class=\"line\">        ip_address = 192.168.7.8</div><div class=\"line\">        number = 2</div><div class=\"line\">        name = cvm</div><div class=\"line\">        cluster = ocfs2</div></pre></td></tr></table></figure></p>\n<p>注意：如果粗心大意地将任意两个节点的编号写成一样，那么执行<em>/etc/init.d/o2cb online ocfs2</em>命令时会出现<strong>o2cb_ctl: Internal logic failure while adding node cvm</strong>的错误信息。</p>\n<h3 id=\"启动OCFS2服务\"><a href=\"#启动OCFS2服务\" class=\"headerlink\" title=\"启动OCFS2服务\"></a>启动OCFS2服务</h3><p>加载OCFS2服务<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">root@cvm:~# /etc/init.d/o2cb load</div><div class=\"line\">Loading filesystem &quot;configfs&quot;: OK</div><div class=\"line\">Mounting configfs filesystem at /sys/kernel/config: OK</div><div class=\"line\">Loading stack plugin &quot;o2cb&quot;: OK</div><div class=\"line\">Loading filesystem &quot;ocfs2_dlmfs&quot;: OK</div><div class=\"line\">Creating directory &apos;/dlm&apos;: OK</div><div class=\"line\">Mounting ocfs2_dlmfs filesystem at /dlm: OK</div></pre></td></tr></table></figure></p>\n<p>启动集群，只有启动集群后才可以格式化共享磁盘。<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">root@cvm:~# /etc/init.d/o2cb online </div><div class=\"line\">Setting cluster stack &quot;o2cb&quot;: OK</div><div class=\"line\">Starting O2CB cluster ocfs2: OK</div><div class=\"line\">root@cvm:~# /etc/init.d/o2cb start</div></pre></td></tr></table></figure></p>\n<p>查看集群状态<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">root@client02:~# /etc/init.d/o2cb status </div><div class=\"line\">Driver for &quot;configfs&quot;: Loaded</div><div class=\"line\">Filesystem &quot;configfs&quot;: Mounted</div><div class=\"line\">Stack glue driver: Loaded</div><div class=\"line\">Stack plugin &quot;o2cb&quot;: Loaded</div><div class=\"line\">Driver for &quot;ocfs2_dlmfs&quot;: Loaded</div><div class=\"line\">Filesystem &quot;ocfs2_dlmfs&quot;: Mounted</div><div class=\"line\">Checking O2CB cluster ocfs2: Online</div><div class=\"line\">Heartbeat dead threshold = 31</div><div class=\"line\">  Network idle timeout: 30000</div><div class=\"line\">  Network keepalive delay: 2000</div><div class=\"line\">  Network reconnect delay: 2000</div><div class=\"line\">Checking O2CB heartbeat: Active</div></pre></td></tr></table></figure></p>\n<p>如果状态中提示<strong>Checking O2CB heartbeat: Not active</strong>信息，那么说明还没挂载共享磁盘。</p>\n<h3 id=\"挂载磁盘\"><a href=\"#挂载磁盘\" class=\"headerlink\" title=\"挂载磁盘\"></a>挂载磁盘</h3><p>在其中一个节点上，将共享磁盘格式化成ocfs2格式。 格式化命令中，-N代表集群允许的最大节点数目。<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">root@cvm:~# mkfs -t ocfs2 -N 4 /dev/sda8</div></pre></td></tr></table></figure></p>\n<p>在每个节点中将共享磁盘挂载到一个挂载点上，这跟挂载Ext4文件系统一致。测试中，我们将磁盘都挂载到test_ocfs2目录。<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">root@cvm:~# mount -t ocfs2 /dev/sdb test_ocfs2/</div><div class=\"line\">root@client01:~# mount -t ocfs2 /dev/sdb test_ocfs2/</div><div class=\"line\">root@client02:~# mount -t ocfs2 /dev/sdb test_ocfs2/</div></pre></td></tr></table></figure></p>\n<p>查看挂载情况<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">root@client01:~# mounted.ocfs2 -f</div><div class=\"line\">Device                FS     Nodes</div><div class=\"line\">/dev/sdb              ocfs2  client01, client02, cvm</div></pre></td></tr></table></figure></p>\n<p>最后，在每个节点中创建一个和自己主机名相同的目录，任意一个节点都可以看到其它节点创建的目录。<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">root@cvm:# ls test_ocfs2 </div><div class=\"line\">client01  client02  cvm</div></pre></td></tr></table></figure></p>\n<p>目录client01在主机client01中创建，目录client02在主机client02中创建，目录cvm在主机cvm中创建。但在每个节点中都能够看到其它节点创建的目录。</p>\n<h3 id=\"其它\"><a href=\"#其它\" class=\"headerlink\" title=\"其它\"></a>其它</h3><p>日志路径： /var/log/syslog</p>\n<p><img src=\"http://images.cnitblog.com/blog/571795/201409/191946290501081.jpg\" alt=\"\"></p>\n<p>实际应用中，单独使用一台服务器的磁盘总是捉襟见肘，并且存在单点故障的问题。这时，可以通过Ceph提供虚拟共享磁盘(即<strong>rbd块</strong>)给ocfs2集群使用。</p>\n<p>参考文献</p>\n<ol>\n<li><a href=\"http://www.cnblogs.com/zhangpengme/archive/2011/12/29/2306362.html\" target=\"_blank\" rel=\"external\">OCFS2在Linux下的配置文档</a>          </li>\n</ol>\n<hr>\n<p><strong>版权声明</strong></p>\n<p>本作品采用<a href=\"https://creativecommons.org/licenses/by/2.5/cn/\" target=\"_blank\" rel=\"external\">知识共享署名2.5 中国大陆许可协议</a>进行许可，欢迎转载，但转载请注明来自<a href=\"https://wuxiangwei.cn/\"><em>Shanno’s Blog</em></a>，并保持转载后文章内容的完整。本人保留所有版权相关权利，侵权必究。</p>\n","excerpt":"<p>OCFS2是基于共享磁盘的集群文件系统，它在一块共享磁盘上创建OCFS2文件系统，让集群中的其它节点可以对磁盘进行读写操作。OCFS2由两部分内容构成，一部分实现文件系统功能，位于VFS之下和Ext4同级别；另一部分实现集群节点的管理。</p>\n<p><img src=\"http://images.cnitblog.com/blog/571795/201409/191946064093699.jpg\" alt=\"\"></p>\n<p>如上图所示，测试环境中OCFS2集群由三台服务器组成。我们将cvm服务器中的/dev/sda8分区作为共享磁盘，共享磁盘通过iSCSI共享给client01和client02服务器。</p>","more":"<h3 id=\"共享磁盘\"><a href=\"#共享磁盘\" class=\"headerlink\" title=\"共享磁盘\"></a>共享磁盘</h3><p>如果cvm节点中没有单独的磁盘分区，可以参考<a href=\"http://www.cnblogs.com/shanno/p/3973366.html\">Linux命令总结：fdisk</a>从现有的磁盘分区中划分出一个新分区。测试环境中，我们将cvm节点的/dev/sd8分区作为共享磁盘。磁盘分区通过iSCSI挂给集群中的其它节点，关于iSCSI的用法可以参考<a href=\"http://www.cnblogs.com/shanno/p/3979675.html\">iSCSI:环境搭建</a>一文。</p>\n<p>另外，值得注意的是<strong>cvm节点自己也要通过iSCSI连接共享磁盘，并且mount时挂载iscsi共享磁盘sdb而不是sda8</strong>。否则，在cvm节点写入的数据不能同步到其它节点，并可能导致节点重启。</p>\n<h3 id=\"配置集群\"><a href=\"#配置集群\" class=\"headerlink\" title=\"配置集群\"></a>配置集群</h3><p>OCFS2集群中每个节点的配置都相同，因此只要在其中一个节点中准备好配置，然后scp到其余节点即可。<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">root@cvm:~# cat /etc/ocfs2/cluster.conf </div><div class=\"line\">cluster:</div><div class=\"line\">        node_count = 3            &lt;== 集群节点数目</div><div class=\"line\">        name = ocfs2              &lt;== 集群名字</div><div class=\"line\">node:</div><div class=\"line\">        ip_port = 777</div><div class=\"line\">        ip_address = 192.168.7.10</div><div class=\"line\">        number = 0                &lt;== 节点编号</div><div class=\"line\">        name = client01           &lt;== 节点名字</div><div class=\"line\">        cluster = ocfs2</div><div class=\"line\">node:</div><div class=\"line\">        ip_port = 777</div><div class=\"line\">        ip_address = 192.168.7.11</div><div class=\"line\">        number = 1</div><div class=\"line\">        name = client02</div><div class=\"line\">        cluster = ocfs2</div><div class=\"line\">node:</div><div class=\"line\">        ip_port = 777</div><div class=\"line\">        ip_address = 192.168.7.8</div><div class=\"line\">        number = 2</div><div class=\"line\">        name = cvm</div><div class=\"line\">        cluster = ocfs2</div></pre></td></tr></table></figure></p>\n<p>注意：如果粗心大意地将任意两个节点的编号写成一样，那么执行<em>/etc/init.d/o2cb online ocfs2</em>命令时会出现<strong>o2cb_ctl: Internal logic failure while adding node cvm</strong>的错误信息。</p>\n<h3 id=\"启动OCFS2服务\"><a href=\"#启动OCFS2服务\" class=\"headerlink\" title=\"启动OCFS2服务\"></a>启动OCFS2服务</h3><p>加载OCFS2服务<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">root@cvm:~# /etc/init.d/o2cb load</div><div class=\"line\">Loading filesystem &quot;configfs&quot;: OK</div><div class=\"line\">Mounting configfs filesystem at /sys/kernel/config: OK</div><div class=\"line\">Loading stack plugin &quot;o2cb&quot;: OK</div><div class=\"line\">Loading filesystem &quot;ocfs2_dlmfs&quot;: OK</div><div class=\"line\">Creating directory &apos;/dlm&apos;: OK</div><div class=\"line\">Mounting ocfs2_dlmfs filesystem at /dlm: OK</div></pre></td></tr></table></figure></p>\n<p>启动集群，只有启动集群后才可以格式化共享磁盘。<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">root@cvm:~# /etc/init.d/o2cb online </div><div class=\"line\">Setting cluster stack &quot;o2cb&quot;: OK</div><div class=\"line\">Starting O2CB cluster ocfs2: OK</div><div class=\"line\">root@cvm:~# /etc/init.d/o2cb start</div></pre></td></tr></table></figure></p>\n<p>查看集群状态<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">root@client02:~# /etc/init.d/o2cb status </div><div class=\"line\">Driver for &quot;configfs&quot;: Loaded</div><div class=\"line\">Filesystem &quot;configfs&quot;: Mounted</div><div class=\"line\">Stack glue driver: Loaded</div><div class=\"line\">Stack plugin &quot;o2cb&quot;: Loaded</div><div class=\"line\">Driver for &quot;ocfs2_dlmfs&quot;: Loaded</div><div class=\"line\">Filesystem &quot;ocfs2_dlmfs&quot;: Mounted</div><div class=\"line\">Checking O2CB cluster ocfs2: Online</div><div class=\"line\">Heartbeat dead threshold = 31</div><div class=\"line\">  Network idle timeout: 30000</div><div class=\"line\">  Network keepalive delay: 2000</div><div class=\"line\">  Network reconnect delay: 2000</div><div class=\"line\">Checking O2CB heartbeat: Active</div></pre></td></tr></table></figure></p>\n<p>如果状态中提示<strong>Checking O2CB heartbeat: Not active</strong>信息，那么说明还没挂载共享磁盘。</p>\n<h3 id=\"挂载磁盘\"><a href=\"#挂载磁盘\" class=\"headerlink\" title=\"挂载磁盘\"></a>挂载磁盘</h3><p>在其中一个节点上，将共享磁盘格式化成ocfs2格式。 格式化命令中，-N代表集群允许的最大节点数目。<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">root@cvm:~# mkfs -t ocfs2 -N 4 /dev/sda8</div></pre></td></tr></table></figure></p>\n<p>在每个节点中将共享磁盘挂载到一个挂载点上，这跟挂载Ext4文件系统一致。测试中，我们将磁盘都挂载到test_ocfs2目录。<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">root@cvm:~# mount -t ocfs2 /dev/sdb test_ocfs2/</div><div class=\"line\">root@client01:~# mount -t ocfs2 /dev/sdb test_ocfs2/</div><div class=\"line\">root@client02:~# mount -t ocfs2 /dev/sdb test_ocfs2/</div></pre></td></tr></table></figure></p>\n<p>查看挂载情况<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">root@client01:~# mounted.ocfs2 -f</div><div class=\"line\">Device                FS     Nodes</div><div class=\"line\">/dev/sdb              ocfs2  client01, client02, cvm</div></pre></td></tr></table></figure></p>\n<p>最后，在每个节点中创建一个和自己主机名相同的目录，任意一个节点都可以看到其它节点创建的目录。<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">root@cvm:# ls test_ocfs2 </div><div class=\"line\">client01  client02  cvm</div></pre></td></tr></table></figure></p>\n<p>目录client01在主机client01中创建，目录client02在主机client02中创建，目录cvm在主机cvm中创建。但在每个节点中都能够看到其它节点创建的目录。</p>\n<h3 id=\"其它\"><a href=\"#其它\" class=\"headerlink\" title=\"其它\"></a>其它</h3><p>日志路径： /var/log/syslog</p>\n<p><img src=\"http://images.cnitblog.com/blog/571795/201409/191946290501081.jpg\" alt=\"\"></p>\n<p>实际应用中，单独使用一台服务器的磁盘总是捉襟见肘，并且存在单点故障的问题。这时，可以通过Ceph提供虚拟共享磁盘(即<strong>rbd块</strong>)给ocfs2集群使用。</p>\n<p>参考文献</p>\n<ol>\n<li><a href=\"http://www.cnblogs.com/zhangpengme/archive/2011/12/29/2306362.html\">OCFS2在Linux下的配置文档</a>          </li>\n</ol>\n<hr>\n<p><strong>版权声明</strong></p>\n<p>本作品采用<a href=\"https://creativecommons.org/licenses/by/2.5/cn/\">知识共享署名2.5 中国大陆许可协议</a>进行许可，欢迎转载，但转载请注明来自<a href=\"https://wuxiangwei.cn/\"><em>Shanno’s Blog</em></a>，并保持转载后文章内容的完整。本人保留所有版权相关权利，侵权必究。</p>"},{"title":"Git Submodule（子模组）","date":"2016-10-09T05:53:09.000Z","toc":true,"_content":"\n```\nhexo-site    --- hexo-site版本库\n└── themes\n    └─nova --- nova版本库\n```\n\n需求：一个版本库引用其它版本库的文件。如上所示，hexo-site版本库引用nova版本库中的文件。\n\n\n<!--more-->\n\n\n## 基本操作\n\n### 添加Submodule\n\n```\ncd hexo-site/\ngit submodule add -f https://github.com/wuxiangwei/hexo-theme-nova.git themes/nova\n```\n执行添加子模块命令后，hexo-site目录中将生成1个新文件**.gitmodule**，文件记录submodule的引用信息，包括在当前项目的位置以及仓库所在。\n\n### 查看Submodule\n\n```\ngit submodule status \ndabe550a44b98194546515ec78f8ec8c11b41340 themes/nova (v0.1.2-13-gdabe550)\n```\n可以看到每个Submodule的commit id，这commit id是固定的，每次克隆主版本库时不变，外部版本库更新时不变。\n\n### 克隆带Submodule的版本库\n\n克隆带Submodule的版本库，并不能自动克隆Submodule的版本库。这个特点的好处是，克隆版本库速度快，数据没有冗余。\n\n```\ngit submodule init\ngit submodule update\n```\n使用上述两个命令能够克隆Submodule的版本库。\n\n### 同步Submodule版本库的修改\n\n使用场景：Submodule版本库的主线(hexo-theme-nova)已修改，但这修改没有同步到主版本库(hexo-site)。\n\nSubmodule版本库的主线的commit：\n```\nroot@bs-dev:~/repo/hexo-theme-nova# git log --pretty=oneline\n6dccc29d93b0928a402290a3d9a0d2b1fd57e031 关闭打赏页面\ndabe550a44b98194546515ec78f8ec8c11b41340 Merge pull request #15 from Jamling/dev\n```\n\n主版本库的Submodule版本的commit：\n```\nroot@bs-dev:~/repo/hexo-site# git submodule status \n-dabe550a44b98194546515ec78f8ec8c11b41340 themes/nova\n```\n\n克隆Submodule版本库时(执行git submodule update命令)，自动将Submodule版本库checkout到给定的commit，进入Submodule路径可以查看到版本库的状态。在主版本中同步Submodule修改的步骤如下:\n\n```\ncd hexo-site/themes/nova\ngit checkout master\n\ncd hexo-site\ngit status\n```\n进入Submodule版本库/hexo-site/themes/nova，checkout出master分支\n进入主版本库/hexo-site，发现theme/nova已修改，提交修改。\n\n\n## .gitmodule文件\n\n```\n[submodule \"themes/nova\"]\n\tpath = themes/nova\n\turl = https://github.com/wuxiangwei/hexo-theme-nova.git\n```\n添加Submodule后，将在hexo-site目录中生成.gitmodule文件用以记录Submodule的引用信息，包括在主版本库中的路径以及仓库地址。\n\n\n","source":"_posts/Other/git_submodule.md","raw":"title: Git Submodule（子模组）\ndate: 2016-10-09 13:53:09\ncategories: [Other]\ntags: [Git]\ntoc: true\n---\n\n```\nhexo-site    --- hexo-site版本库\n└── themes\n    └─nova --- nova版本库\n```\n\n需求：一个版本库引用其它版本库的文件。如上所示，hexo-site版本库引用nova版本库中的文件。\n\n\n<!--more-->\n\n\n## 基本操作\n\n### 添加Submodule\n\n```\ncd hexo-site/\ngit submodule add -f https://github.com/wuxiangwei/hexo-theme-nova.git themes/nova\n```\n执行添加子模块命令后，hexo-site目录中将生成1个新文件**.gitmodule**，文件记录submodule的引用信息，包括在当前项目的位置以及仓库所在。\n\n### 查看Submodule\n\n```\ngit submodule status \ndabe550a44b98194546515ec78f8ec8c11b41340 themes/nova (v0.1.2-13-gdabe550)\n```\n可以看到每个Submodule的commit id，这commit id是固定的，每次克隆主版本库时不变，外部版本库更新时不变。\n\n### 克隆带Submodule的版本库\n\n克隆带Submodule的版本库，并不能自动克隆Submodule的版本库。这个特点的好处是，克隆版本库速度快，数据没有冗余。\n\n```\ngit submodule init\ngit submodule update\n```\n使用上述两个命令能够克隆Submodule的版本库。\n\n### 同步Submodule版本库的修改\n\n使用场景：Submodule版本库的主线(hexo-theme-nova)已修改，但这修改没有同步到主版本库(hexo-site)。\n\nSubmodule版本库的主线的commit：\n```\nroot@bs-dev:~/repo/hexo-theme-nova# git log --pretty=oneline\n6dccc29d93b0928a402290a3d9a0d2b1fd57e031 关闭打赏页面\ndabe550a44b98194546515ec78f8ec8c11b41340 Merge pull request #15 from Jamling/dev\n```\n\n主版本库的Submodule版本的commit：\n```\nroot@bs-dev:~/repo/hexo-site# git submodule status \n-dabe550a44b98194546515ec78f8ec8c11b41340 themes/nova\n```\n\n克隆Submodule版本库时(执行git submodule update命令)，自动将Submodule版本库checkout到给定的commit，进入Submodule路径可以查看到版本库的状态。在主版本中同步Submodule修改的步骤如下:\n\n```\ncd hexo-site/themes/nova\ngit checkout master\n\ncd hexo-site\ngit status\n```\n进入Submodule版本库/hexo-site/themes/nova，checkout出master分支\n进入主版本库/hexo-site，发现theme/nova已修改，提交修改。\n\n\n## .gitmodule文件\n\n```\n[submodule \"themes/nova\"]\n\tpath = themes/nova\n\turl = https://github.com/wuxiangwei/hexo-theme-nova.git\n```\n添加Submodule后，将在hexo-site目录中生成.gitmodule文件用以记录Submodule的引用信息，包括在主版本库中的路径以及仓库地址。\n\n\n","slug":"Other/git_submodule","published":1,"updated":"2016-12-15T07:18:40.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cix1kgav4002adaei4fytglgw","content":"<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">hexo-site    --- hexo-site版本库</div><div class=\"line\">└── themes</div><div class=\"line\">    └─nova --- nova版本库</div></pre></td></tr></table></figure>\n<p>需求：一个版本库引用其它版本库的文件。如上所示，hexo-site版本库引用nova版本库中的文件。</p>\n<a id=\"more\"></a>\n<h2 id=\"基本操作\"><a href=\"#基本操作\" class=\"headerlink\" title=\"基本操作\"></a>基本操作</h2><h3 id=\"添加Submodule\"><a href=\"#添加Submodule\" class=\"headerlink\" title=\"添加Submodule\"></a>添加Submodule</h3><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">cd hexo-site/</div><div class=\"line\">git submodule add -f https://github.com/wuxiangwei/hexo-theme-nova.git themes/nova</div></pre></td></tr></table></figure>\n<p>执行添加子模块命令后，hexo-site目录中将生成1个新文件<strong>.gitmodule</strong>，文件记录submodule的引用信息，包括在当前项目的位置以及仓库所在。</p>\n<h3 id=\"查看Submodule\"><a href=\"#查看Submodule\" class=\"headerlink\" title=\"查看Submodule\"></a>查看Submodule</h3><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">git submodule status </div><div class=\"line\">dabe550a44b98194546515ec78f8ec8c11b41340 themes/nova (v0.1.2-13-gdabe550)</div></pre></td></tr></table></figure>\n<p>可以看到每个Submodule的commit id，这commit id是固定的，每次克隆主版本库时不变，外部版本库更新时不变。</p>\n<h3 id=\"克隆带Submodule的版本库\"><a href=\"#克隆带Submodule的版本库\" class=\"headerlink\" title=\"克隆带Submodule的版本库\"></a>克隆带Submodule的版本库</h3><p>克隆带Submodule的版本库，并不能自动克隆Submodule的版本库。这个特点的好处是，克隆版本库速度快，数据没有冗余。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">git submodule init</div><div class=\"line\">git submodule update</div></pre></td></tr></table></figure>\n<p>使用上述两个命令能够克隆Submodule的版本库。</p>\n<h3 id=\"同步Submodule版本库的修改\"><a href=\"#同步Submodule版本库的修改\" class=\"headerlink\" title=\"同步Submodule版本库的修改\"></a>同步Submodule版本库的修改</h3><p>使用场景：Submodule版本库的主线(hexo-theme-nova)已修改，但这修改没有同步到主版本库(hexo-site)。</p>\n<p>Submodule版本库的主线的commit：<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">root@bs-dev:~/repo/hexo-theme-nova# git log --pretty=oneline</div><div class=\"line\">6dccc29d93b0928a402290a3d9a0d2b1fd57e031 关闭打赏页面</div><div class=\"line\">dabe550a44b98194546515ec78f8ec8c11b41340 Merge pull request #15 from Jamling/dev</div></pre></td></tr></table></figure></p>\n<p>主版本库的Submodule版本的commit：<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">root@bs-dev:~/repo/hexo-site# git submodule status </div><div class=\"line\">-dabe550a44b98194546515ec78f8ec8c11b41340 themes/nova</div></pre></td></tr></table></figure></p>\n<p>克隆Submodule版本库时(执行git submodule update命令)，自动将Submodule版本库checkout到给定的commit，进入Submodule路径可以查看到版本库的状态。在主版本中同步Submodule修改的步骤如下:</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">cd hexo-site/themes/nova</div><div class=\"line\">git checkout master</div><div class=\"line\"></div><div class=\"line\">cd hexo-site</div><div class=\"line\">git status</div></pre></td></tr></table></figure>\n<p>进入Submodule版本库/hexo-site/themes/nova，checkout出master分支<br>进入主版本库/hexo-site，发现theme/nova已修改，提交修改。</p>\n<h2 id=\"gitmodule文件\"><a href=\"#gitmodule文件\" class=\"headerlink\" title=\".gitmodule文件\"></a>.gitmodule文件</h2><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">[submodule &quot;themes/nova&quot;]</div><div class=\"line\">\tpath = themes/nova</div><div class=\"line\">\turl = https://github.com/wuxiangwei/hexo-theme-nova.git</div></pre></td></tr></table></figure>\n<p>添加Submodule后，将在hexo-site目录中生成.gitmodule文件用以记录Submodule的引用信息，包括在主版本库中的路径以及仓库地址。</p>\n<hr>\n<p><strong>版权声明</strong></p>\n<p>本作品采用<a href=\"https://creativecommons.org/licenses/by/2.5/cn/\" target=\"_blank\" rel=\"external\">知识共享署名2.5 中国大陆许可协议</a>进行许可，欢迎转载，但转载请注明来自<a href=\"https://wuxiangwei.cn/\"><em>Shanno’s Blog</em></a>，并保持转载后文章内容的完整。本人保留所有版权相关权利，侵权必究。</p>\n","excerpt":"<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">hexo-site    --- hexo-site版本库</div><div class=\"line\">└── themes</div><div class=\"line\">    └─nova --- nova版本库</div></pre></td></tr></table></figure>\n<p>需求：一个版本库引用其它版本库的文件。如上所示，hexo-site版本库引用nova版本库中的文件。</p>","more":"<h2 id=\"基本操作\"><a href=\"#基本操作\" class=\"headerlink\" title=\"基本操作\"></a>基本操作</h2><h3 id=\"添加Submodule\"><a href=\"#添加Submodule\" class=\"headerlink\" title=\"添加Submodule\"></a>添加Submodule</h3><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">cd hexo-site/</div><div class=\"line\">git submodule add -f https://github.com/wuxiangwei/hexo-theme-nova.git themes/nova</div></pre></td></tr></table></figure>\n<p>执行添加子模块命令后，hexo-site目录中将生成1个新文件<strong>.gitmodule</strong>，文件记录submodule的引用信息，包括在当前项目的位置以及仓库所在。</p>\n<h3 id=\"查看Submodule\"><a href=\"#查看Submodule\" class=\"headerlink\" title=\"查看Submodule\"></a>查看Submodule</h3><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">git submodule status </div><div class=\"line\">dabe550a44b98194546515ec78f8ec8c11b41340 themes/nova (v0.1.2-13-gdabe550)</div></pre></td></tr></table></figure>\n<p>可以看到每个Submodule的commit id，这commit id是固定的，每次克隆主版本库时不变，外部版本库更新时不变。</p>\n<h3 id=\"克隆带Submodule的版本库\"><a href=\"#克隆带Submodule的版本库\" class=\"headerlink\" title=\"克隆带Submodule的版本库\"></a>克隆带Submodule的版本库</h3><p>克隆带Submodule的版本库，并不能自动克隆Submodule的版本库。这个特点的好处是，克隆版本库速度快，数据没有冗余。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">git submodule init</div><div class=\"line\">git submodule update</div></pre></td></tr></table></figure>\n<p>使用上述两个命令能够克隆Submodule的版本库。</p>\n<h3 id=\"同步Submodule版本库的修改\"><a href=\"#同步Submodule版本库的修改\" class=\"headerlink\" title=\"同步Submodule版本库的修改\"></a>同步Submodule版本库的修改</h3><p>使用场景：Submodule版本库的主线(hexo-theme-nova)已修改，但这修改没有同步到主版本库(hexo-site)。</p>\n<p>Submodule版本库的主线的commit：<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">root@bs-dev:~/repo/hexo-theme-nova# git log --pretty=oneline</div><div class=\"line\">6dccc29d93b0928a402290a3d9a0d2b1fd57e031 关闭打赏页面</div><div class=\"line\">dabe550a44b98194546515ec78f8ec8c11b41340 Merge pull request #15 from Jamling/dev</div></pre></td></tr></table></figure></p>\n<p>主版本库的Submodule版本的commit：<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">root@bs-dev:~/repo/hexo-site# git submodule status </div><div class=\"line\">-dabe550a44b98194546515ec78f8ec8c11b41340 themes/nova</div></pre></td></tr></table></figure></p>\n<p>克隆Submodule版本库时(执行git submodule update命令)，自动将Submodule版本库checkout到给定的commit，进入Submodule路径可以查看到版本库的状态。在主版本中同步Submodule修改的步骤如下:</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">cd hexo-site/themes/nova</div><div class=\"line\">git checkout master</div><div class=\"line\"></div><div class=\"line\">cd hexo-site</div><div class=\"line\">git status</div></pre></td></tr></table></figure>\n<p>进入Submodule版本库/hexo-site/themes/nova，checkout出master分支<br>进入主版本库/hexo-site，发现theme/nova已修改，提交修改。</p>\n<h2 id=\"gitmodule文件\"><a href=\"#gitmodule文件\" class=\"headerlink\" title=\".gitmodule文件\"></a>.gitmodule文件</h2><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">[submodule &quot;themes/nova&quot;]</div><div class=\"line\">\tpath = themes/nova</div><div class=\"line\">\turl = https://github.com/wuxiangwei/hexo-theme-nova.git</div></pre></td></tr></table></figure>\n<p>添加Submodule后，将在hexo-site目录中生成.gitmodule文件用以记录Submodule的引用信息，包括在主版本库中的路径以及仓库地址。</p>\n<hr>\n<p><strong>版权声明</strong></p>\n<p>本作品采用<a href=\"https://creativecommons.org/licenses/by/2.5/cn/\">知识共享署名2.5 中国大陆许可协议</a>进行许可，欢迎转载，但转载请注明来自<a href=\"https://wuxiangwei.cn/\"><em>Shanno’s Blog</em></a>，并保持转载后文章内容的完整。本人保留所有版权相关权利，侵权必究。</p>"},{"title":"Hexo笔记","date":"2015-02-27T13:59:09.000Z","_content":"\n关于使用Hexo搭建博客的教程已经很多并且足够详细，本文侧重搭建过程中碰到问题的解决。我对博客的要求不高，只要满足下面几点就可以了:\n\n* 不含广告\n* 界面足够简洁\n* 具备标签、评论、分享功能，支持markdown编辑\n* 能使用Git对文章进行版本管理，并且可移植性好\n\n\n<!--more-->\n\n\n## 修改本地服务的IP地址\n\n输入hexo s命令启动本地服务。然后，浏览器输入localhost:4000，就能够看到预览效果。服务器的ip地址和端口号在\\_config.yaml文件内配置。\n\n```\nport: 4000\nserver_ip: 192.168.7.50\n```\n\n如果Linux系统没装图形界面，设置server\\_ip为主机IP地址，这样就能够通过本地局域网的其它主机来访问了。\n\n## Git Bash显示中文\n\n打开git bash，控制台输入下面的命令，为ls命令取别名。\n\n```\n$ alias ls='ls --show-control-chars --color=auto'\n```\n\n## 添加多说评论\n\n\nHexo添加多说评论时需要提供[多说](http://www.duoshuo.com)账号的shortname，找了半天没找到这东西。多说没有提供注册账号的入口而是让用户直接以微博、百度等账号登入，登入后在“后台管理”中才有。\n\n## Markdown注意事项\n\n1. 引用内部不要有空行，否则显示有问题。\n2. 超链接时，网址一定要以Http打头，不能以www开始。\n\n## 部署到gitcafe\n\n### 生成ssh公钥\n```\nssh-keygen -t rsa -C shanno@yeah.net -f ~/.ssh/gitcafe\n```\n执行上面的命令在~/.ssh目录下生成gitcafe、gitcafe.pub两个文件，它们分别代表私钥和公钥文件。\n\n### 创建gitcafe-pages分支\n\n```\nroot@taurus:~/test/tiger/.deploy# git remote add origin 'git@gitcafe.com:shannon/shannon.git\nroot@taurus:~/test/tiger/.deploy# git remote add origin 'git@gitcafe.com:shannon/shannon.git'\nroot@taurus:~/test/tiger/.deploy# git push -u origin gitcafe-pages \n```\n\n## 参考资料\n\n1. [hexo你的博客](http://ibruce.info/2013/11/22/hexo-your-blog/)\n2. [google字库导致hexo modernist首页加载变慢](http://ibruce.info/2013/12/03/fonts-googleapis-lead-to-slow/)\n3. [windows下git bash显示中文](http://blog.csdn.net/self001/article/details/7337182)\n4. [如何同时使用多个公钥](https://gitcafe.com/GitCafe/Help/wiki/%E5%A6%82%E4%BD%95%E5%90%8C%E6%97%B6%E4%BD%BF%E7%94%A8%E5%A4%9A%E4%B8%AA%E5%85%AC%E7%A7%98%E9%92%A5)\n5. [如何将托管在github上的hexo博客转到gitcafe](http://blog.maxwi.com/2014/03/19/hexo-github-to-gitcafe/)\n\n","source":"_posts/Other/hexo_note.md","raw":"title: Hexo笔记\ndate: 2015-02-27 21:59:09\ncategories: [Other]\ntags: [Hexo]\n---\n\n关于使用Hexo搭建博客的教程已经很多并且足够详细，本文侧重搭建过程中碰到问题的解决。我对博客的要求不高，只要满足下面几点就可以了:\n\n* 不含广告\n* 界面足够简洁\n* 具备标签、评论、分享功能，支持markdown编辑\n* 能使用Git对文章进行版本管理，并且可移植性好\n\n\n<!--more-->\n\n\n## 修改本地服务的IP地址\n\n输入hexo s命令启动本地服务。然后，浏览器输入localhost:4000，就能够看到预览效果。服务器的ip地址和端口号在\\_config.yaml文件内配置。\n\n```\nport: 4000\nserver_ip: 192.168.7.50\n```\n\n如果Linux系统没装图形界面，设置server\\_ip为主机IP地址，这样就能够通过本地局域网的其它主机来访问了。\n\n## Git Bash显示中文\n\n打开git bash，控制台输入下面的命令，为ls命令取别名。\n\n```\n$ alias ls='ls --show-control-chars --color=auto'\n```\n\n## 添加多说评论\n\n\nHexo添加多说评论时需要提供[多说](http://www.duoshuo.com)账号的shortname，找了半天没找到这东西。多说没有提供注册账号的入口而是让用户直接以微博、百度等账号登入，登入后在“后台管理”中才有。\n\n## Markdown注意事项\n\n1. 引用内部不要有空行，否则显示有问题。\n2. 超链接时，网址一定要以Http打头，不能以www开始。\n\n## 部署到gitcafe\n\n### 生成ssh公钥\n```\nssh-keygen -t rsa -C shanno@yeah.net -f ~/.ssh/gitcafe\n```\n执行上面的命令在~/.ssh目录下生成gitcafe、gitcafe.pub两个文件，它们分别代表私钥和公钥文件。\n\n### 创建gitcafe-pages分支\n\n```\nroot@taurus:~/test/tiger/.deploy# git remote add origin 'git@gitcafe.com:shannon/shannon.git\nroot@taurus:~/test/tiger/.deploy# git remote add origin 'git@gitcafe.com:shannon/shannon.git'\nroot@taurus:~/test/tiger/.deploy# git push -u origin gitcafe-pages \n```\n\n## 参考资料\n\n1. [hexo你的博客](http://ibruce.info/2013/11/22/hexo-your-blog/)\n2. [google字库导致hexo modernist首页加载变慢](http://ibruce.info/2013/12/03/fonts-googleapis-lead-to-slow/)\n3. [windows下git bash显示中文](http://blog.csdn.net/self001/article/details/7337182)\n4. [如何同时使用多个公钥](https://gitcafe.com/GitCafe/Help/wiki/%E5%A6%82%E4%BD%95%E5%90%8C%E6%97%B6%E4%BD%BF%E7%94%A8%E5%A4%9A%E4%B8%AA%E5%85%AC%E7%A7%98%E9%92%A5)\n5. [如何将托管在github上的hexo博客转到gitcafe](http://blog.maxwi.com/2014/03/19/hexo-github-to-gitcafe/)\n\n","slug":"Other/hexo_note","published":1,"updated":"2016-12-15T07:18:40.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cix1kgav6002fdaei8hcun347","content":"<p>关于使用Hexo搭建博客的教程已经很多并且足够详细，本文侧重搭建过程中碰到问题的解决。我对博客的要求不高，只要满足下面几点就可以了:</p>\n<ul>\n<li>不含广告</li>\n<li>界面足够简洁</li>\n<li>具备标签、评论、分享功能，支持markdown编辑</li>\n<li>能使用Git对文章进行版本管理，并且可移植性好</li>\n</ul>\n<a id=\"more\"></a>\n<h2 id=\"修改本地服务的IP地址\"><a href=\"#修改本地服务的IP地址\" class=\"headerlink\" title=\"修改本地服务的IP地址\"></a>修改本地服务的IP地址</h2><p>输入hexo s命令启动本地服务。然后，浏览器输入localhost:4000，就能够看到预览效果。服务器的ip地址和端口号在_config.yaml文件内配置。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">port: 4000</div><div class=\"line\">server_ip: 192.168.7.50</div></pre></td></tr></table></figure>\n<p>如果Linux系统没装图形界面，设置server_ip为主机IP地址，这样就能够通过本地局域网的其它主机来访问了。</p>\n<h2 id=\"Git-Bash显示中文\"><a href=\"#Git-Bash显示中文\" class=\"headerlink\" title=\"Git Bash显示中文\"></a>Git Bash显示中文</h2><p>打开git bash，控制台输入下面的命令，为ls命令取别名。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">$ alias ls=&apos;ls --show-control-chars --color=auto&apos;</div></pre></td></tr></table></figure>\n<h2 id=\"添加多说评论\"><a href=\"#添加多说评论\" class=\"headerlink\" title=\"添加多说评论\"></a>添加多说评论</h2><p>Hexo添加多说评论时需要提供<a href=\"http://www.duoshuo.com\" target=\"_blank\" rel=\"external\">多说</a>账号的shortname，找了半天没找到这东西。多说没有提供注册账号的入口而是让用户直接以微博、百度等账号登入，登入后在“后台管理”中才有。</p>\n<h2 id=\"Markdown注意事项\"><a href=\"#Markdown注意事项\" class=\"headerlink\" title=\"Markdown注意事项\"></a>Markdown注意事项</h2><ol>\n<li>引用内部不要有空行，否则显示有问题。</li>\n<li>超链接时，网址一定要以Http打头，不能以www开始。</li>\n</ol>\n<h2 id=\"部署到gitcafe\"><a href=\"#部署到gitcafe\" class=\"headerlink\" title=\"部署到gitcafe\"></a>部署到gitcafe</h2><h3 id=\"生成ssh公钥\"><a href=\"#生成ssh公钥\" class=\"headerlink\" title=\"生成ssh公钥\"></a>生成ssh公钥</h3><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">ssh-keygen -t rsa -C shanno@yeah.net -f ~/.ssh/gitcafe</div></pre></td></tr></table></figure>\n<p>执行上面的命令在~/.ssh目录下生成gitcafe、gitcafe.pub两个文件，它们分别代表私钥和公钥文件。</p>\n<h3 id=\"创建gitcafe-pages分支\"><a href=\"#创建gitcafe-pages分支\" class=\"headerlink\" title=\"创建gitcafe-pages分支\"></a>创建gitcafe-pages分支</h3><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">root@taurus:~/test/tiger/.deploy# git remote add origin &apos;git@gitcafe.com:shannon/shannon.git</div><div class=\"line\">root@taurus:~/test/tiger/.deploy# git remote add origin &apos;git@gitcafe.com:shannon/shannon.git&apos;</div><div class=\"line\">root@taurus:~/test/tiger/.deploy# git push -u origin gitcafe-pages</div></pre></td></tr></table></figure>\n<h2 id=\"参考资料\"><a href=\"#参考资料\" class=\"headerlink\" title=\"参考资料\"></a>参考资料</h2><ol>\n<li><a href=\"http://ibruce.info/2013/11/22/hexo-your-blog/\" target=\"_blank\" rel=\"external\">hexo你的博客</a></li>\n<li><a href=\"http://ibruce.info/2013/12/03/fonts-googleapis-lead-to-slow/\" target=\"_blank\" rel=\"external\">google字库导致hexo modernist首页加载变慢</a></li>\n<li><a href=\"http://blog.csdn.net/self001/article/details/7337182\" target=\"_blank\" rel=\"external\">windows下git bash显示中文</a></li>\n<li><a href=\"https://gitcafe.com/GitCafe/Help/wiki/%E5%A6%82%E4%BD%95%E5%90%8C%E6%97%B6%E4%BD%BF%E7%94%A8%E5%A4%9A%E4%B8%AA%E5%85%AC%E7%A7%98%E9%92%A5\" target=\"_blank\" rel=\"external\">如何同时使用多个公钥</a></li>\n<li><a href=\"http://blog.maxwi.com/2014/03/19/hexo-github-to-gitcafe/\" target=\"_blank\" rel=\"external\">如何将托管在github上的hexo博客转到gitcafe</a></li>\n</ol>\n<hr>\n<p><strong>版权声明</strong></p>\n<p>本作品采用<a href=\"https://creativecommons.org/licenses/by/2.5/cn/\" target=\"_blank\" rel=\"external\">知识共享署名2.5 中国大陆许可协议</a>进行许可，欢迎转载，但转载请注明来自<a href=\"https://wuxiangwei.cn/\"><em>Shanno’s Blog</em></a>，并保持转载后文章内容的完整。本人保留所有版权相关权利，侵权必究。</p>\n","excerpt":"<p>关于使用Hexo搭建博客的教程已经很多并且足够详细，本文侧重搭建过程中碰到问题的解决。我对博客的要求不高，只要满足下面几点就可以了:</p>\n<ul>\n<li>不含广告</li>\n<li>界面足够简洁</li>\n<li>具备标签、评论、分享功能，支持markdown编辑</li>\n<li>能使用Git对文章进行版本管理，并且可移植性好</li>\n</ul>","more":"<h2 id=\"修改本地服务的IP地址\"><a href=\"#修改本地服务的IP地址\" class=\"headerlink\" title=\"修改本地服务的IP地址\"></a>修改本地服务的IP地址</h2><p>输入hexo s命令启动本地服务。然后，浏览器输入localhost:4000，就能够看到预览效果。服务器的ip地址和端口号在_config.yaml文件内配置。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">port: 4000</div><div class=\"line\">server_ip: 192.168.7.50</div></pre></td></tr></table></figure>\n<p>如果Linux系统没装图形界面，设置server_ip为主机IP地址，这样就能够通过本地局域网的其它主机来访问了。</p>\n<h2 id=\"Git-Bash显示中文\"><a href=\"#Git-Bash显示中文\" class=\"headerlink\" title=\"Git Bash显示中文\"></a>Git Bash显示中文</h2><p>打开git bash，控制台输入下面的命令，为ls命令取别名。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">$ alias ls=&apos;ls --show-control-chars --color=auto&apos;</div></pre></td></tr></table></figure>\n<h2 id=\"添加多说评论\"><a href=\"#添加多说评论\" class=\"headerlink\" title=\"添加多说评论\"></a>添加多说评论</h2><p>Hexo添加多说评论时需要提供<a href=\"http://www.duoshuo.com\">多说</a>账号的shortname，找了半天没找到这东西。多说没有提供注册账号的入口而是让用户直接以微博、百度等账号登入，登入后在“后台管理”中才有。</p>\n<h2 id=\"Markdown注意事项\"><a href=\"#Markdown注意事项\" class=\"headerlink\" title=\"Markdown注意事项\"></a>Markdown注意事项</h2><ol>\n<li>引用内部不要有空行，否则显示有问题。</li>\n<li>超链接时，网址一定要以Http打头，不能以www开始。</li>\n</ol>\n<h2 id=\"部署到gitcafe\"><a href=\"#部署到gitcafe\" class=\"headerlink\" title=\"部署到gitcafe\"></a>部署到gitcafe</h2><h3 id=\"生成ssh公钥\"><a href=\"#生成ssh公钥\" class=\"headerlink\" title=\"生成ssh公钥\"></a>生成ssh公钥</h3><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">ssh-keygen -t rsa -C shanno@yeah.net -f ~/.ssh/gitcafe</div></pre></td></tr></table></figure>\n<p>执行上面的命令在~/.ssh目录下生成gitcafe、gitcafe.pub两个文件，它们分别代表私钥和公钥文件。</p>\n<h3 id=\"创建gitcafe-pages分支\"><a href=\"#创建gitcafe-pages分支\" class=\"headerlink\" title=\"创建gitcafe-pages分支\"></a>创建gitcafe-pages分支</h3><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">root@taurus:~/test/tiger/.deploy# git remote add origin &apos;git@gitcafe.com:shannon/shannon.git</div><div class=\"line\">root@taurus:~/test/tiger/.deploy# git remote add origin &apos;git@gitcafe.com:shannon/shannon.git&apos;</div><div class=\"line\">root@taurus:~/test/tiger/.deploy# git push -u origin gitcafe-pages</div></pre></td></tr></table></figure>\n<h2 id=\"参考资料\"><a href=\"#参考资料\" class=\"headerlink\" title=\"参考资料\"></a>参考资料</h2><ol>\n<li><a href=\"http://ibruce.info/2013/11/22/hexo-your-blog/\">hexo你的博客</a></li>\n<li><a href=\"http://ibruce.info/2013/12/03/fonts-googleapis-lead-to-slow/\">google字库导致hexo modernist首页加载变慢</a></li>\n<li><a href=\"http://blog.csdn.net/self001/article/details/7337182\">windows下git bash显示中文</a></li>\n<li><a href=\"https://gitcafe.com/GitCafe/Help/wiki/%E5%A6%82%E4%BD%95%E5%90%8C%E6%97%B6%E4%BD%BF%E7%94%A8%E5%A4%9A%E4%B8%AA%E5%85%AC%E7%A7%98%E9%92%A5\">如何同时使用多个公钥</a></li>\n<li><a href=\"http://blog.maxwi.com/2014/03/19/hexo-github-to-gitcafe/\">如何将托管在github上的hexo博客转到gitcafe</a></li>\n</ol>\n<hr>\n<p><strong>版权声明</strong></p>\n<p>本作品采用<a href=\"https://creativecommons.org/licenses/by/2.5/cn/\">知识共享署名2.5 中国大陆许可协议</a>进行许可，欢迎转载，但转载请注明来自<a href=\"https://wuxiangwei.cn/\"><em>Shanno’s Blog</em></a>，并保持转载后文章内容的完整。本人保留所有版权相关权利，侵权必究。</p>"},{"title":"Python笔记","date":"2014-07-31T01:13:43.000Z","toc":"ture","_content":"\n记录使用python过程中碰到的小问题、小窍门。\n\n<!--more-->\n\n### 剔除字符串末尾的空白字符\n\n```python\nwith open(pkglist, 'r') as fd:\n   for line in fd.readlines():\n    if not line.isspace():\n       cmdstr = \"dpkg -i '%s'\" % os.path.join(pkgpath, line.rstrip())\n       os.system(cmdstr)\n```\n将命令写在文件中，然后逐行读取文件中的命令执行。由于从文件中读取出来的行都带有换行符号，所以执行命令时会出错，这时候可以使用字符串**str**类的*rstrip()*方法将行末尾的空白字符剔除。\n\n\n### 中文乱码\n\n```python\nimport sys  \nreload(sys)  \nsys.setdefaultencoding('utf-8')\n```\n在出现乱码的地方添加上面的代码。\n\n\n### with用法和异常处理\n\n```python\n# !/usr/bin/env python  \n# -*-coding:utf8-*-  \n  \n# 为文件中每行的开头添加行号  \ndef addLineNumberForFile(srcFilePath, dstFilePath):  \n    if srcFilePath == dstFilePath:  \n        raise NameError('Invalid arguments, ' + 'p1 = ' + srcFilePath + ', p2 = ' + dstFilePath)  \n  \n    with open(srcFilePath) as srcfd:  \n        lineNum = 0  \n        with open(dstFilePath, 'w') as dstfd:  \n            for line in srcfd.readlines():  \n                lineNum += 1  \n                dstfd.write(str(lineNum) + \"\\t\" + line)  \n  \n# 单元测试  \nif __name__ == '__main__':    \n    srcFilePath = 'data.txt'  \n    dstFilePath = 'data_latest.txt'  \n    try:  \n        addLineNumberForFile(srcFilePath, dstFilePath)  \n        print('Success to add line number for the file named ' + srcFilePath)  \n        print('The result file is ' + dstFilePath)  \n    except NameError as nError:  \n        print nError  \n```\n","source":"_posts/Python/python-notes.md","raw":"title: Python笔记\ndate: 2014-07-31 09:13:43\ncategories: [Python]\ntags: [Python]\ntoc: ture\n---\n\n记录使用python过程中碰到的小问题、小窍门。\n\n<!--more-->\n\n### 剔除字符串末尾的空白字符\n\n```python\nwith open(pkglist, 'r') as fd:\n   for line in fd.readlines():\n    if not line.isspace():\n       cmdstr = \"dpkg -i '%s'\" % os.path.join(pkgpath, line.rstrip())\n       os.system(cmdstr)\n```\n将命令写在文件中，然后逐行读取文件中的命令执行。由于从文件中读取出来的行都带有换行符号，所以执行命令时会出错，这时候可以使用字符串**str**类的*rstrip()*方法将行末尾的空白字符剔除。\n\n\n### 中文乱码\n\n```python\nimport sys  \nreload(sys)  \nsys.setdefaultencoding('utf-8')\n```\n在出现乱码的地方添加上面的代码。\n\n\n### with用法和异常处理\n\n```python\n# !/usr/bin/env python  \n# -*-coding:utf8-*-  \n  \n# 为文件中每行的开头添加行号  \ndef addLineNumberForFile(srcFilePath, dstFilePath):  \n    if srcFilePath == dstFilePath:  \n        raise NameError('Invalid arguments, ' + 'p1 = ' + srcFilePath + ', p2 = ' + dstFilePath)  \n  \n    with open(srcFilePath) as srcfd:  \n        lineNum = 0  \n        with open(dstFilePath, 'w') as dstfd:  \n            for line in srcfd.readlines():  \n                lineNum += 1  \n                dstfd.write(str(lineNum) + \"\\t\" + line)  \n  \n# 单元测试  \nif __name__ == '__main__':    \n    srcFilePath = 'data.txt'  \n    dstFilePath = 'data_latest.txt'  \n    try:  \n        addLineNumberForFile(srcFilePath, dstFilePath)  \n        print('Success to add line number for the file named ' + srcFilePath)  \n        print('The result file is ' + dstFilePath)  \n    except NameError as nError:  \n        print nError  \n```\n","slug":"Python/python-notes","published":1,"updated":"2016-12-15T07:18:40.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cix1kgav8002jdaeic9phubkk","content":"<p>记录使用python过程中碰到的小问题、小窍门。</p>\n<a id=\"more\"></a>\n<h3 id=\"剔除字符串末尾的空白字符\"><a href=\"#剔除字符串末尾的空白字符\" class=\"headerlink\" title=\"剔除字符串末尾的空白字符\"></a>剔除字符串末尾的空白字符</h3><figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">with</span> open(pkglist, <span class=\"string\">'r'</span>) <span class=\"keyword\">as</span> fd:</div><div class=\"line\">   <span class=\"keyword\">for</span> line <span class=\"keyword\">in</span> fd.readlines():</div><div class=\"line\">    <span class=\"keyword\">if</span> <span class=\"keyword\">not</span> line.isspace():</div><div class=\"line\">       cmdstr = <span class=\"string\">\"dpkg -i '%s'\"</span> % os.path.join(pkgpath, line.rstrip())</div><div class=\"line\">       os.system(cmdstr)</div></pre></td></tr></table></figure>\n<p>将命令写在文件中，然后逐行读取文件中的命令执行。由于从文件中读取出来的行都带有换行符号，所以执行命令时会出错，这时候可以使用字符串<strong>str</strong>类的<em>rstrip()</em>方法将行末尾的空白字符剔除。</p>\n<h3 id=\"中文乱码\"><a href=\"#中文乱码\" class=\"headerlink\" title=\"中文乱码\"></a>中文乱码</h3><figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">import</span> sys  </div><div class=\"line\">reload(sys)  </div><div class=\"line\">sys.setdefaultencoding(<span class=\"string\">'utf-8'</span>)</div></pre></td></tr></table></figure>\n<p>在出现乱码的地方添加上面的代码。</p>\n<h3 id=\"with用法和异常处理\"><a href=\"#with用法和异常处理\" class=\"headerlink\" title=\"with用法和异常处理\"></a>with用法和异常处理</h3><figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\"># !/usr/bin/env python  </span></div><div class=\"line\"><span class=\"comment\"># -*-coding:utf8-*-  </span></div><div class=\"line\">  </div><div class=\"line\"><span class=\"comment\"># 为文件中每行的开头添加行号  </span></div><div class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">addLineNumberForFile</span><span class=\"params\">(srcFilePath, dstFilePath)</span>:</span>  </div><div class=\"line\">    <span class=\"keyword\">if</span> srcFilePath == dstFilePath:  </div><div class=\"line\">        <span class=\"keyword\">raise</span> NameError(<span class=\"string\">'Invalid arguments, '</span> + <span class=\"string\">'p1 = '</span> + srcFilePath + <span class=\"string\">', p2 = '</span> + dstFilePath)  </div><div class=\"line\">  </div><div class=\"line\">    <span class=\"keyword\">with</span> open(srcFilePath) <span class=\"keyword\">as</span> srcfd:  </div><div class=\"line\">        lineNum = <span class=\"number\">0</span>  </div><div class=\"line\">        <span class=\"keyword\">with</span> open(dstFilePath, <span class=\"string\">'w'</span>) <span class=\"keyword\">as</span> dstfd:  </div><div class=\"line\">            <span class=\"keyword\">for</span> line <span class=\"keyword\">in</span> srcfd.readlines():  </div><div class=\"line\">                lineNum += <span class=\"number\">1</span>  </div><div class=\"line\">                dstfd.write(str(lineNum) + <span class=\"string\">\"\\t\"</span> + line)  </div><div class=\"line\">  </div><div class=\"line\"><span class=\"comment\"># 单元测试  </span></div><div class=\"line\"><span class=\"keyword\">if</span> __name__ == <span class=\"string\">'__main__'</span>:    </div><div class=\"line\">    srcFilePath = <span class=\"string\">'data.txt'</span>  </div><div class=\"line\">    dstFilePath = <span class=\"string\">'data_latest.txt'</span>  </div><div class=\"line\">    <span class=\"keyword\">try</span>:  </div><div class=\"line\">        addLineNumberForFile(srcFilePath, dstFilePath)  </div><div class=\"line\">        print(<span class=\"string\">'Success to add line number for the file named '</span> + srcFilePath)  </div><div class=\"line\">        print(<span class=\"string\">'The result file is '</span> + dstFilePath)  </div><div class=\"line\">    <span class=\"keyword\">except</span> NameError <span class=\"keyword\">as</span> nError:  </div><div class=\"line\">        <span class=\"keyword\">print</span> nError</div></pre></td></tr></table></figure>\n<hr>\n<p><strong>版权声明</strong></p>\n<p>本作品采用<a href=\"https://creativecommons.org/licenses/by/2.5/cn/\" target=\"_blank\" rel=\"external\">知识共享署名2.5 中国大陆许可协议</a>进行许可，欢迎转载，但转载请注明来自<a href=\"https://wuxiangwei.cn/\"><em>Shanno’s Blog</em></a>，并保持转载后文章内容的完整。本人保留所有版权相关权利，侵权必究。</p>\n","excerpt":"<p>记录使用python过程中碰到的小问题、小窍门。</p>","more":"<h3 id=\"剔除字符串末尾的空白字符\"><a href=\"#剔除字符串末尾的空白字符\" class=\"headerlink\" title=\"剔除字符串末尾的空白字符\"></a>剔除字符串末尾的空白字符</h3><figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">with</span> open(pkglist, <span class=\"string\">'r'</span>) <span class=\"keyword\">as</span> fd:</div><div class=\"line\">   <span class=\"keyword\">for</span> line <span class=\"keyword\">in</span> fd.readlines():</div><div class=\"line\">    <span class=\"keyword\">if</span> <span class=\"keyword\">not</span> line.isspace():</div><div class=\"line\">       cmdstr = <span class=\"string\">\"dpkg -i '%s'\"</span> % os.path.join(pkgpath, line.rstrip())</div><div class=\"line\">       os.system(cmdstr)</div></pre></td></tr></table></figure>\n<p>将命令写在文件中，然后逐行读取文件中的命令执行。由于从文件中读取出来的行都带有换行符号，所以执行命令时会出错，这时候可以使用字符串<strong>str</strong>类的<em>rstrip()</em>方法将行末尾的空白字符剔除。</p>\n<h3 id=\"中文乱码\"><a href=\"#中文乱码\" class=\"headerlink\" title=\"中文乱码\"></a>中文乱码</h3><figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">import</span> sys  </div><div class=\"line\">reload(sys)  </div><div class=\"line\">sys.setdefaultencoding(<span class=\"string\">'utf-8'</span>)</div></pre></td></tr></table></figure>\n<p>在出现乱码的地方添加上面的代码。</p>\n<h3 id=\"with用法和异常处理\"><a href=\"#with用法和异常处理\" class=\"headerlink\" title=\"with用法和异常处理\"></a>with用法和异常处理</h3><figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\"># !/usr/bin/env python  </span></div><div class=\"line\"><span class=\"comment\"># -*-coding:utf8-*-  </span></div><div class=\"line\">  </div><div class=\"line\"><span class=\"comment\"># 为文件中每行的开头添加行号  </span></div><div class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">addLineNumberForFile</span><span class=\"params\">(srcFilePath, dstFilePath)</span>:</span>  </div><div class=\"line\">    <span class=\"keyword\">if</span> srcFilePath == dstFilePath:  </div><div class=\"line\">        <span class=\"keyword\">raise</span> NameError(<span class=\"string\">'Invalid arguments, '</span> + <span class=\"string\">'p1 = '</span> + srcFilePath + <span class=\"string\">', p2 = '</span> + dstFilePath)  </div><div class=\"line\">  </div><div class=\"line\">    <span class=\"keyword\">with</span> open(srcFilePath) <span class=\"keyword\">as</span> srcfd:  </div><div class=\"line\">        lineNum = <span class=\"number\">0</span>  </div><div class=\"line\">        <span class=\"keyword\">with</span> open(dstFilePath, <span class=\"string\">'w'</span>) <span class=\"keyword\">as</span> dstfd:  </div><div class=\"line\">            <span class=\"keyword\">for</span> line <span class=\"keyword\">in</span> srcfd.readlines():  </div><div class=\"line\">                lineNum += <span class=\"number\">1</span>  </div><div class=\"line\">                dstfd.write(str(lineNum) + <span class=\"string\">\"\\t\"</span> + line)  </div><div class=\"line\">  </div><div class=\"line\"><span class=\"comment\"># 单元测试  </span></div><div class=\"line\"><span class=\"keyword\">if</span> __name__ == <span class=\"string\">'__main__'</span>:    </div><div class=\"line\">    srcFilePath = <span class=\"string\">'data.txt'</span>  </div><div class=\"line\">    dstFilePath = <span class=\"string\">'data_latest.txt'</span>  </div><div class=\"line\">    <span class=\"keyword\">try</span>:  </div><div class=\"line\">        addLineNumberForFile(srcFilePath, dstFilePath)  </div><div class=\"line\">        print(<span class=\"string\">'Success to add line number for the file named '</span> + srcFilePath)  </div><div class=\"line\">        print(<span class=\"string\">'The result file is '</span> + dstFilePath)  </div><div class=\"line\">    <span class=\"keyword\">except</span> NameError <span class=\"keyword\">as</span> nError:  </div><div class=\"line\">        <span class=\"keyword\">print</span> nError</div></pre></td></tr></table></figure>\n<hr>\n<p><strong>版权声明</strong></p>\n<p>本作品采用<a href=\"https://creativecommons.org/licenses/by/2.5/cn/\">知识共享署名2.5 中国大陆许可协议</a>进行许可，欢迎转载，但转载请注明来自<a href=\"https://wuxiangwei.cn/\"><em>Shanno’s Blog</em></a>，并保持转载后文章内容的完整。本人保留所有版权相关权利，侵权必究。</p>"},{"title":"简单的VIM配置","date":"2015-03-05T00:57:42.000Z","_content":"\n我的vim配置，简单不花哨，尽量使用默认快捷键\n\n<!--more-->\n\n\n```\nset encoding=utf-8\nset termencoding=utf-8\n\nif has(\"win32\")    \n    set fileencoding=chinese\nelse\n    set fileencoding=utf-8\nendif\n\nif has(\"win32\")\n    set nocompatible\n    source $VIMRUNTIME/vimrc_example.vim\n    source $VIMRUNTIME/mswin.vim\n    behave mswin\nendif\n\nsyntax on\n\"set cul\n\"set cuc\nset ruler\nset showcmd\nset nocompatible\n\nset cursorline \" 高亮当前行\n\"set paste \" 设置粘贴模式，使粘贴不错位\nset novisualbell \" 设置不闪烁\n\nset autoindent \" 继承前行的缩进方式\nset cindent\nset tabstop=4 \" 制表符为4\nset softtabstop=4 \" 按backspace按键时可以删掉4个空格\nset shiftwidth=4 \" 设置<<或>>移动的宽度为4\nset expandtab\nset smarttab \" 为C程序提供自动缩进\nset number\nset history=1000\n\nset textwidth=120 \" 设置每行120个字符自动换行\n\nset autochdir \" 自动切换当前目录为当前文件所在目录\n\nset hlsearch\nset incsearch\n\nset cmdheight=2\n\nfiletype on\nfiletype plugin on\nfiletype indent on\n\nset autoread \" 设置当文件被改动时自动载入\nset autowrite \" 自动保存\n\nset nobackup\nset noswapfile\n\nset showmatch \" 高亮显示匹配的括号\nset completeopt=preview,menu \" 代码补全\n\n\" 自动括号\n\"\ninoremap ( ()<ESC>i\ninoremap { {}<ESC>i\n\ncolorscheme peachpuff\n\n```\n\n\n### 快捷键\n\n\ngg\nG\n/\n?\nvimgrep\ncopen\nc-n\nc-p\nfx\n\ndd\ndw\ndl\n\ni\nI\na\nA\n","source":"_posts/Vim/config.md","raw":"title: 简单的VIM配置\ndate: 2015-03-05 08:57:42\ncategories: [Vim]\ntags: [Vim]\n---\n\n我的vim配置，简单不花哨，尽量使用默认快捷键\n\n<!--more-->\n\n\n```\nset encoding=utf-8\nset termencoding=utf-8\n\nif has(\"win32\")    \n    set fileencoding=chinese\nelse\n    set fileencoding=utf-8\nendif\n\nif has(\"win32\")\n    set nocompatible\n    source $VIMRUNTIME/vimrc_example.vim\n    source $VIMRUNTIME/mswin.vim\n    behave mswin\nendif\n\nsyntax on\n\"set cul\n\"set cuc\nset ruler\nset showcmd\nset nocompatible\n\nset cursorline \" 高亮当前行\n\"set paste \" 设置粘贴模式，使粘贴不错位\nset novisualbell \" 设置不闪烁\n\nset autoindent \" 继承前行的缩进方式\nset cindent\nset tabstop=4 \" 制表符为4\nset softtabstop=4 \" 按backspace按键时可以删掉4个空格\nset shiftwidth=4 \" 设置<<或>>移动的宽度为4\nset expandtab\nset smarttab \" 为C程序提供自动缩进\nset number\nset history=1000\n\nset textwidth=120 \" 设置每行120个字符自动换行\n\nset autochdir \" 自动切换当前目录为当前文件所在目录\n\nset hlsearch\nset incsearch\n\nset cmdheight=2\n\nfiletype on\nfiletype plugin on\nfiletype indent on\n\nset autoread \" 设置当文件被改动时自动载入\nset autowrite \" 自动保存\n\nset nobackup\nset noswapfile\n\nset showmatch \" 高亮显示匹配的括号\nset completeopt=preview,menu \" 代码补全\n\n\" 自动括号\n\"\ninoremap ( ()<ESC>i\ninoremap { {}<ESC>i\n\ncolorscheme peachpuff\n\n```\n\n\n### 快捷键\n\n\ngg\nG\n/\n?\nvimgrep\ncopen\nc-n\nc-p\nfx\n\ndd\ndw\ndl\n\ni\nI\na\nA\n","slug":"Vim/config","published":1,"updated":"2016-12-15T07:18:40.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cix1kgavb002odaeiakb7wnhh","content":"<p>我的vim配置，简单不花哨，尽量使用默认快捷键</p>\n<a id=\"more\"></a>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">set encoding=utf-8</div><div class=\"line\">set termencoding=utf-8</div><div class=\"line\"></div><div class=\"line\">if has(&quot;win32&quot;)    </div><div class=\"line\">    set fileencoding=chinese</div><div class=\"line\">else</div><div class=\"line\">    set fileencoding=utf-8</div><div class=\"line\">endif</div><div class=\"line\"></div><div class=\"line\">if has(&quot;win32&quot;)</div><div class=\"line\">    set nocompatible</div><div class=\"line\">    source $VIMRUNTIME/vimrc_example.vim</div><div class=\"line\">    source $VIMRUNTIME/mswin.vim</div><div class=\"line\">    behave mswin</div><div class=\"line\">endif</div><div class=\"line\"></div><div class=\"line\">syntax on</div><div class=\"line\">&quot;set cul</div><div class=\"line\">&quot;set cuc</div><div class=\"line\">set ruler</div><div class=\"line\">set showcmd</div><div class=\"line\">set nocompatible</div><div class=\"line\"></div><div class=\"line\">set cursorline &quot; 高亮当前行</div><div class=\"line\">&quot;set paste &quot; 设置粘贴模式，使粘贴不错位</div><div class=\"line\">set novisualbell &quot; 设置不闪烁</div><div class=\"line\"></div><div class=\"line\">set autoindent &quot; 继承前行的缩进方式</div><div class=\"line\">set cindent</div><div class=\"line\">set tabstop=4 &quot; 制表符为4</div><div class=\"line\">set softtabstop=4 &quot; 按backspace按键时可以删掉4个空格</div><div class=\"line\">set shiftwidth=4 &quot; 设置&lt;&lt;或&gt;&gt;移动的宽度为4</div><div class=\"line\">set expandtab</div><div class=\"line\">set smarttab &quot; 为C程序提供自动缩进</div><div class=\"line\">set number</div><div class=\"line\">set history=1000</div><div class=\"line\"></div><div class=\"line\">set textwidth=120 &quot; 设置每行120个字符自动换行</div><div class=\"line\"></div><div class=\"line\">set autochdir &quot; 自动切换当前目录为当前文件所在目录</div><div class=\"line\"></div><div class=\"line\">set hlsearch</div><div class=\"line\">set incsearch</div><div class=\"line\"></div><div class=\"line\">set cmdheight=2</div><div class=\"line\"></div><div class=\"line\">filetype on</div><div class=\"line\">filetype plugin on</div><div class=\"line\">filetype indent on</div><div class=\"line\"></div><div class=\"line\">set autoread &quot; 设置当文件被改动时自动载入</div><div class=\"line\">set autowrite &quot; 自动保存</div><div class=\"line\"></div><div class=\"line\">set nobackup</div><div class=\"line\">set noswapfile</div><div class=\"line\"></div><div class=\"line\">set showmatch &quot; 高亮显示匹配的括号</div><div class=\"line\">set completeopt=preview,menu &quot; 代码补全</div><div class=\"line\"></div><div class=\"line\">&quot; 自动括号</div><div class=\"line\">&quot;</div><div class=\"line\">inoremap ( ()&lt;ESC&gt;i</div><div class=\"line\">inoremap &#123; &#123;&#125;&lt;ESC&gt;i</div><div class=\"line\"></div><div class=\"line\">colorscheme peachpuff</div></pre></td></tr></table></figure>\n<h3 id=\"快捷键\"><a href=\"#快捷键\" class=\"headerlink\" title=\"快捷键\"></a>快捷键</h3><p>gg<br>G<br>/<br>?<br>vimgrep<br>copen<br>c-n<br>c-p<br>fx</p>\n<p>dd<br>dw<br>dl</p>\n<p>i<br>I<br>a<br>A</p>\n<hr>\n<p><strong>版权声明</strong></p>\n<p>本作品采用<a href=\"https://creativecommons.org/licenses/by/2.5/cn/\" target=\"_blank\" rel=\"external\">知识共享署名2.5 中国大陆许可协议</a>进行许可，欢迎转载，但转载请注明来自<a href=\"https://wuxiangwei.cn/\"><em>Shanno’s Blog</em></a>，并保持转载后文章内容的完整。本人保留所有版权相关权利，侵权必究。</p>\n","excerpt":"<p>我的vim配置，简单不花哨，尽量使用默认快捷键</p>","more":"<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">set encoding=utf-8</div><div class=\"line\">set termencoding=utf-8</div><div class=\"line\"></div><div class=\"line\">if has(&quot;win32&quot;)    </div><div class=\"line\">    set fileencoding=chinese</div><div class=\"line\">else</div><div class=\"line\">    set fileencoding=utf-8</div><div class=\"line\">endif</div><div class=\"line\"></div><div class=\"line\">if has(&quot;win32&quot;)</div><div class=\"line\">    set nocompatible</div><div class=\"line\">    source $VIMRUNTIME/vimrc_example.vim</div><div class=\"line\">    source $VIMRUNTIME/mswin.vim</div><div class=\"line\">    behave mswin</div><div class=\"line\">endif</div><div class=\"line\"></div><div class=\"line\">syntax on</div><div class=\"line\">&quot;set cul</div><div class=\"line\">&quot;set cuc</div><div class=\"line\">set ruler</div><div class=\"line\">set showcmd</div><div class=\"line\">set nocompatible</div><div class=\"line\"></div><div class=\"line\">set cursorline &quot; 高亮当前行</div><div class=\"line\">&quot;set paste &quot; 设置粘贴模式，使粘贴不错位</div><div class=\"line\">set novisualbell &quot; 设置不闪烁</div><div class=\"line\"></div><div class=\"line\">set autoindent &quot; 继承前行的缩进方式</div><div class=\"line\">set cindent</div><div class=\"line\">set tabstop=4 &quot; 制表符为4</div><div class=\"line\">set softtabstop=4 &quot; 按backspace按键时可以删掉4个空格</div><div class=\"line\">set shiftwidth=4 &quot; 设置&lt;&lt;或&gt;&gt;移动的宽度为4</div><div class=\"line\">set expandtab</div><div class=\"line\">set smarttab &quot; 为C程序提供自动缩进</div><div class=\"line\">set number</div><div class=\"line\">set history=1000</div><div class=\"line\"></div><div class=\"line\">set textwidth=120 &quot; 设置每行120个字符自动换行</div><div class=\"line\"></div><div class=\"line\">set autochdir &quot; 自动切换当前目录为当前文件所在目录</div><div class=\"line\"></div><div class=\"line\">set hlsearch</div><div class=\"line\">set incsearch</div><div class=\"line\"></div><div class=\"line\">set cmdheight=2</div><div class=\"line\"></div><div class=\"line\">filetype on</div><div class=\"line\">filetype plugin on</div><div class=\"line\">filetype indent on</div><div class=\"line\"></div><div class=\"line\">set autoread &quot; 设置当文件被改动时自动载入</div><div class=\"line\">set autowrite &quot; 自动保存</div><div class=\"line\"></div><div class=\"line\">set nobackup</div><div class=\"line\">set noswapfile</div><div class=\"line\"></div><div class=\"line\">set showmatch &quot; 高亮显示匹配的括号</div><div class=\"line\">set completeopt=preview,menu &quot; 代码补全</div><div class=\"line\"></div><div class=\"line\">&quot; 自动括号</div><div class=\"line\">&quot;</div><div class=\"line\">inoremap ( ()&lt;ESC&gt;i</div><div class=\"line\">inoremap &#123; &#123;&#125;&lt;ESC&gt;i</div><div class=\"line\"></div><div class=\"line\">colorscheme peachpuff</div></pre></td></tr></table></figure>\n<h3 id=\"快捷键\"><a href=\"#快捷键\" class=\"headerlink\" title=\"快捷键\"></a>快捷键</h3><p>gg<br>G<br>/<br>?<br>vimgrep<br>copen<br>c-n<br>c-p<br>fx</p>\n<p>dd<br>dw<br>dl</p>\n<p>i<br>I<br>a<br>A</p>\n<hr>\n<p><strong>版权声明</strong></p>\n<p>本作品采用<a href=\"https://creativecommons.org/licenses/by/2.5/cn/\">知识共享署名2.5 中国大陆许可协议</a>进行许可，欢迎转载，但转载请注明来自<a href=\"https://wuxiangwei.cn/\"><em>Shanno’s Blog</em></a>，并保持转载后文章内容的完整。本人保留所有版权相关权利，侵权必究。</p>"},{"title":"iSCSI 多路径","date":"2014-09-18T00:50:10.000Z","_content":"\n\n![](http://images.cnitblog.com/blog/571795/201410/081927249055512.jpg)\n\n上图(a)给出了计算机的总线结构，SCSI磁盘挂在SCSI卡上，SCSI卡和网卡都挂接在PCI总线。向SCSI磁盘读写数据时，由SCSI驱动程序生成SCSI指令并发送到挂接在PCI总线上的SCSI卡，SCSI卡翻译指令驱动硬盘进行读写操作。如果将由SCSI驱动程序生成的SCSI指令封装成报文通过网络传输到另外一台计算机，由远端计算机从报文中解析出SCSI命令，并将SCSI命令应用在自己的本地硬盘上，这就实现了iSCSI的功能。如图(b)所示，绿色的曲线表示读写数据的计算机(initiator端)将SCSI指令封装成报文通过网卡发送出去，而接受报文的计算机(target端)解析出报文中的SCSI指令后通过红色曲线读写本地的SCSI磁盘。\n\n<!--more-->\n\n![](http://images.cnitblog.com/blog/571795/201410/081927462181456.jpg)\n\n图(c)给出了iSCSI多路径的实现。Initiator端通过多个网卡将数据发送到Target端，充分利用多网卡提高读写远端磁盘的效率。Initiator端通过多个网卡将Target端的Lun挂接到本地，如上图中的sdb和sdc。要实现多个网卡共同分担读写磁盘的负载，还需要将这两个Lun合并成一个Lun，即sde。这需要通过multipath工具来实现。Multipath不仅实现了I/O流量的负载分担，还实现了故障切换和恢复。当一块网卡故障时，它可以只通过另一块网卡传输数据。\n\n**参考资料**\n\n1. [iSCSI多路径实现KVM高可用](http://www.ibm.com/developerworks/cn/linux/1303_zhuzq_iscsikvmha/)    \n2. [Multipath实现LUN设备名称的持久化](http://blog.csdn.net/tianlesoftware/article/details/5979061)          \n3. [iSCSI Network Designs: Part5-iSCSI Multipathing,Host Bus Adapters,High Availability and Redundancy](http://etherealmind.com/iscsi-network-design-part-5-iscsi-multipathing-hba-ha-high-availability-redundancy/)    \n4. [Linux 内核中的 Device Mapper 机制](http://www.ibm.com/developerworks/cn/linux/l-devmapper/index.html)          \n","source":"_posts/iSCSI/iscsi-multipath.md","raw":"title: iSCSI 多路径\ndate: 2014-09-18 08:50:10\ncategories: [iSCSI]\ntags: [iSCSI]\n---\n\n\n![](http://images.cnitblog.com/blog/571795/201410/081927249055512.jpg)\n\n上图(a)给出了计算机的总线结构，SCSI磁盘挂在SCSI卡上，SCSI卡和网卡都挂接在PCI总线。向SCSI磁盘读写数据时，由SCSI驱动程序生成SCSI指令并发送到挂接在PCI总线上的SCSI卡，SCSI卡翻译指令驱动硬盘进行读写操作。如果将由SCSI驱动程序生成的SCSI指令封装成报文通过网络传输到另外一台计算机，由远端计算机从报文中解析出SCSI命令，并将SCSI命令应用在自己的本地硬盘上，这就实现了iSCSI的功能。如图(b)所示，绿色的曲线表示读写数据的计算机(initiator端)将SCSI指令封装成报文通过网卡发送出去，而接受报文的计算机(target端)解析出报文中的SCSI指令后通过红色曲线读写本地的SCSI磁盘。\n\n<!--more-->\n\n![](http://images.cnitblog.com/blog/571795/201410/081927462181456.jpg)\n\n图(c)给出了iSCSI多路径的实现。Initiator端通过多个网卡将数据发送到Target端，充分利用多网卡提高读写远端磁盘的效率。Initiator端通过多个网卡将Target端的Lun挂接到本地，如上图中的sdb和sdc。要实现多个网卡共同分担读写磁盘的负载，还需要将这两个Lun合并成一个Lun，即sde。这需要通过multipath工具来实现。Multipath不仅实现了I/O流量的负载分担，还实现了故障切换和恢复。当一块网卡故障时，它可以只通过另一块网卡传输数据。\n\n**参考资料**\n\n1. [iSCSI多路径实现KVM高可用](http://www.ibm.com/developerworks/cn/linux/1303_zhuzq_iscsikvmha/)    \n2. [Multipath实现LUN设备名称的持久化](http://blog.csdn.net/tianlesoftware/article/details/5979061)          \n3. [iSCSI Network Designs: Part5-iSCSI Multipathing,Host Bus Adapters,High Availability and Redundancy](http://etherealmind.com/iscsi-network-design-part-5-iscsi-multipathing-hba-ha-high-availability-redundancy/)    \n4. [Linux 内核中的 Device Mapper 机制](http://www.ibm.com/developerworks/cn/linux/l-devmapper/index.html)          \n","slug":"iSCSI/iscsi-multipath","published":1,"updated":"2016-12-15T07:18:40.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cix1kgavd002tdaeit41bbccs","content":"<p><img src=\"http://images.cnitblog.com/blog/571795/201410/081927249055512.jpg\" alt=\"\"></p>\n<p>上图(a)给出了计算机的总线结构，SCSI磁盘挂在SCSI卡上，SCSI卡和网卡都挂接在PCI总线。向SCSI磁盘读写数据时，由SCSI驱动程序生成SCSI指令并发送到挂接在PCI总线上的SCSI卡，SCSI卡翻译指令驱动硬盘进行读写操作。如果将由SCSI驱动程序生成的SCSI指令封装成报文通过网络传输到另外一台计算机，由远端计算机从报文中解析出SCSI命令，并将SCSI命令应用在自己的本地硬盘上，这就实现了iSCSI的功能。如图(b)所示，绿色的曲线表示读写数据的计算机(initiator端)将SCSI指令封装成报文通过网卡发送出去，而接受报文的计算机(target端)解析出报文中的SCSI指令后通过红色曲线读写本地的SCSI磁盘。</p>\n<a id=\"more\"></a>\n<p><img src=\"http://images.cnitblog.com/blog/571795/201410/081927462181456.jpg\" alt=\"\"></p>\n<p>图(c)给出了iSCSI多路径的实现。Initiator端通过多个网卡将数据发送到Target端，充分利用多网卡提高读写远端磁盘的效率。Initiator端通过多个网卡将Target端的Lun挂接到本地，如上图中的sdb和sdc。要实现多个网卡共同分担读写磁盘的负载，还需要将这两个Lun合并成一个Lun，即sde。这需要通过multipath工具来实现。Multipath不仅实现了I/O流量的负载分担，还实现了故障切换和恢复。当一块网卡故障时，它可以只通过另一块网卡传输数据。</p>\n<p><strong>参考资料</strong></p>\n<ol>\n<li><a href=\"http://www.ibm.com/developerworks/cn/linux/1303_zhuzq_iscsikvmha/\" target=\"_blank\" rel=\"external\">iSCSI多路径实现KVM高可用</a>    </li>\n<li><a href=\"http://blog.csdn.net/tianlesoftware/article/details/5979061\" target=\"_blank\" rel=\"external\">Multipath实现LUN设备名称的持久化</a>          </li>\n<li><a href=\"http://etherealmind.com/iscsi-network-design-part-5-iscsi-multipathing-hba-ha-high-availability-redundancy/\" target=\"_blank\" rel=\"external\">iSCSI Network Designs: Part5-iSCSI Multipathing,Host Bus Adapters,High Availability and Redundancy</a>    </li>\n<li><a href=\"http://www.ibm.com/developerworks/cn/linux/l-devmapper/index.html\" target=\"_blank\" rel=\"external\">Linux 内核中的 Device Mapper 机制</a>          </li>\n</ol>\n<hr>\n<p><strong>版权声明</strong></p>\n<p>本作品采用<a href=\"https://creativecommons.org/licenses/by/2.5/cn/\" target=\"_blank\" rel=\"external\">知识共享署名2.5 中国大陆许可协议</a>进行许可，欢迎转载，但转载请注明来自<a href=\"https://wuxiangwei.cn/\"><em>Shanno’s Blog</em></a>，并保持转载后文章内容的完整。本人保留所有版权相关权利，侵权必究。</p>\n","excerpt":"<p><img src=\"http://images.cnitblog.com/blog/571795/201410/081927249055512.jpg\" alt=\"\"></p>\n<p>上图(a)给出了计算机的总线结构，SCSI磁盘挂在SCSI卡上，SCSI卡和网卡都挂接在PCI总线。向SCSI磁盘读写数据时，由SCSI驱动程序生成SCSI指令并发送到挂接在PCI总线上的SCSI卡，SCSI卡翻译指令驱动硬盘进行读写操作。如果将由SCSI驱动程序生成的SCSI指令封装成报文通过网络传输到另外一台计算机，由远端计算机从报文中解析出SCSI命令，并将SCSI命令应用在自己的本地硬盘上，这就实现了iSCSI的功能。如图(b)所示，绿色的曲线表示读写数据的计算机(initiator端)将SCSI指令封装成报文通过网卡发送出去，而接受报文的计算机(target端)解析出报文中的SCSI指令后通过红色曲线读写本地的SCSI磁盘。</p>","more":"<p><img src=\"http://images.cnitblog.com/blog/571795/201410/081927462181456.jpg\" alt=\"\"></p>\n<p>图(c)给出了iSCSI多路径的实现。Initiator端通过多个网卡将数据发送到Target端，充分利用多网卡提高读写远端磁盘的效率。Initiator端通过多个网卡将Target端的Lun挂接到本地，如上图中的sdb和sdc。要实现多个网卡共同分担读写磁盘的负载，还需要将这两个Lun合并成一个Lun，即sde。这需要通过multipath工具来实现。Multipath不仅实现了I/O流量的负载分担，还实现了故障切换和恢复。当一块网卡故障时，它可以只通过另一块网卡传输数据。</p>\n<p><strong>参考资料</strong></p>\n<ol>\n<li><a href=\"http://www.ibm.com/developerworks/cn/linux/1303_zhuzq_iscsikvmha/\">iSCSI多路径实现KVM高可用</a>    </li>\n<li><a href=\"http://blog.csdn.net/tianlesoftware/article/details/5979061\">Multipath实现LUN设备名称的持久化</a>          </li>\n<li><a href=\"http://etherealmind.com/iscsi-network-design-part-5-iscsi-multipathing-hba-ha-high-availability-redundancy/\">iSCSI Network Designs: Part5-iSCSI Multipathing,Host Bus Adapters,High Availability and Redundancy</a>    </li>\n<li><a href=\"http://www.ibm.com/developerworks/cn/linux/l-devmapper/index.html\">Linux 内核中的 Device Mapper 机制</a>          </li>\n</ol>\n<hr>\n<p><strong>版权声明</strong></p>\n<p>本作品采用<a href=\"https://creativecommons.org/licenses/by/2.5/cn/\">知识共享署名2.5 中国大陆许可协议</a>进行许可，欢迎转载，但转载请注明来自<a href=\"https://wuxiangwei.cn/\"><em>Shanno’s Blog</em></a>，并保持转载后文章内容的完整。本人保留所有版权相关权利，侵权必究。</p>"},{"title":"搭建iSCSI环境","date":"2014-09-18T09:00:18.000Z","toc":true,"_content":"\n组网环境\n\n```\n+----------+---------------+---------------+\n| hostname |  ip address   |  iscsi role   |\n+----------+---------------+---------------+\n| cvm      |  192.168.7.8  |    target     |\n| client01 |  192.168.7.10 |    initiator  |\n| client02 |  192.168.7.11 |    initiator  |\n+----------+---------------+---------------+\n```\n机器cvm将本机上的/dev/sda8磁盘通过iscsi共享出去，机器client01和client02连接到共享的磁盘。\n\n<!--more-->\n\n### iSCSI target侧\n\n修改tgt配置文件/etc/tgt/target.conf，将/dev/sda8共享出去。注意，target名称个格式为**iqn.yyyy-mm.<reversed domain name>:identifier**，即以iqn开头，后接日期和反转域名。identifier为target的标识，可以自己取，这里我们取为test-tgt。\n```\nroot@cvm:~# cat /etc/tgt/targets.conf\n<target iqn.2014-09.com.h3c.cvm:test-tgt>\n        backing-store /dev/sda8\n</target>\n```\n\n配置文件修改完成后，重启tgt服务。tgtd默认监听3260端口。\n```\nroot@cvm:~# service tgt restart        <== 重启target服务\nroot@cvm:~# netstat -npl | egrep 3260\ntcp        0      0 0.0.0.0:3260            0.0.0.0:*               LISTEN      686/tgtd        \ntcp6       0      0 :::3260                 :::*                    LISTEN      686/tgtd   \n```\n\n查看配置文件共享出去的Lun的信息。**注意，共享出去的分区*/dev/sda8*不能被使用，否则看不到相应的LUN**。\n```\nroot@cvm:~# tgt-admin -s\nTarget 1: iqn.2014-09.com.h3c.cvm:test-tgt\n    System information:\n        Driver: iscsi\n        State: ready\n    I_T nexus information:\n    LUN information:\n        LUN: 0\n            Type: controller\n            SCSI ID: IET     00010000\n            SCSI SN: beaf10\n            Size: 0 MB, Block size: 1\n            Online: Yes\n            Removable media: No\n            Readonly: No\n            Backing store type: null\n            Backing store path: None\n            Backing store flags: \n                LUN: 1                    <== 如果/dev/sda8被使用，那么这里不会显示LUN:1\n            Type: disk\n            SCSI ID: IET     00010001\n            SCSI SN: beaf11\n            Size: 280690 MB, Block size: 512\n            Online: Yes\n            Removable media: No\n            Readonly: No\n            Backing store type: rdwr\n            Backing store path: /dev/sda8\n            Backing store flags: \n    Account information:\n    ACL information:\n        ALL\nroot@cvm:~# \n```\n查询连接到target端的initiator节点，使用tgtadm --lld iscsi --op show --mode target命令。\n\n### iSCSI initiator侧\n\n发现iscsi target\n```\nroot@client01:~# iscsiadm -m discovery -t st -p 192.168.7.8\n192.168.7.8:3260,1 iqn.2014-09.com.h3c.cvm:test-tgt\n```\n\n连接设备\n```\nroot@client01:~# iscsiadm -m node iqn.2014-09.com.h3c.cvm:test-tgt -p 192.168.7.8 -l\nLogging in to [iface: default, target: iqn.2014-09.com.h3c.cvm:test-tgt, portal: 192.168.7.8,3260] (multiple)\nLogin to [iface: default, target: iqn.2014-09.com.h3c.cvm:test-tgt, portal: 192.168.7.8,3260] successful.\n```\n\n查询挂接到本地的iscsi磁盘，因为本地已经有一块iscsi磁盘sda了，所以新的磁盘名字为sdb。\n```\nroot@client01:~# ls -l /dev/disk/by-path/\nlrwxrwxrwx 1 root root 9 Sep 18 04:15 ip-192.168.7.8:3260-iscsi-iqn.2014-09.com.h3c.cvm:test-tgt-lun-1 -> ../../sdb\n```\n\n查看iscsi连接状态\n```\nroot@client01:~# iscsiadm -m node -S\n192.168.7.8:3260,1 iqn.2014-09.com.h3c.cvm:test-tgt\n```\n\n卸载设备\n```\nroot@client01:~# iscsiadm -m node -T iqn.2014-09.com.h3c.cvm:test-tgt -u\nLogging out of session [sid: 3, target: iqn.2014-09.com.h3c.cvm:test-tgt, portal: 192.168.7.8,3260]\nLogout of [sid: 3, target: iqn.2014-09.com.h3c.cvm:test-tgt, portal: 192.168.7.8,3260] successful.\n```\n\n### initiator 权限\n\n修改tgt的配置文件，可以指定能够连接到target的initiator。如下，只允许192.168.7.11的initiator节点连接到target。\n```\nroot@cvm:~# cat /etc/tgt/targets.conf\n<target iqn.2014-09.com.h3c.cvm:test-tgt>\n        backing-store /dev/sda8\n        initiator-address 192.168.7.11      <== 只准许7.11的节点连接到target\n</target>\n```\n\n分别在client01和client02上连接target，只有client02可以连接到target。\n```\nroot@client01:~# iscsiadm -m discovery -t st -p 192.168.7.8 \niscsiadm: No portals found\n\nroot@client02:~# iscsiadm -m discovery -t st -p 192.168.7.8\n192.168.7.8:3260,1 iqn.2014-09.com.h3c.cvm:test-tgt\n```\n如果要使client01和client02都可以连接到target，那么只需要修改配置文件的initiator-address为192.168.0.0/16即可。\n\n### 其它\n\n1、tgt的日志信息位于/var/log/syslog文件             \n2、查看initiator的名字。名字记录在*/etc/iscsi/initiatorname.iscsi*文件中          \n\n参考资料\n\n1. [网络驱动器装置： iSCSI 服务器](http://vbird.dic.ksu.edu.tw/linux_server/0460iscsi.php)           \n2. [ISCSI学习文档](http://blog.sina.com.cn/s/blog_755da69701014ckv.html)\n\n\n","source":"_posts/iSCSI/iscsi-setup.md","raw":"---\ntitle: 搭建iSCSI环境\ndate: 2014-09-18 17:00:18\ncategories: [iSCSI]\ntags: iSCSI\ntoc: true\n---\n\n组网环境\n\n```\n+----------+---------------+---------------+\n| hostname |  ip address   |  iscsi role   |\n+----------+---------------+---------------+\n| cvm      |  192.168.7.8  |    target     |\n| client01 |  192.168.7.10 |    initiator  |\n| client02 |  192.168.7.11 |    initiator  |\n+----------+---------------+---------------+\n```\n机器cvm将本机上的/dev/sda8磁盘通过iscsi共享出去，机器client01和client02连接到共享的磁盘。\n\n<!--more-->\n\n### iSCSI target侧\n\n修改tgt配置文件/etc/tgt/target.conf，将/dev/sda8共享出去。注意，target名称个格式为**iqn.yyyy-mm.<reversed domain name>:identifier**，即以iqn开头，后接日期和反转域名。identifier为target的标识，可以自己取，这里我们取为test-tgt。\n```\nroot@cvm:~# cat /etc/tgt/targets.conf\n<target iqn.2014-09.com.h3c.cvm:test-tgt>\n        backing-store /dev/sda8\n</target>\n```\n\n配置文件修改完成后，重启tgt服务。tgtd默认监听3260端口。\n```\nroot@cvm:~# service tgt restart        <== 重启target服务\nroot@cvm:~# netstat -npl | egrep 3260\ntcp        0      0 0.0.0.0:3260            0.0.0.0:*               LISTEN      686/tgtd        \ntcp6       0      0 :::3260                 :::*                    LISTEN      686/tgtd   \n```\n\n查看配置文件共享出去的Lun的信息。**注意，共享出去的分区*/dev/sda8*不能被使用，否则看不到相应的LUN**。\n```\nroot@cvm:~# tgt-admin -s\nTarget 1: iqn.2014-09.com.h3c.cvm:test-tgt\n    System information:\n        Driver: iscsi\n        State: ready\n    I_T nexus information:\n    LUN information:\n        LUN: 0\n            Type: controller\n            SCSI ID: IET     00010000\n            SCSI SN: beaf10\n            Size: 0 MB, Block size: 1\n            Online: Yes\n            Removable media: No\n            Readonly: No\n            Backing store type: null\n            Backing store path: None\n            Backing store flags: \n                LUN: 1                    <== 如果/dev/sda8被使用，那么这里不会显示LUN:1\n            Type: disk\n            SCSI ID: IET     00010001\n            SCSI SN: beaf11\n            Size: 280690 MB, Block size: 512\n            Online: Yes\n            Removable media: No\n            Readonly: No\n            Backing store type: rdwr\n            Backing store path: /dev/sda8\n            Backing store flags: \n    Account information:\n    ACL information:\n        ALL\nroot@cvm:~# \n```\n查询连接到target端的initiator节点，使用tgtadm --lld iscsi --op show --mode target命令。\n\n### iSCSI initiator侧\n\n发现iscsi target\n```\nroot@client01:~# iscsiadm -m discovery -t st -p 192.168.7.8\n192.168.7.8:3260,1 iqn.2014-09.com.h3c.cvm:test-tgt\n```\n\n连接设备\n```\nroot@client01:~# iscsiadm -m node iqn.2014-09.com.h3c.cvm:test-tgt -p 192.168.7.8 -l\nLogging in to [iface: default, target: iqn.2014-09.com.h3c.cvm:test-tgt, portal: 192.168.7.8,3260] (multiple)\nLogin to [iface: default, target: iqn.2014-09.com.h3c.cvm:test-tgt, portal: 192.168.7.8,3260] successful.\n```\n\n查询挂接到本地的iscsi磁盘，因为本地已经有一块iscsi磁盘sda了，所以新的磁盘名字为sdb。\n```\nroot@client01:~# ls -l /dev/disk/by-path/\nlrwxrwxrwx 1 root root 9 Sep 18 04:15 ip-192.168.7.8:3260-iscsi-iqn.2014-09.com.h3c.cvm:test-tgt-lun-1 -> ../../sdb\n```\n\n查看iscsi连接状态\n```\nroot@client01:~# iscsiadm -m node -S\n192.168.7.8:3260,1 iqn.2014-09.com.h3c.cvm:test-tgt\n```\n\n卸载设备\n```\nroot@client01:~# iscsiadm -m node -T iqn.2014-09.com.h3c.cvm:test-tgt -u\nLogging out of session [sid: 3, target: iqn.2014-09.com.h3c.cvm:test-tgt, portal: 192.168.7.8,3260]\nLogout of [sid: 3, target: iqn.2014-09.com.h3c.cvm:test-tgt, portal: 192.168.7.8,3260] successful.\n```\n\n### initiator 权限\n\n修改tgt的配置文件，可以指定能够连接到target的initiator。如下，只允许192.168.7.11的initiator节点连接到target。\n```\nroot@cvm:~# cat /etc/tgt/targets.conf\n<target iqn.2014-09.com.h3c.cvm:test-tgt>\n        backing-store /dev/sda8\n        initiator-address 192.168.7.11      <== 只准许7.11的节点连接到target\n</target>\n```\n\n分别在client01和client02上连接target，只有client02可以连接到target。\n```\nroot@client01:~# iscsiadm -m discovery -t st -p 192.168.7.8 \niscsiadm: No portals found\n\nroot@client02:~# iscsiadm -m discovery -t st -p 192.168.7.8\n192.168.7.8:3260,1 iqn.2014-09.com.h3c.cvm:test-tgt\n```\n如果要使client01和client02都可以连接到target，那么只需要修改配置文件的initiator-address为192.168.0.0/16即可。\n\n### 其它\n\n1、tgt的日志信息位于/var/log/syslog文件             \n2、查看initiator的名字。名字记录在*/etc/iscsi/initiatorname.iscsi*文件中          \n\n参考资料\n\n1. [网络驱动器装置： iSCSI 服务器](http://vbird.dic.ksu.edu.tw/linux_server/0460iscsi.php)           \n2. [ISCSI学习文档](http://blog.sina.com.cn/s/blog_755da69701014ckv.html)\n\n\n","slug":"iSCSI/iscsi-setup","published":1,"updated":"2016-12-15T07:18:40.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cix1kgavf002xdaeicks4xv8e","content":"<p>组网环境</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">+----------+---------------+---------------+</div><div class=\"line\">| hostname |  ip address   |  iscsi role   |</div><div class=\"line\">+----------+---------------+---------------+</div><div class=\"line\">| cvm      |  192.168.7.8  |    target     |</div><div class=\"line\">| client01 |  192.168.7.10 |    initiator  |</div><div class=\"line\">| client02 |  192.168.7.11 |    initiator  |</div><div class=\"line\">+----------+---------------+---------------+</div></pre></td></tr></table></figure>\n<p>机器cvm将本机上的/dev/sda8磁盘通过iscsi共享出去，机器client01和client02连接到共享的磁盘。</p>\n<a id=\"more\"></a>\n<h3 id=\"iSCSI-target侧\"><a href=\"#iSCSI-target侧\" class=\"headerlink\" title=\"iSCSI target侧\"></a>iSCSI target侧</h3><p>修改tgt配置文件/etc/tgt/target.conf，将/dev/sda8共享出去。注意，target名称个格式为<strong>iqn.yyyy-mm.<reversed domain=\"\" name=\"\">:identifier</reversed></strong>，即以iqn开头，后接日期和反转域名。identifier为target的标识，可以自己取，这里我们取为test-tgt。<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">root@cvm:~# cat /etc/tgt/targets.conf</div><div class=\"line\">&lt;target iqn.2014-09.com.h3c.cvm:test-tgt&gt;</div><div class=\"line\">        backing-store /dev/sda8</div><div class=\"line\">&lt;/target&gt;</div></pre></td></tr></table></figure></p>\n<p>配置文件修改完成后，重启tgt服务。tgtd默认监听3260端口。<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">root@cvm:~# service tgt restart        &lt;== 重启target服务</div><div class=\"line\">root@cvm:~# netstat -npl | egrep 3260</div><div class=\"line\">tcp        0      0 0.0.0.0:3260            0.0.0.0:*               LISTEN      686/tgtd        </div><div class=\"line\">tcp6       0      0 :::3260                 :::*                    LISTEN      686/tgtd</div></pre></td></tr></table></figure></p>\n<p>查看配置文件共享出去的Lun的信息。<strong>注意，共享出去的分区<em>/dev/sda8</em>不能被使用，否则看不到相应的LUN</strong>。<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">root@cvm:~# tgt-admin -s</div><div class=\"line\">Target 1: iqn.2014-09.com.h3c.cvm:test-tgt</div><div class=\"line\">    System information:</div><div class=\"line\">        Driver: iscsi</div><div class=\"line\">        State: ready</div><div class=\"line\">    I_T nexus information:</div><div class=\"line\">    LUN information:</div><div class=\"line\">        LUN: 0</div><div class=\"line\">            Type: controller</div><div class=\"line\">            SCSI ID: IET     00010000</div><div class=\"line\">            SCSI SN: beaf10</div><div class=\"line\">            Size: 0 MB, Block size: 1</div><div class=\"line\">            Online: Yes</div><div class=\"line\">            Removable media: No</div><div class=\"line\">            Readonly: No</div><div class=\"line\">            Backing store type: null</div><div class=\"line\">            Backing store path: None</div><div class=\"line\">            Backing store flags: </div><div class=\"line\">                LUN: 1                    &lt;== 如果/dev/sda8被使用，那么这里不会显示LUN:1</div><div class=\"line\">            Type: disk</div><div class=\"line\">            SCSI ID: IET     00010001</div><div class=\"line\">            SCSI SN: beaf11</div><div class=\"line\">            Size: 280690 MB, Block size: 512</div><div class=\"line\">            Online: Yes</div><div class=\"line\">            Removable media: No</div><div class=\"line\">            Readonly: No</div><div class=\"line\">            Backing store type: rdwr</div><div class=\"line\">            Backing store path: /dev/sda8</div><div class=\"line\">            Backing store flags: </div><div class=\"line\">    Account information:</div><div class=\"line\">    ACL information:</div><div class=\"line\">        ALL</div><div class=\"line\">root@cvm:~#</div></pre></td></tr></table></figure></p>\n<p>查询连接到target端的initiator节点，使用tgtadm –lld iscsi –op show –mode target命令。</p>\n<h3 id=\"iSCSI-initiator侧\"><a href=\"#iSCSI-initiator侧\" class=\"headerlink\" title=\"iSCSI initiator侧\"></a>iSCSI initiator侧</h3><p>发现iscsi target<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">root@client01:~# iscsiadm -m discovery -t st -p 192.168.7.8</div><div class=\"line\">192.168.7.8:3260,1 iqn.2014-09.com.h3c.cvm:test-tgt</div></pre></td></tr></table></figure></p>\n<p>连接设备<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">root@client01:~# iscsiadm -m node iqn.2014-09.com.h3c.cvm:test-tgt -p 192.168.7.8 -l</div><div class=\"line\">Logging in to [iface: default, target: iqn.2014-09.com.h3c.cvm:test-tgt, portal: 192.168.7.8,3260] (multiple)</div><div class=\"line\">Login to [iface: default, target: iqn.2014-09.com.h3c.cvm:test-tgt, portal: 192.168.7.8,3260] successful.</div></pre></td></tr></table></figure></p>\n<p>查询挂接到本地的iscsi磁盘，因为本地已经有一块iscsi磁盘sda了，所以新的磁盘名字为sdb。<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">root@client01:~# ls -l /dev/disk/by-path/</div><div class=\"line\">lrwxrwxrwx 1 root root 9 Sep 18 04:15 ip-192.168.7.8:3260-iscsi-iqn.2014-09.com.h3c.cvm:test-tgt-lun-1 -&gt; ../../sdb</div></pre></td></tr></table></figure></p>\n<p>查看iscsi连接状态<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">root@client01:~# iscsiadm -m node -S</div><div class=\"line\">192.168.7.8:3260,1 iqn.2014-09.com.h3c.cvm:test-tgt</div></pre></td></tr></table></figure></p>\n<p>卸载设备<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">root@client01:~# iscsiadm -m node -T iqn.2014-09.com.h3c.cvm:test-tgt -u</div><div class=\"line\">Logging out of session [sid: 3, target: iqn.2014-09.com.h3c.cvm:test-tgt, portal: 192.168.7.8,3260]</div><div class=\"line\">Logout of [sid: 3, target: iqn.2014-09.com.h3c.cvm:test-tgt, portal: 192.168.7.8,3260] successful.</div></pre></td></tr></table></figure></p>\n<h3 id=\"initiator-权限\"><a href=\"#initiator-权限\" class=\"headerlink\" title=\"initiator 权限\"></a>initiator 权限</h3><p>修改tgt的配置文件，可以指定能够连接到target的initiator。如下，只允许192.168.7.11的initiator节点连接到target。<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">root@cvm:~# cat /etc/tgt/targets.conf</div><div class=\"line\">&lt;target iqn.2014-09.com.h3c.cvm:test-tgt&gt;</div><div class=\"line\">        backing-store /dev/sda8</div><div class=\"line\">        initiator-address 192.168.7.11      &lt;== 只准许7.11的节点连接到target</div><div class=\"line\">&lt;/target&gt;</div></pre></td></tr></table></figure></p>\n<p>分别在client01和client02上连接target，只有client02可以连接到target。<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">root@client01:~# iscsiadm -m discovery -t st -p 192.168.7.8 </div><div class=\"line\">iscsiadm: No portals found</div><div class=\"line\"></div><div class=\"line\">root@client02:~# iscsiadm -m discovery -t st -p 192.168.7.8</div><div class=\"line\">192.168.7.8:3260,1 iqn.2014-09.com.h3c.cvm:test-tgt</div></pre></td></tr></table></figure></p>\n<p>如果要使client01和client02都可以连接到target，那么只需要修改配置文件的initiator-address为192.168.0.0/16即可。</p>\n<h3 id=\"其它\"><a href=\"#其它\" class=\"headerlink\" title=\"其它\"></a>其它</h3><p>1、tgt的日志信息位于/var/log/syslog文件<br>2、查看initiator的名字。名字记录在<em>/etc/iscsi/initiatorname.iscsi</em>文件中          </p>\n<p>参考资料</p>\n<ol>\n<li><a href=\"http://vbird.dic.ksu.edu.tw/linux_server/0460iscsi.php\" target=\"_blank\" rel=\"external\">网络驱动器装置： iSCSI 服务器</a>           </li>\n<li><a href=\"http://blog.sina.com.cn/s/blog_755da69701014ckv.html\" target=\"_blank\" rel=\"external\">ISCSI学习文档</a></li>\n</ol>\n<hr>\n<p><strong>版权声明</strong></p>\n<p>本作品采用<a href=\"https://creativecommons.org/licenses/by/2.5/cn/\" target=\"_blank\" rel=\"external\">知识共享署名2.5 中国大陆许可协议</a>进行许可，欢迎转载，但转载请注明来自<a href=\"https://wuxiangwei.cn/\"><em>Shanno’s Blog</em></a>，并保持转载后文章内容的完整。本人保留所有版权相关权利，侵权必究。</p>\n","excerpt":"<p>组网环境</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">+----------+---------------+---------------+</div><div class=\"line\">| hostname |  ip address   |  iscsi role   |</div><div class=\"line\">+----------+---------------+---------------+</div><div class=\"line\">| cvm      |  192.168.7.8  |    target     |</div><div class=\"line\">| client01 |  192.168.7.10 |    initiator  |</div><div class=\"line\">| client02 |  192.168.7.11 |    initiator  |</div><div class=\"line\">+----------+---------------+---------------+</div></pre></td></tr></table></figure>\n<p>机器cvm将本机上的/dev/sda8磁盘通过iscsi共享出去，机器client01和client02连接到共享的磁盘。</p>","more":"<h3 id=\"iSCSI-target侧\"><a href=\"#iSCSI-target侧\" class=\"headerlink\" title=\"iSCSI target侧\"></a>iSCSI target侧</h3><p>修改tgt配置文件/etc/tgt/target.conf，将/dev/sda8共享出去。注意，target名称个格式为<strong>iqn.yyyy-mm.<reversed domain name>:identifier</strong>，即以iqn开头，后接日期和反转域名。identifier为target的标识，可以自己取，这里我们取为test-tgt。<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">root@cvm:~# cat /etc/tgt/targets.conf</div><div class=\"line\">&lt;target iqn.2014-09.com.h3c.cvm:test-tgt&gt;</div><div class=\"line\">        backing-store /dev/sda8</div><div class=\"line\">&lt;/target&gt;</div></pre></td></tr></table></figure></p>\n<p>配置文件修改完成后，重启tgt服务。tgtd默认监听3260端口。<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">root@cvm:~# service tgt restart        &lt;== 重启target服务</div><div class=\"line\">root@cvm:~# netstat -npl | egrep 3260</div><div class=\"line\">tcp        0      0 0.0.0.0:3260            0.0.0.0:*               LISTEN      686/tgtd        </div><div class=\"line\">tcp6       0      0 :::3260                 :::*                    LISTEN      686/tgtd</div></pre></td></tr></table></figure></p>\n<p>查看配置文件共享出去的Lun的信息。<strong>注意，共享出去的分区<em>/dev/sda8</em>不能被使用，否则看不到相应的LUN</strong>。<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">root@cvm:~# tgt-admin -s</div><div class=\"line\">Target 1: iqn.2014-09.com.h3c.cvm:test-tgt</div><div class=\"line\">    System information:</div><div class=\"line\">        Driver: iscsi</div><div class=\"line\">        State: ready</div><div class=\"line\">    I_T nexus information:</div><div class=\"line\">    LUN information:</div><div class=\"line\">        LUN: 0</div><div class=\"line\">            Type: controller</div><div class=\"line\">            SCSI ID: IET     00010000</div><div class=\"line\">            SCSI SN: beaf10</div><div class=\"line\">            Size: 0 MB, Block size: 1</div><div class=\"line\">            Online: Yes</div><div class=\"line\">            Removable media: No</div><div class=\"line\">            Readonly: No</div><div class=\"line\">            Backing store type: null</div><div class=\"line\">            Backing store path: None</div><div class=\"line\">            Backing store flags: </div><div class=\"line\">                LUN: 1                    &lt;== 如果/dev/sda8被使用，那么这里不会显示LUN:1</div><div class=\"line\">            Type: disk</div><div class=\"line\">            SCSI ID: IET     00010001</div><div class=\"line\">            SCSI SN: beaf11</div><div class=\"line\">            Size: 280690 MB, Block size: 512</div><div class=\"line\">            Online: Yes</div><div class=\"line\">            Removable media: No</div><div class=\"line\">            Readonly: No</div><div class=\"line\">            Backing store type: rdwr</div><div class=\"line\">            Backing store path: /dev/sda8</div><div class=\"line\">            Backing store flags: </div><div class=\"line\">    Account information:</div><div class=\"line\">    ACL information:</div><div class=\"line\">        ALL</div><div class=\"line\">root@cvm:~#</div></pre></td></tr></table></figure></p>\n<p>查询连接到target端的initiator节点，使用tgtadm –lld iscsi –op show –mode target命令。</p>\n<h3 id=\"iSCSI-initiator侧\"><a href=\"#iSCSI-initiator侧\" class=\"headerlink\" title=\"iSCSI initiator侧\"></a>iSCSI initiator侧</h3><p>发现iscsi target<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">root@client01:~# iscsiadm -m discovery -t st -p 192.168.7.8</div><div class=\"line\">192.168.7.8:3260,1 iqn.2014-09.com.h3c.cvm:test-tgt</div></pre></td></tr></table></figure></p>\n<p>连接设备<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">root@client01:~# iscsiadm -m node iqn.2014-09.com.h3c.cvm:test-tgt -p 192.168.7.8 -l</div><div class=\"line\">Logging in to [iface: default, target: iqn.2014-09.com.h3c.cvm:test-tgt, portal: 192.168.7.8,3260] (multiple)</div><div class=\"line\">Login to [iface: default, target: iqn.2014-09.com.h3c.cvm:test-tgt, portal: 192.168.7.8,3260] successful.</div></pre></td></tr></table></figure></p>\n<p>查询挂接到本地的iscsi磁盘，因为本地已经有一块iscsi磁盘sda了，所以新的磁盘名字为sdb。<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">root@client01:~# ls -l /dev/disk/by-path/</div><div class=\"line\">lrwxrwxrwx 1 root root 9 Sep 18 04:15 ip-192.168.7.8:3260-iscsi-iqn.2014-09.com.h3c.cvm:test-tgt-lun-1 -&gt; ../../sdb</div></pre></td></tr></table></figure></p>\n<p>查看iscsi连接状态<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">root@client01:~# iscsiadm -m node -S</div><div class=\"line\">192.168.7.8:3260,1 iqn.2014-09.com.h3c.cvm:test-tgt</div></pre></td></tr></table></figure></p>\n<p>卸载设备<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">root@client01:~# iscsiadm -m node -T iqn.2014-09.com.h3c.cvm:test-tgt -u</div><div class=\"line\">Logging out of session [sid: 3, target: iqn.2014-09.com.h3c.cvm:test-tgt, portal: 192.168.7.8,3260]</div><div class=\"line\">Logout of [sid: 3, target: iqn.2014-09.com.h3c.cvm:test-tgt, portal: 192.168.7.8,3260] successful.</div></pre></td></tr></table></figure></p>\n<h3 id=\"initiator-权限\"><a href=\"#initiator-权限\" class=\"headerlink\" title=\"initiator 权限\"></a>initiator 权限</h3><p>修改tgt的配置文件，可以指定能够连接到target的initiator。如下，只允许192.168.7.11的initiator节点连接到target。<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">root@cvm:~# cat /etc/tgt/targets.conf</div><div class=\"line\">&lt;target iqn.2014-09.com.h3c.cvm:test-tgt&gt;</div><div class=\"line\">        backing-store /dev/sda8</div><div class=\"line\">        initiator-address 192.168.7.11      &lt;== 只准许7.11的节点连接到target</div><div class=\"line\">&lt;/target&gt;</div></pre></td></tr></table></figure></p>\n<p>分别在client01和client02上连接target，只有client02可以连接到target。<br><figure class=\"highlight plain\"><table><tr><td class=\"code\"><pre><div class=\"line\">root@client01:~# iscsiadm -m discovery -t st -p 192.168.7.8 </div><div class=\"line\">iscsiadm: No portals found</div><div class=\"line\"></div><div class=\"line\">root@client02:~# iscsiadm -m discovery -t st -p 192.168.7.8</div><div class=\"line\">192.168.7.8:3260,1 iqn.2014-09.com.h3c.cvm:test-tgt</div></pre></td></tr></table></figure></p>\n<p>如果要使client01和client02都可以连接到target，那么只需要修改配置文件的initiator-address为192.168.0.0/16即可。</p>\n<h3 id=\"其它\"><a href=\"#其它\" class=\"headerlink\" title=\"其它\"></a>其它</h3><p>1、tgt的日志信息位于/var/log/syslog文件<br>2、查看initiator的名字。名字记录在<em>/etc/iscsi/initiatorname.iscsi</em>文件中          </p>\n<p>参考资料</p>\n<ol>\n<li><a href=\"http://vbird.dic.ksu.edu.tw/linux_server/0460iscsi.php\">网络驱动器装置： iSCSI 服务器</a>           </li>\n<li><a href=\"http://blog.sina.com.cn/s/blog_755da69701014ckv.html\">ISCSI学习文档</a></li>\n</ol>\n<hr>\n<p><strong>版权声明</strong></p>\n<p>本作品采用<a href=\"https://creativecommons.org/licenses/by/2.5/cn/\">知识共享署名2.5 中国大陆许可协议</a>进行许可，欢迎转载，但转载请注明来自<a href=\"https://wuxiangwei.cn/\"><em>Shanno’s Blog</em></a>，并保持转载后文章内容的完整。本人保留所有版权相关权利，侵权必究。</p>"}],"PostAsset":[],"PostCategory":[{"post_id":"cix1kgath0003daeiknbk7v8m","category_id":"cix1kgatl0004daeizjf911u4","_id":"cix1kgatn0007daeifrnzf8ty"},{"post_id":"cix1kgato0008daeiigojygvl","category_id":"cix1kgatl0004daeizjf911u4","_id":"cix1kgatq0009daeiw4yj9pwe"},{"post_id":"cix1kgatr000adaei5qxqpn13","category_id":"cix1kgatl0004daeizjf911u4","_id":"cix1kgats000cdaei6x1o0i95"},{"post_id":"cix1kgatt000fdaei3gv2k1fl","category_id":"cix1kgatl0004daeizjf911u4","_id":"cix1kgatv000hdaeiy9677022"},{"post_id":"cix1kgaty000kdaei67pcsy9y","category_id":"cix1kgatl0004daeizjf911u4","_id":"cix1kgau0000mdaei9eh8cmo0"},{"post_id":"cix1kgau1000odaei3xt886ar","category_id":"cix1kgatl0004daeizjf911u4","_id":"cix1kgau2000qdaeigrr7d90h"},{"post_id":"cix1kgau4000rdaeiwh5ya24f","category_id":"cix1kgau6000sdaein02icmqs","_id":"cix1kgau7000vdaeidey5ntu8"},{"post_id":"cix1kgau7000wdaeil6pf5ja6","category_id":"cix1kgau6000sdaein02icmqs","_id":"cix1kgaua000ydaeig78vyfk6"},{"post_id":"cix1kgaub000zdaeieifaadhm","category_id":"cix1kgau6000sdaein02icmqs","_id":"cix1kgaud0011daeiyqof4v54"},{"post_id":"cix1kgaue0012daeimkjmfmbr","category_id":"cix1kgau6000sdaein02icmqs","_id":"cix1kgauf0014daeicm273q30"},{"post_id":"cix1kgaug0015daei19zrvje5","category_id":"cix1kgau6000sdaein02icmqs","_id":"cix1kgauh0017daeiglnl2aw2"},{"post_id":"cix1kgaui0018daeily4id7za","category_id":"cix1kgauk0019daeitdlivjil","_id":"cix1kgauk001adaeiwyv18wez"},{"post_id":"cix1kgaul001bdaei9rva3fi4","category_id":"cix1kgauk0019daeitdlivjil","_id":"cix1kgaum001ddaeivolmrupj"},{"post_id":"cix1kgaun001hdaei9qzzn8ro","category_id":"cix1kgauk0019daeitdlivjil","_id":"cix1kgauo001idaeih1fw515s"},{"post_id":"cix1kgaup001jdaeie3mu230k","category_id":"cix1kgauk0019daeitdlivjil","_id":"cix1kgauq001ldaeingu22ad5"},{"post_id":"cix1kgaut001qdaei9fes9zfe","category_id":"cix1kgauk0019daeitdlivjil","_id":"cix1kgauu001sdaei3j5ylzix"},{"post_id":"cix1kgauv001vdaei0sffj8h4","category_id":"cix1kgauk0019daeitdlivjil","_id":"cix1kgaux001xdaeiiuwxynld"},{"post_id":"cix1kgauy001ydaeinerqmylh","category_id":"cix1kgauk0019daeitdlivjil","_id":"cix1kgauz001zdaei4gzwxynb"},{"post_id":"cix1kgauz0020daeiuj76dni7","category_id":"cix1kgauk0019daeitdlivjil","_id":"cix1kgav10022daeiufcwhc7u"},{"post_id":"cix1kgav30027daeih8mx5njv","category_id":"cix1kgauk0019daeitdlivjil","_id":"cix1kgav40029daeijyiplys9"},{"post_id":"cix1kgav4002adaei4fytglgw","category_id":"cix1kgav5002bdaei6xcpog4r","_id":"cix1kgav6002edaeievx5bhis"},{"post_id":"cix1kgav6002fdaei8hcun347","category_id":"cix1kgav5002bdaei6xcpog4r","_id":"cix1kgav8002hdaeiejbivrx5"},{"post_id":"cix1kgav8002jdaeic9phubkk","category_id":"cix1kgava002kdaei3i30bh0u","_id":"cix1kgava002ndaei862mcbw9"},{"post_id":"cix1kgavb002odaeiakb7wnhh","category_id":"cix1kgavc002pdaeiyxbzpdy7","_id":"cix1kgavc002sdaei2wcj9j42"},{"post_id":"cix1kgavd002tdaeit41bbccs","category_id":"cix1kgave002udaein4lqmo2t","_id":"cix1kgave002wdaeid94wrunw"},{"post_id":"cix1kgavf002xdaeicks4xv8e","category_id":"cix1kgave002udaein4lqmo2t","_id":"cix1kgavg002zdaeiyzfueq09"}],"PostTag":[{"post_id":"cix1kgath0003daeiknbk7v8m","tag_id":"cix1kgatm0005daeibekpq15g","_id":"cix1kgatn0006daeicqcpmhnw"},{"post_id":"cix1kgatr000adaei5qxqpn13","tag_id":"cix1kgatr000bdaeiaryfq9yr","_id":"cix1kgats000ddaeium41js90"},{"post_id":"cix1kgatr000adaei5qxqpn13","tag_id":"cix1kgatm0005daeibekpq15g","_id":"cix1kgats000edaeiuun0mzvh"},{"post_id":"cix1kgatt000fdaei3gv2k1fl","tag_id":"cix1kgatm0005daeibekpq15g","_id":"cix1kgatv000idaeibfpxaf80"},{"post_id":"cix1kgatt000fdaei3gv2k1fl","tag_id":"cix1kgatv000gdaeibnu1x0y7","_id":"cix1kgatv000jdaeic3prlkbi"},{"post_id":"cix1kgaty000kdaei67pcsy9y","tag_id":"cix1kgatm0005daeibekpq15g","_id":"cix1kgau0000ldaeifhwybdz0"},{"post_id":"cix1kgaty000kdaei67pcsy9y","tag_id":"cix1kgatv000gdaeibnu1x0y7","_id":"cix1kgau0000ndaeihtvx7r5b"},{"post_id":"cix1kgau1000odaei3xt886ar","tag_id":"cix1kgatm0005daeibekpq15g","_id":"cix1kgau2000pdaei7hy8g8uq"},{"post_id":"cix1kgau4000rdaeiwh5ya24f","tag_id":"cix1kgau6000tdaeikr3t2pa0","_id":"cix1kgau7000udaeizxosdb5b"},{"post_id":"cix1kgau7000wdaeil6pf5ja6","tag_id":"cix1kgau6000tdaeikr3t2pa0","_id":"cix1kgau9000xdaeisvw3h816"},{"post_id":"cix1kgaub000zdaeieifaadhm","tag_id":"cix1kgau6000tdaeikr3t2pa0","_id":"cix1kgaud0010daei8pb02gp5"},{"post_id":"cix1kgaue0012daeimkjmfmbr","tag_id":"cix1kgau6000tdaeikr3t2pa0","_id":"cix1kgauf0013daei53la5wj2"},{"post_id":"cix1kgaug0015daei19zrvje5","tag_id":"cix1kgau6000tdaeikr3t2pa0","_id":"cix1kgauh0016daeiih1p677e"},{"post_id":"cix1kgaul001bdaei9rva3fi4","tag_id":"cix1kgaum001cdaeirtt4a7wf","_id":"cix1kgaun001fdaei6727x9kl"},{"post_id":"cix1kgaul001bdaei9rva3fi4","tag_id":"cix1kgaum001edaei7ofgi0be","_id":"cix1kgaun001gdaeijns33h5t"},{"post_id":"cix1kgaup001jdaeie3mu230k","tag_id":"cix1kgaum001cdaeirtt4a7wf","_id":"cix1kgaus001ndaeiiwekin0n"},{"post_id":"cix1kgaup001jdaeie3mu230k","tag_id":"cix1kgauq001kdaeizofn8xei","_id":"cix1kgaus001odaeingpxz7d7"},{"post_id":"cix1kgaup001jdaeie3mu230k","tag_id":"cix1kgaur001mdaei2c4iaq42","_id":"cix1kgaus001pdaeindfi9hvt"},{"post_id":"cix1kgaut001qdaei9fes9zfe","tag_id":"cix1kgaum001cdaeirtt4a7wf","_id":"cix1kgauv001tdaeiw75gohtz"},{"post_id":"cix1kgaut001qdaei9fes9zfe","tag_id":"cix1kgauu001rdaei8jp7fqh4","_id":"cix1kgauv001udaeid5riihqi"},{"post_id":"cix1kgauv001vdaei0sffj8h4","tag_id":"cix1kgaur001mdaei2c4iaq42","_id":"cix1kgaux001wdaei3s83znu4"},{"post_id":"cix1kgauz0020daeiuj76dni7","tag_id":"cix1kgav10021daeiixqssx6m","_id":"cix1kgav20024daei4aynriyk"},{"post_id":"cix1kgauz0020daeiuj76dni7","tag_id":"cix1kgatm0005daeibekpq15g","_id":"cix1kgav20025daei09c04cvq"},{"post_id":"cix1kgauz0020daeiuj76dni7","tag_id":"cix1kgav20023daeiy138w7er","_id":"cix1kgav20026daeiv6g8a2yt"},{"post_id":"cix1kgav30027daeih8mx5njv","tag_id":"cix1kgav10021daeiixqssx6m","_id":"cix1kgav30028daeikv1pkrdl"},{"post_id":"cix1kgav4002adaei4fytglgw","tag_id":"cix1kgav5002cdaei9dvl0mp4","_id":"cix1kgav6002ddaeiyblkr1ag"},{"post_id":"cix1kgav6002fdaei8hcun347","tag_id":"cix1kgav7002gdaeiu2ekvjzc","_id":"cix1kgav8002idaeiseg56dom"},{"post_id":"cix1kgav8002jdaeic9phubkk","tag_id":"cix1kgava002ldaeiiwwi07zk","_id":"cix1kgava002mdaeidfjaztgn"},{"post_id":"cix1kgavb002odaeiakb7wnhh","tag_id":"cix1kgavc002qdaeion83aiuk","_id":"cix1kgavc002rdaeiy8ro3fet"},{"post_id":"cix1kgavd002tdaeit41bbccs","tag_id":"cix1kgav20023daeiy138w7er","_id":"cix1kgave002vdaeiozcqoezc"},{"post_id":"cix1kgavf002xdaeicks4xv8e","tag_id":"cix1kgav20023daeiy138w7er","_id":"cix1kgavg002ydaeiqsdv6tak"}],"Tag":[{"name":"Ceph","_id":"cix1kgatm0005daeibekpq15g"},{"name":"OOD","_id":"cix1kgatr000bdaeiaryfq9yr"},{"name":"Paxos","_id":"cix1kgatv000gdaeibnu1x0y7"},{"name":"mClock","_id":"cix1kgau6000tdaeikr3t2pa0"},{"name":"Linux","_id":"cix1kgaum001cdaeirtt4a7wf"},{"name":"ctdb","_id":"cix1kgaum001edaei7ofgi0be"},{"name":"ldap","_id":"cix1kgauq001kdaeizofn8xei"},{"name":"nfs","_id":"cix1kgaur001mdaei2c4iaq42"},{"name":"msys2","_id":"cix1kgauu001rdaei8jp7fqh4"},{"name":"OCFS2","_id":"cix1kgav10021daeiixqssx6m"},{"name":"iSCSI","_id":"cix1kgav20023daeiy138w7er"},{"name":"Git","_id":"cix1kgav5002cdaei9dvl0mp4"},{"name":"Hexo","_id":"cix1kgav7002gdaeiu2ekvjzc"},{"name":"Python","_id":"cix1kgava002ldaeiiwwi07zk"},{"name":"Vim","_id":"cix1kgavc002qdaeion83aiuk"}]}}